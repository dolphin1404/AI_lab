{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dolphin1404/AI_lab/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 도서-스크립트 변환 프로젝트: 데이터 수집 및 전처리\n",
    "\n",
    "## 프로젝트 개요\n",
    "- **목표**: 도서 텍스트를 비디오 스크립트 형식으로 변환하는 LLM 모델 개발\n",
    "- **단계**: 데이터 수집 → 전처리 → 모델링 → 성능평가\n",
    "- **일정**: 데이터 전처리 마감 - 10월 27일\n",
    "\n",
    "## 데이터 전처리 목표\n",
    "1. 공개 도서 아카이브에서 텍스트 데이터 수집 (Project Gutenberg, 국내 디지털 도서관)\n",
    "2. 노이즈 제거 및 텍스트 정제\n",
    "3. 핵심 요소 추출 (인물, 장소, 시간)\n",
    "4. 도서 문체 → 스크립트 문체 변환을 위한 학습 데이터셋 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "install_packages",
    "outputId": "5912e9dc-ac53-4708-a2b0-d4eb08e9abfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting ko-core-news-sm==3.8.0\n",
      "Collecting ko-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/14.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ko_core_news_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ko_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install gutenberg requests beautifulsoup4 nltk spacy transformers datasets\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ko_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "import_libraries",
    "outputId": "67937159-6414-4abb-b09f-d32cbacb861e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# SpaCy 모델 로드\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_collection"
   },
   "source": [
    "## 2. 데이터 수집 (Data Collection)\n",
    "\n",
    "### 2.1 Project Gutenberg에서 도서 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gutenberg_collection",
    "outputId": "39f0a7e9-6dbf-4f9c-c42d-e67b4f65f24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 총 10개의 도서를 처리합니다:\n",
      "  - Book ID: 1342\n",
      "  - Book ID: 2701\n",
      "  - Book ID: 84\n",
      "  - Book ID: 1661\n",
      "  - Book ID: 11\n",
      "  - Book ID: 98\n",
      "  - Book ID: 74\n",
      "  - Book ID: 345\n",
      "  - Book ID: 46\n",
      "  - Book ID: 1952\n"
     ]
    }
   ],
   "source": [
    "# 구텐버그에서 수집할 도서 ID 리스트 (랜덤 10개)\n",
    "# 인기있고 챕터 구분이 명확한 고전 소설들을 선택\n",
    "# 각 ID는 Project Gutenberg의 책 번호입니다\n",
    "\n",
    "import random\n",
    "\n",
    "popular_book_ids = [\n",
    "    1342,  # Pride and Prejudice by Jane Austen\n",
    "    2701,  # Moby Dick by Herman Melville  \n",
    "    84,    # Frankenstein by Mary Shelley\n",
    "    1661,  # The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "    11,    # Alice's Adventures in Wonderland by Lewis Carroll\n",
    "    98,    # A Tale of Two Cities by Charles Dickens\n",
    "    74,    # The Adventures of Tom Sawyer by Mark Twain\n",
    "    345,   # Dracula by Bram Stoker\n",
    "    46,    # A Christmas Carol by Charles Dickens\n",
    "    1952,  # The Yellow Wallpaper by Charlotte Perkins Gilman\n",
    "]\n",
    "\n",
    "# 랜덤하게 10개 선택 (이미 10개이므로 전체 사용)\n",
    "random.seed(42)  # 재현성을 위한 시드 설정\n",
    "book_ids_to_process = popular_book_ids[:10]\n",
    "\n",
    "print(f\"📚 총 {len(book_ids_to_process)}개의 도서를 처리합니다:\")\n",
    "for book_id in book_ids_to_process:\n",
    "    print(f\"  - Book ID: {book_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gutenberg Collector initialized\n"
     ]
    }
   ],
   "source": [
    "class GutenbergCollector:\n",
    "    \"\"\"Project Gutenberg에서 도서를 수집하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.gutenberg.org/files/\"\n",
    "        self.cache_dir = \"./gutenberg_cache\"\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    \n",
    "    def download_book(self, book_id):\n",
    "        \"\"\"\n",
    "        도서 다운로드\n",
    "        \n",
    "        Args:\n",
    "            book_id (int): 도서 ID\n",
    "            \n",
    "        Returns:\n",
    "            str: 도서 텍스트 (실패 시 None)\n",
    "        \"\"\"\n",
    "        # 캐시 확인\n",
    "        cache_file = os.path.join(self.cache_dir, f\"book_{book_id}.txt\")\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"  ✓ Loading from cache: {cache_file}\")\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        # 다운로드 시도\n",
    "        url = f\"{self.base_url}{book_id}/{book_id}-0.txt\"\n",
    "        try:\n",
    "            print(f\"  Downloading from: {url}\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            text = response.text\n",
    "            \n",
    "            # 캐시에 저장\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to download book {book_id}: {e}\")\n",
    "            \n",
    "            # 대체 URL 시도 (-0 없이)\n",
    "            alt_url = f\"{self.base_url}{book_id}/{book_id}.txt\"\n",
    "            try:\n",
    "                print(f\"  Trying alternative URL: {alt_url}\")\n",
    "                response = requests.get(alt_url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                text = response.text\n",
    "                \n",
    "                # 캐시에 저장\n",
    "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(text)\n",
    "                \n",
    "                return text\n",
    "            except Exception as e2:\n",
    "                print(f\"  ✗ Alternative URL also failed: {e2}\")\n",
    "                return None\n",
    "\n",
    "# GutenbergCollector 초기화\n",
    "collector = GutenbergCollector()\n",
    "print(\"✓ Gutenberg Collector initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📥 샘플 도서 다운로드 중... (Book ID: 1342)\n",
      "  ✓ Loading from cache: ./gutenberg_cache/book_1342.txt\n",
      "✓ 다운로드 완료: 728846 문자\n",
      "\n",
      "텍스트 미리보기 (처음 500자):\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "\n",
      "                        156 CHARING CROSS ROAD\n",
      "                                LONDON\n",
      "\n",
      "                             RUSKIN HOUSE\n",
      "                                   ]\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "               _Reading Jane’s Letters._      _Chap 34._\n",
      "                                   ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "# 샘플 도서 다운로드 테스트 (첫 번째 도서)\n",
    "if book_ids_to_process:\n",
    "    book_id = book_ids_to_process[0]\n",
    "    print(f\"\\n📥 샘플 도서 다운로드 중... (Book ID: {book_id})\")\n",
    "    book_text = collector.download_book(book_id)\n",
    "    \n",
    "    if book_text:\n",
    "        print(f\"✓ 다운로드 완료: {len(book_text)} 문자\")\n",
    "        print(f\"\\n텍스트 미리보기 (처음 500자):\")\n",
    "        print(book_text[:500])\n",
    "    else:\n",
    "        print(\"✗ 다운로드 실패\")\n",
    "        book_text = None\n",
    "else:\n",
    "    print(\"book_ids_to_process가 정의되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## 3. 데이터 전처리 (Data Preprocessing)\n",
    "\n",
    "### 3.1 텍스트 정제 (Text Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "text_cleaning"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "# nlp_en is not defined in the provided code, so I'm commenting it out.\n",
    "# You would typically initialize a spaCy model here, for example:\n",
    "# import spacy\n",
    "# nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"텍스트 전처리를 위한 클래스\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.nlp = nlp_en\n",
    "        pass\n",
    "\n",
    "    def remove_gutenberg_header_footer(self, text):\n",
    "        \"\"\"\n",
    "        Project Gutenberg 헤더와 푸터 제거\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "\n",
    "        Returns:\n",
    "            str: 헤더/푸터가 제거된 텍스트\n",
    "        \"\"\"\n",
    "        # 시작 마커 찾기\n",
    "        start_markers = [\n",
    "            \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "            \"*** START OF THE PROJECT GUTENBERG\",\n",
    "            \"***START OF THE PROJECT GUTENBERG\"\n",
    "        ]\n",
    "\n",
    "        # 종료 마커 찾기\n",
    "        end_markers = [\n",
    "            \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "            \"*** END OF THE PROJECT GUTENBERG\",\n",
    "            \"***END OF THE PROJECT GUTENBERG\"\n",
    "        ]\n",
    "\n",
    "        start_idx = 0\n",
    "        for marker in start_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                start_idx = text.find('\\n', idx) + 1\n",
    "                break\n",
    "\n",
    "        end_idx = len(text)\n",
    "        for marker in end_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                end_idx = idx\n",
    "                break\n",
    "\n",
    "        return text[start_idx:end_idx].strip()\n",
    "\n",
    "    def remove_table_of_contents(self, text):\n",
    "        \"\"\"\n",
    "        목차(Table of Contents) 섹션 제거 (v4 - 개선된 정확도)\n",
    "\n",
    "        개선 사항:\n",
    "        - 실제 챕터와 목차 항목 구분 강화\n",
    "        - Chapter I부터 정확히 보존\n",
    "        - 목차 패턴 더 정밀하게 감지\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "\n",
    "        Returns:\n",
    "            str: 목차가 제거된 텍스트\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        result_lines = []\n",
    "        in_toc = False\n",
    "        toc_start_idx = -1\n",
    "        toc_line_count = 0\n",
    "        consecutive_heading_lines = 0\n",
    "        found_first_chapter = False\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line_stripped = line.strip()\n",
    "            line_lower = line_stripped.lower()\n",
    "\n",
    "            # 목차 시작 감지 (더 정확한 패턴)\n",
    "            if not in_toc and not found_first_chapter:\n",
    "                # \"CONTENTS\" 단독으로 나오는 경우 (일반적인 목차 시작)\n",
    "                if line_stripped.upper() in ['CONTENTS', 'TABLE OF CONTENTS', 'LIST OF CHAPTERS']:\n",
    "                    in_toc = True\n",
    "                    toc_start_idx = i\n",
    "                    toc_line_count = 0\n",
    "                    continue\n",
    "\n",
    "                # \"Heading to Chapter\" 패턴 여러 줄 연속 (Pride and Prejudice 스타일)\n",
    "                if 'heading to chapter' in line_lower or 'heading to CHAPTER' in line_lower:\n",
    "                    consecutive_heading_lines += 1\n",
    "                    # 2줄 이상 연속으로 나오면 목차\n",
    "                    if consecutive_heading_lines >= 2:\n",
    "                        in_toc = True\n",
    "                        toc_start_idx = i - consecutive_heading_lines\n",
    "                        continue\n",
    "                else:\n",
    "                    if 0 < consecutive_heading_lines < 2:\n",
    "                        # 1줄만 있었으면 실제 내용일 수 있음\n",
    "                        consecutive_heading_lines = 0\n",
    "\n",
    "            # 실제 챕터 시작 확인 (목차 종료 또는 첫 챕터 발견)\n",
    "            # 더 엄격한 조건: 줄이 짧고(80자 이하), 패턴이 명확하고, 다음 줄에 내용이 있어야 함\n",
    "            is_real_chapter = False\n",
    "            if len(line_stripped) <= 80:\n",
    "                # \"CHAPTER I\", \"Chapter 1\", \"CHAPTER ONE\" 등의 패턴\n",
    "                chapter_pattern = r'^\\s*(CHAPTER|Chapter)\\s+(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX|XXI|XXII|XXIII|XXIV|XXV|XXVI|XXVII|XXVIII|XXIX|XXX|XXXI|XXXII|XXXIII|XXXIV|XXXV|XXXVI|XXXVII|XXXVIII|XXXIX|XL|XLI|XLII|XLIII|XLIV|XLV|XLVI|XLVII|XLVIII|XLIX|L|LI|LII|LIII|LIV|LV|LVI|LVII|LVIII|LIX|LX|LXI|LXII|LXIII|LXIV|LXV|LXVI|LXVII|LXVIII|LXIX|LXX|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty)\\.?\\s*$'\n",
    "                chapter_match = re.match(chapter_pattern, line_stripped)\n",
    "\n",
    "                if chapter_match:\n",
    "                    # 다음 몇 줄을 확인하여 실제 챕터 내용이 있는지 검증\n",
    "                    has_content_after = False\n",
    "                    for j in range(i + 1, min(i + 10, len(lines))):\n",
    "                        next_line = lines[j].strip()\n",
    "                        # 빈 줄이 아니고, 40자 이상의 실제 문장이 있으면 챕터로 간주\n",
    "                        if next_line and len(next_line) > 40:\n",
    "                            # 목차 키워드가 없어야 함\n",
    "                            if 'heading to' not in next_line.lower() and 'page' not in next_line.lower():\n",
    "                                has_content_after = True\n",
    "                                break\n",
    "\n",
    "                    if has_content_after:\n",
    "                        is_real_chapter = True\n",
    "                        found_first_chapter = True\n",
    "\n",
    "                        # 목차 중이었다면 목차 종료\n",
    "                        if in_toc and toc_line_count > 3:\n",
    "                            in_toc = False\n",
    "                            # 목차 부분 스킵 (이미 추가 안 함)\n",
    "\n",
    "            # 목차 중이면 줄 카운트만 증가하고 추가 안 함\n",
    "            if in_toc:\n",
    "                toc_line_count += 1\n",
    "                # 목차가 너무 길면 (150줄 이상) 종료\n",
    "                if toc_line_count > 150:\n",
    "                    in_toc = False\n",
    "                continue # 목차 내용은 결과에 추가하지 않음\n",
    "\n",
    "            # 실제 챕터를 만났거나, 목차 상태가 아니면 라인 추가\n",
    "            if is_real_chapter or not in_toc:\n",
    "                result_lines.append(line)\n",
    "\n",
    "        return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        기본 텍스트 정제\n",
    "        - 과도한 공백 제거\n",
    "        - 특수 문자 정규화\n",
    "        - 줄바꿈 정규화\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "\n",
    "        Returns:\n",
    "            str: 정제된 텍스트\n",
    "        \"\"\"\n",
    "        # 여러 줄바꿈을 단일 줄바꿈으로\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # 여러 공백을 단일 공백으로\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # 줄 시작/끝 공백 제거\n",
    "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "        return text.strip()\n",
    "\n",
    "    def split_into_chapters(self, text):\n",
    "        \"\"\"\n",
    "        텍스트를 챕터별로 분할 (개선된 버전 v2)\n",
    "\n",
    "        개선 사항:\n",
    "        - 목차(Table of Contents) 자동 제거\n",
    "        - \"Heading to Chapter\" 같은 목차 항목 필터링\n",
    "        - 실제 챕터 내용만 추출\n",
    "        - 더 정확한 챕터 감지\n",
    "\n",
    "        지원하는 형식:\n",
    "        - \"CHAPTER I\", \"CHAPTER 1\", \"CHAPTER ONE\"\n",
    "        - \"Chapter I.\", \"Chapter 1.\", \"Chapter One.\"\n",
    "        - 실제 챕터 제목이 있는 경우 (e.g., \"CHAPTER I. The Beginning\")\n",
    "\n",
    "        Args:\n",
    "            text (str): 전체 텍스트\n",
    "\n",
    "        Returns:\n",
    "            list: 챕터별 텍스트 리스트\n",
    "        \"\"\"\n",
    "        # 1단계: 목차 제거\n",
    "        text_without_toc = self.remove_table_of_contents(text)\n",
    "\n",
    "        # 2단계: 챕터 패턴 매칭\n",
    "        # 안전한 패턴 사용 (catastrophic backtracking 방지)\n",
    "        word_numbers = \"One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty|Twenty-one|Twenty-two|Twenty-three|Twenty-four|Twenty-five|Twenty-six|Twenty-seven|Twenty-eight|Twenty-nine|Thirty|Thirty-one|Thirty-two|Thirty-three|Thirty-four|Thirty-five|Thirty-six|Thirty-seven|Thirty-eight|Thirty-nine|Forty|Forty-one|Forty-two|Forty-three|Forty-four|Forty-five|Forty-six|Forty-seven|Forty-eight|Forty-nine|Fifty|Fifty-one|Fifty-two|Fifty-three|Fifty-four|Fifty-five|Fifty-six|Fifty-seven|Fifty-eight|Fifty-nine|Sixty|Sixty-one\"\n",
    "\n",
    "        patterns = [\n",
    "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|' + word_numbers + r')(?:\\.\\s*|\\s+|\\n)',\n",
    "            # Pattern 2: \"BOOK I\", \"PART I\"\n",
    "            r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)(?:\\.\\s*|\\s+|\\n)',\n",
    "        ]\n",
    "\n",
    "        best_split = None\n",
    "        best_count = 0\n",
    "        best_pattern_idx = -1\n",
    "\n",
    "        # 각 패턴 시도 (타임아웃 추가)\n",
    "        for idx, pattern in enumerate(patterns):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                splits = re.split(pattern, text_without_toc)\n",
    "\n",
    "                # 타임아웃 체크 (3초)\n",
    "                if time.time() - start_time > 3:\n",
    "                    print(f\"Warning: Pattern {idx} took too long, skipping\")\n",
    "                    continue\n",
    "\n",
    "                # 챕터 수 계산 (3개 요소가 1개 챕터: prefix, number, content)\n",
    "                chapter_count = (len(splits) - 1) // 3 if len(splits) > 1 else 0\n",
    "\n",
    "                # 합리적인 범위의 챕터 수 (2-150개)\n",
    "                if 2 <= chapter_count <= 150:\n",
    "                    # 챕터 평균 길이 확인 (너무 짧으면 목차일 가능성)\n",
    "                    total_length = sum(len(splits[i]) for i in range(2, len(splits), 3) if i < len(splits))\n",
    "                    avg_length = total_length / chapter_count if chapter_count > 0 else 0\n",
    "\n",
    "                    # 평균 길이가 300자 이상이어야 실제 챕터\n",
    "                    if avg_length >= 300 and chapter_count > best_count:\n",
    "                        best_count = chapter_count\n",
    "                        best_split = (pattern, splits)\n",
    "                        best_pattern_idx = idx\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Pattern {idx} failed with error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 3단계: 챕터 추출\n",
    "        if best_split:\n",
    "            pattern, chapters = best_split\n",
    "            result = []\n",
    "\n",
    "            for i in range(1, len(chapters), 3):\n",
    "                if i + 2 <= len(chapters):\n",
    "                    chapter_prefix = chapters[i].strip()    # \"CHAPTER\" or \"Chapter\"\n",
    "                    chapter_number = chapters[i + 1].strip()  # \"I\", \"1\", etc.\n",
    "                    chapter_content = chapters[i + 2].strip() if i + 2 < len(chapters) else \"\"\n",
    "\n",
    "                    # 챕터 내용이 충분히 긴지 확인 (최소 200자)\n",
    "                    if len(chapter_content) < 200:\n",
    "                        continue\n",
    "\n",
    "                    # 챕터 제목에서 실제 제목 추출\n",
    "                    content_lines = chapter_content.split('\\n', 2)\n",
    "                    chapter_title_suffix = \"\"\n",
    "\n",
    "                    # 첫 줄이 짧으면 (60자 이하) 챕터 제목으로 간주\n",
    "                    if content_lines and len(content_lines[0]) <= 60 and content_lines[0].strip():\n",
    "                        chapter_title_suffix = content_lines[0].strip()\n",
    "                        # 제목을 제외한 나머지가 내용\n",
    "                        if len(content_lines) > 1:\n",
    "                            chapter_content = '\\n'.join(content_lines[1:]).strip()\n",
    "\n",
    "                    # 최종 챕터 제목 생성\n",
    "                    if chapter_prefix:\n",
    "                        if chapter_title_suffix:\n",
    "                            chapter_title = f\"{chapter_prefix} {chapter_number}. {chapter_title_suffix}\"\n",
    "                        else:\n",
    "                            chapter_title = f\"{chapter_prefix} {chapter_number}\"\n",
    "                    else:\n",
    "                        chapter_title = f\"Chapter {chapter_number}\"\n",
    "\n",
    "                    if chapter_content:  # 내용이 있을 때만 추가\n",
    "                        result.append({\n",
    "                            'title': chapter_title,\n",
    "                            'content': chapter_content\n",
    "                        })\n",
    "\n",
    "            # 결과 검증: 최소 2개 이상의 챕터\n",
    "            if len(result) >= 2:\n",
    "                return result\n",
    "\n",
    "        # 4단계: Fallback - 수동 라인 분석\n",
    "        return self._fallback_chapter_split(text_without_toc)\n",
    "\n",
    "\n",
    "    def _fallback_chapter_split(self, text):\n",
    "        \"\"\"\n",
    "        Fallback 방법: 줄 단위로 챕터 마커 찾기\n",
    "        목차 항목은 제외하고 실제 챕터만 추출\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        chapters = []\n",
    "        current_chapter = None\n",
    "        current_content = []\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line_stripped = line.strip()\n",
    "            line_upper = line_stripped.upper()\n",
    "\n",
    "            # 목차 항목 건너뛰기\n",
    "            if 'HEADING TO' in line_upper or 'CONTENTS' in line_upper:\n",
    "                continue\n",
    "\n",
    "            # 챕터 헤더 감지\n",
    "            is_chapter = False\n",
    "            chapter_title = None\n",
    "\n",
    "            # 짧은 줄 (60자 이하)에서만 챕터 검사\n",
    "            if len(line_stripped) <= 60 and line_stripped:\n",
    "                # \"CHAPTER X\" 형식\n",
    "                chapter_match = re.match(r'^(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)', line_stripped)\n",
    "                if chapter_match:\n",
    "                    is_chapter = True\n",
    "                    chapter_title = line_stripped\n",
    "                # \"BOOK X\", \"PART X\" 형식\n",
    "                elif re.match(r'^(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)', line_stripped):\n",
    "                    is_chapter = True\n",
    "                    chapter_title = line_stripped\n",
    "\n",
    "            if is_chapter and chapter_title:\n",
    "                # 이전 챕터 저장\n",
    "                if current_chapter and current_content:\n",
    "                    content_text = '\\n'.join(current_content).strip()\n",
    "                    # 내용이 충분히 긴 경우만 (최소 300자)\n",
    "                    if len(content_text) >= 300:\n",
    "                        chapters.append({\n",
    "                            'title': current_chapter,\n",
    "                            'content': content_text\n",
    "                        })\n",
    "\n",
    "                # 새 챕터 시작\n",
    "                current_chapter = chapter_title\n",
    "                current_content = []\n",
    "            else:\n",
    "                # 현재 챕터에 내용 추가\n",
    "                if current_chapter:  # 챕터가 시작된 후에만\n",
    "                    current_content.append(line)\n",
    "\n",
    "        # 마지막 챕터 저장\n",
    "        if current_chapter and current_content:\n",
    "            content_text = '\\n'.join(current_content).strip()\n",
    "            if len(content_text) >= 300:\n",
    "                chapters.append({\n",
    "                    'title': current_chapter,\n",
    "                    'content': content_text\n",
    "                })\n",
    "\n",
    "        # 최소 2개 이상의 챕터가 있어야 유효\n",
    "        if len(chapters) >= 2:\n",
    "            return chapters\n",
    "        else:\n",
    "            return [{'title': 'Full Text', 'content': text}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apply_preprocessing",
    "outputId": "5fd67467-0957-4631-be8e-b6aa2b12b929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 전처리 시작 ===\n",
      "\n",
      "✓ Original length: 728,846 characters\n",
      "✓ Cleaned length: 720,973 characters\n",
      "✓ Removed: 7,873 characters\n",
      "\n",
      "챕터 분할 중...\n",
      "✓ Found 61 chapters\n",
      "\n",
      "첫 번째 챕터:\n",
      "  제목: Chapter I.]\n",
      "  길이: 4,741 문자\n",
      "  미리보기: It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first enter...\n"
     ]
    }
   ],
   "source": [
    "# 전처리 파이프라인 테스트\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# 샘플 도서에 전처리 적용\n",
    "if 'book_text' in globals() and book_text:\n",
    "    print(\"=== 전처리 시작 ===\\n\")\n",
    "    \n",
    "    # 헤더/푸터 제거\n",
    "    cleaned_text = preprocessor.remove_gutenberg_header_footer(book_text)\n",
    "    cleaned_text = preprocessor.clean_text(cleaned_text)\n",
    "\n",
    "    print(f\"✓ Original length: {len(book_text):,} characters\")\n",
    "    print(f\"✓ Cleaned length: {len(cleaned_text):,} characters\")\n",
    "    print(f\"✓ Removed: {len(book_text) - len(cleaned_text):,} characters\")\n",
    "\n",
    "    # 챕터 분할\n",
    "    print(f\"\\n챕터 분할 중...\")\n",
    "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
    "    print(f\"✓ Found {len(chapters)} chapters\")\n",
    "\n",
    "    if chapters:\n",
    "        print(f\"\\n첫 번째 챕터:\")\n",
    "        print(f\"  제목: {chapters[0]['title']}\")\n",
    "        print(f\"  길이: {len(chapters[0]['content']):,} 문자\")\n",
    "        print(f\"  미리보기: {chapters[0]['content'][:200]}...\")\n",
    "else:\n",
    "    print(\"⚠️  먼저 book_text를 다운로드해주세요 (위의 셀을 실행하세요)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "entity_extraction"
   },
   "source": [
    "### 3.2 핵심 요소 추출 (Entity Extraction)\n",
    "\n",
    "인물, 장소, 시간 등 핵심 요소를 추출하여 스크립트 변환에 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "entity_extractor",
    "outputId": "cc3e3515-6b38-4798-9bdd-a7175709dd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Entity Extractor initialized\n"
     ]
    }
   ],
   "source": [
    "class EntityExtractor:\n",
    "    \"\"\"Named Entity Recognition을 통한 핵심 요소 추출 클래스\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp_en\n",
    "\n",
    "    def extract_entities(self, text, max_length=1000000):\n",
    "        \"\"\"\n",
    "        텍스트에서 개체명 추출\n",
    "\n",
    "        Args:\n",
    "            text (str): 분석할 텍스트\n",
    "            max_length (int): 처리할 최대 텍스트 길이\n",
    "\n",
    "        Returns:\n",
    "            dict: 카테고리별 개체명 사전\n",
    "        \"\"\"\n",
    "        # SpaCy의 max_length 설정\n",
    "        self.nlp.max_length = max_length\n",
    "\n",
    "        # 텍스트 분석\n",
    "        doc = self.nlp(text[:max_length])\n",
    "\n",
    "        # 개체명 분류\n",
    "        entities = {\n",
    "            'PERSON': [],      # 인물\n",
    "            'GPE': [],          # 지정학적 개체 (도시, 국가 등)\n",
    "            'LOC': [],          # 위치\n",
    "            'DATE': [],         # 날짜\n",
    "            'TIME': [],         # 시간\n",
    "            'ORG': [],          # 조직\n",
    "            'EVENT': []         # 이벤트\n",
    "        }\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in entities:\n",
    "                entities[ent.label_].append(ent.text)\n",
    "\n",
    "        # 중복 제거 및 빈도수 계산\n",
    "        for key in entities:\n",
    "            entity_counts = defaultdict(int)\n",
    "            for entity in entities[key]:\n",
    "                entity_counts[entity] += 1\n",
    "            entities[key] = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def get_main_characters(self, entities, top_n=10):\n",
    "        \"\"\"\n",
    "        주요 인물 추출\n",
    "\n",
    "        Args:\n",
    "            entities (dict): extract_entities의 결과\n",
    "            top_n (int): 반환할 주요 인물 수\n",
    "\n",
    "        Returns:\n",
    "            list: 주요 인물 리스트\n",
    "        \"\"\"\n",
    "        return entities['PERSON'][:top_n]\n",
    "\n",
    "    def get_main_locations(self, entities, top_n=10):\n",
    "        \"\"\"\n",
    "        주요 장소 추출\n",
    "\n",
    "        Args:\n",
    "            entities (dict): extract_entities의 결과\n",
    "            top_n (int): 반환할 주요 장소 수\n",
    "\n",
    "        Returns:\n",
    "            list: 주요 장소 리스트\n",
    "        \"\"\"\n",
    "        locations = entities['GPE'] + entities['LOC']\n",
    "        # 빈도수로 재정렬\n",
    "        location_dict = defaultdict(int)\n",
    "        for loc, count in locations:\n",
    "            location_dict[loc] += count\n",
    "        return sorted(location_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# 사용 예시\n",
    "extractor = EntityExtractor()\n",
    "print(\"✓ Entity Extractor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "extract_sample_entities",
    "outputId": "b167395d-c00e-4047-ab13-cccdfcdaafdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 개체명 추출 중 ===\n",
      "\n",
      "✓ Entity extraction completed\n",
      "\n",
      "📍 Main Characters:\n",
      "  - Bennet: 6 mentions\n",
      "  - Bingley: 4 mentions\n",
      "  - George Allen: 3 mentions\n",
      "  - Lizzy: 3 mentions\n",
      "  - Long: 2 mentions\n",
      "\n",
      "🗺️  Main Locations:\n",
      "  - England: 1 mentions\n",
      "  - Lydia: 1 mentions\n",
      "\n",
      "📅 Temporal Information:\n",
      "  - 1894: 3 mentions\n",
      "  - one day: 1 mentions\n",
      "  - Monday: 1 mentions\n",
      "  - the end of next week: 1 mentions\n",
      "  - five thousand a year: 1 mentions\n",
      "✓ Entity extraction completed\n",
      "\n",
      "📍 Main Characters:\n",
      "  - Bennet: 6 mentions\n",
      "  - Bingley: 4 mentions\n",
      "  - George Allen: 3 mentions\n",
      "  - Lizzy: 3 mentions\n",
      "  - Long: 2 mentions\n",
      "\n",
      "🗺️  Main Locations:\n",
      "  - England: 1 mentions\n",
      "  - Lydia: 1 mentions\n",
      "\n",
      "📅 Temporal Information:\n",
      "  - 1894: 3 mentions\n",
      "  - one day: 1 mentions\n",
      "  - Monday: 1 mentions\n",
      "  - the end of next week: 1 mentions\n",
      "  - five thousand a year: 1 mentions\n"
     ]
    }
   ],
   "source": [
    "# 샘플 챕터에서 개체명 추출\n",
    "if 'chapters' in globals() and chapters:\n",
    "    # 첫 번째 챕터 분석\n",
    "    sample_chapter = chapters[0]['content']\n",
    "    print(\"=== 개체명 추출 중 ===\\n\")\n",
    "    entities = extractor.extract_entities(sample_chapter)\n",
    "\n",
    "    print(\"✓ Entity extraction completed\\n\")\n",
    "\n",
    "    # 주요 인물\n",
    "    main_characters = extractor.get_main_characters(entities, top_n=5)\n",
    "    print(\"📍 Main Characters:\")\n",
    "    for char, count in main_characters:\n",
    "        print(f\"  - {char}: {count} mentions\")\n",
    "\n",
    "    # 주요 장소\n",
    "    main_locations = extractor.get_main_locations(entities, top_n=5)\n",
    "    print(\"\\n🗺️  Main Locations:\")\n",
    "    for loc, count in main_locations:\n",
    "        print(f\"  - {loc}: {count} mentions\")\n",
    "\n",
    "    # 시간 정보\n",
    "    if entities['DATE']:\n",
    "        print(\"\\n📅 Temporal Information:\")\n",
    "        for date, count in entities['DATE'][:5]:\n",
    "            print(f\"  - {date}: {count} mentions\")\n",
    "else:\n",
    "    print(\"⚠️  먼저 chapters를 생성해주세요 (위의 전처리 셀을 실행하세요)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "script_conversion"
   },
   "source": [
    "### 3.3 스크립트 변환을 위한 데이터 구조화\n",
    "\n",
    "도서 텍스트를 스크립트 학습에 적합한 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "script_formatter",
    "outputId": "f2157de3-bfe9-4185-d737-5a7a25d576ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Script Formatter initialized\n"
     ]
    }
   ],
   "source": [
    "class ScriptFormatter:\n",
    "    \"\"\"도서 텍스트를 스크립트 형식으로 변환하는 클래스\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp_en\n",
    "\n",
    "    def extract_dialogues(self, text):\n",
    "        \"\"\"\n",
    "        텍스트에서 대화문 추출\n",
    "\n",
    "        Args:\n",
    "            text (str): 분석할 텍스트\n",
    "\n",
    "        Returns:\n",
    "            list: 대화문 리스트\n",
    "        \"\"\"\n",
    "        # 따옴표로 둘러싸인 대화문 추출\n",
    "        dialogue_pattern = r'[\"\\']([^\"\\']+)[\"\\']'\n",
    "        dialogues = re.findall(dialogue_pattern, text)\n",
    "\n",
    "        # 짧은 대화 필터링 (3단어 이상)\n",
    "        dialogues = [d for d in dialogues if len(d.split()) >= 3]\n",
    "\n",
    "        return dialogues\n",
    "\n",
    "    def extract_narrative(self, text):\n",
    "        \"\"\"\n",
    "        서술 부분 추출 (대화가 아닌 부분)\n",
    "\n",
    "        Args:\n",
    "            text (str): 분석할 텍스트\n",
    "\n",
    "        Returns:\n",
    "            str: 서술 텍스트\n",
    "        \"\"\"\n",
    "        # 대화문 제거\n",
    "        narrative = re.sub(r'[\"\\'][^\"\\']+[\"\\']', '', text)\n",
    "\n",
    "        # 정제\n",
    "        narrative = re.sub(r' +', ' ', narrative)\n",
    "        narrative = re.sub(r'\\n\\s*\\n', '\\n\\n', narrative)\n",
    "\n",
    "        return narrative.strip()\n",
    "\n",
    "    def create_scene_structure(self, chapter_text, entities):\n",
    "        \"\"\"\n",
    "        챕터를 씬 구조로 변환\n",
    "\n",
    "        Args:\n",
    "            chapter_text (str): 챕터 텍스트\n",
    "            entities (dict): 추출된 개체명\n",
    "\n",
    "        Returns:\n",
    "            dict: 씬 구조 정보\n",
    "        \"\"\"\n",
    "        # 문장 단위로 분할\n",
    "        doc = self.nlp(chapter_text[:100000])  # 처리 속도를 위해 제한\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # 대화문과 서술 분리\n",
    "        dialogues = self.extract_dialogues(chapter_text)\n",
    "        narrative = self.extract_narrative(chapter_text)\n",
    "\n",
    "        scene_data = {\n",
    "            'characters': [char for char, _ in entities.get('PERSON', [])[:5]],\n",
    "            'locations': [loc for loc, _ in (entities.get('GPE', []) + entities.get('LOC', []))[:3]],\n",
    "            'dialogues': dialogues[:10],\n",
    "            'narrative_sentences': sentences[:20],\n",
    "            'total_sentences': len(sentences),\n",
    "            'total_dialogues': len(dialogues)\n",
    "        }\n",
    "\n",
    "        return scene_data\n",
    "\n",
    "# 사용 예시\n",
    "formatter = ScriptFormatter()\n",
    "print(\"✓ Script Formatter initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "create_scene",
    "outputId": "d5c30fb2-d76e-49e3-a2da-57b39b8a0fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 씬 구조 생성 중 ===\n",
      "\n",
      "✓ Scene structure created\n",
      "\n",
      "🎬 Scene Information:\n",
      "  - Main Characters: Bennet, Bingley, George Allen, Lizzy, Long\n",
      "  - Locations: England, Lydia\n",
      "  - Total Sentences: 53\n",
      "  - Total Dialogues: 0\n",
      "\n",
      "📝 Sample Narrative:\n",
      "  1. It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in...\n",
      "  2. However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhoo...\n",
      "  3. “My dear Mr. Bennet,” said his lady to him one day, “have you heard that\n",
      "Netherfield Park is let at ...\n",
      "✓ Scene structure created\n",
      "\n",
      "🎬 Scene Information:\n",
      "  - Main Characters: Bennet, Bingley, George Allen, Lizzy, Long\n",
      "  - Locations: England, Lydia\n",
      "  - Total Sentences: 53\n",
      "  - Total Dialogues: 0\n",
      "\n",
      "📝 Sample Narrative:\n",
      "  1. It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in...\n",
      "  2. However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhoo...\n",
      "  3. “My dear Mr. Bennet,” said his lady to him one day, “have you heard that\n",
      "Netherfield Park is let at ...\n"
     ]
    }
   ],
   "source": [
    "# 샘플 챕터를 씬 구조로 변환\n",
    "if 'chapters' in globals() and 'entities' in globals() and chapters and entities:\n",
    "    print(\"=== 씬 구조 생성 중 ===\\n\")\n",
    "    scene_data = formatter.create_scene_structure(chapters[0]['content'], entities)\n",
    "\n",
    "    print(\"✓ Scene structure created\\n\")\n",
    "    print(f\"🎬 Scene Information:\")\n",
    "    print(f\"  - Main Characters: {', '.join(scene_data['characters'][:5])}\")\n",
    "    print(f\"  - Locations: {', '.join(scene_data['locations'][:3])}\")\n",
    "    print(f\"  - Total Sentences: {scene_data['total_sentences']}\")\n",
    "    print(f\"  - Total Dialogues: {scene_data['total_dialogues']}\")\n",
    "\n",
    "    if scene_data['dialogues']:\n",
    "        print(\"\\n💬 Sample Dialogues:\")\n",
    "        for i, dialogue in enumerate(scene_data['dialogues'][:3], 1):\n",
    "            print(f\"  {i}. \\\"{dialogue[:80]}...\\\"\" if len(dialogue) > 80 else f\"  {i}. \\\"{dialogue}\\\"\")\n",
    "\n",
    "    if scene_data['narrative_sentences']:\n",
    "        print(\"\\n📝 Sample Narrative:\")\n",
    "        for i, sentence in enumerate(scene_data['narrative_sentences'][:3], 1):\n",
    "            preview = sentence[:100] + \"...\" if len(sentence) > 100 else sentence\n",
    "            print(f\"  {i}. {preview}\")\n",
    "else:\n",
    "    print(\"⚠️  먼저 chapters와 entities를 생성해주세요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_creation"
   },
   "source": [
    "## 4. 학습 데이터셋 구축\n",
    "\n",
    "### 4.1 전체 파이프라인 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pipeline",
    "outputId": "143dcc35-e6d5-43be-c1dc-5d5a7f642e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "class BookToScriptPipeline:\n",
    "    \"\"\"도서에서 스크립트 학습 데이터까지의 전체 파이프라인\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.collector = GutenbergCollector()\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.extractor = EntityExtractor()\n",
    "        self.formatter = ScriptFormatter()\n",
    "\n",
    "    def process_book(self, book_id):\n",
    "        \"\"\"\n",
    "        단일 도서 처리\n",
    "\n",
    "        Args:\n",
    "            book_id (int): 도서 ID\n",
    "\n",
    "        Returns:\n",
    "            dict: 처리된 데이터\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing book {book_id}...\")\n",
    "\n",
    "        # 1. 도서 다운로드\n",
    "        book_text = self.collector.download_book(book_id)\n",
    "        if not book_text:\n",
    "            return None\n",
    "        print(f\"  ✓ Downloaded ({len(book_text)} chars)\")\n",
    "\n",
    "        # 2. 전처리\n",
    "        cleaned_text = self.preprocessor.remove_gutenberg_header_footer(book_text)\n",
    "        cleaned_text = self.preprocessor.clean_text(cleaned_text)\n",
    "        print(f\"  ✓ Cleaned ({len(cleaned_text)} chars)\")\n",
    "\n",
    "        # 3. 챕터 분할\n",
    "        chapters = self.preprocessor.split_into_chapters(cleaned_text)\n",
    "        print(f\"  ✓ Split into {len(chapters)} chapters\")\n",
    "\n",
    "        # 4. 각 챕터별 처리 (전체 챕터)\n",
    "        processed_chapters = []\n",
    "        for i, chapter in enumerate(chapters):  # 전체 챕터 처리\n",
    "            # 개체명 추출\n",
    "            entities = self.extractor.extract_entities(chapter['content'])\n",
    "\n",
    "            # 씬 구조 생성\n",
    "            scene_data = self.formatter.create_scene_structure(chapter['content'], entities)\n",
    "\n",
    "            processed_chapters.append({\n",
    "                'chapter_title': chapter['title'],\n",
    "                'chapter_number': i + 1,\n",
    "                'original_text': chapter['content'],\n",
    "                'entities': entities,\n",
    "                'scene_data': scene_data\n",
    "            })\n",
    "\n",
    "        print(f\"  ✓ Processed {len(processed_chapters)} chapters\")\n",
    "\n",
    "        return {\n",
    "            'book_id': book_id,\n",
    "            'total_length': len(cleaned_text),\n",
    "            'total_chapters': len(chapters),\n",
    "            'processed_chapters': processed_chapters\n",
    "        }\n",
    "\n",
    "    def process_multiple_books(self, book_ids, output_dir='./processed_data'):\n",
    "        \"\"\"\n",
    "        여러 도서 처리 및 저장\n",
    "\n",
    "        Args:\n",
    "            book_ids (list): 도서 ID 리스트\n",
    "            output_dir (str): 출력 디렉토리\n",
    "\n",
    "        Returns:\n",
    "            list: 처리 결과 리스트\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        results = []\n",
    "\n",
    "        for book_id in book_ids:\n",
    "            result = self.process_book(book_id)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "\n",
    "                # JSON으로 저장\n",
    "                output_file = os.path.join(output_dir, f'book_{book_id}.json')\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"  ✓ Saved to {output_file}\")\n",
    "\n",
    "        print(f\"\\n✓ Completed processing {len(results)} books\")\n",
    "        return results\n",
    "\n",
    "# 파이프라인 초기화\n",
    "pipeline = BookToScriptPipeline()\n",
    "print(\"✓ Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  먼저 results를 생성해주세요 (파이프라인 실행 필요)\n"
     ]
    }
   ],
   "source": [
    "# 학습 샘플 생성\n",
    "if 'results' in globals() and results:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📊 학습 데이터셋 생성 중...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 샘플 생성\n",
    "    training_samples = dataset_builder.create_training_samples(results)\n",
    "    print(f\"\\n✓ 생성된 학습 샘플: {len(training_samples)}개\")\n",
    "    \n",
    "    # 데이터셋 분할\n",
    "    train_data, val_data, test_data = dataset_builder.split_dataset(training_samples)\n",
    "    print(f\"\\n✓ 데이터셋 분할 완료:\")\n",
    "    print(f\"  - 학습(Train): {len(train_data)} 샘플 ({len(train_data)/len(training_samples)*100:.1f}%)\")\n",
    "    print(f\"  - 검증(Val): {len(val_data)} 샘플 ({len(val_data)/len(training_samples)*100:.1f}%)\")\n",
    "    print(f\"  - 테스트(Test): {len(test_data)} 샘플 ({len(test_data)/len(training_samples)*100:.1f}%)\")\n",
    "    \n",
    "    # 저장\n",
    "    stats = dataset_builder.save_datasets(train_data, val_data, test_data, output_dir='./')\n",
    "    \n",
    "    # 샘플 미리보기\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📝 첫 번째 학습 샘플 미리보기:\")\n",
    "    print(\"=\" * 70)\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\n[입력 (Input)]\")\n",
    "    print(sample['input'][:300] + \"...\")\n",
    "    print(f\"\\n[출력 (Output)]\")\n",
    "    print(sample['output'][:300] + \"...\")\n",
    "    print(f\"\\n[메타데이터]\")\n",
    "    print(f\"  - Book ID: {sample['metadata']['book_id']}\")\n",
    "    print(f\"  - Chapter: {sample['metadata']['chapter_title']}\")\n",
    "    print(f\"  - Input Length: {sample['metadata']['input_length']} chars\")\n",
    "    print(f\"  - Output Length: {sample['metadata']['output_length']} chars\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  먼저 results를 생성해주세요 (파이프라인 실행 필요)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  먼저 training_samples를 생성해주세요\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 통계\n",
    "if 'training_samples' in globals() and training_samples:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📈 데이터셋 통계 분석\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 길이 통계\n",
    "    input_lengths = [s['metadata']['input_length'] for s in training_samples]\n",
    "    output_lengths = [s['metadata']['output_length'] for s in training_samples]\n",
    "    \n",
    "    print(f\"\\n✓ 입력(Input) 텍스트 길이:\")\n",
    "    print(f\"  - 평균: {sum(input_lengths)/len(input_lengths):.0f} 문자\")\n",
    "    print(f\"  - 최소: {min(input_lengths)} 문자\")\n",
    "    print(f\"  - 최대: {max(input_lengths)} 문자\")\n",
    "    \n",
    "    print(f\"\\n✓ 출력(Output) 텍스트 길이:\")\n",
    "    print(f\"  - 평균: {sum(output_lengths)/len(output_lengths):.0f} 문자\")\n",
    "    print(f\"  - 최소: {min(output_lengths)} 문자\")\n",
    "    print(f\"  - 최대: {max(output_lengths)} 문자\")\n",
    "    \n",
    "    # 도서별 샘플 수\n",
    "    book_sample_counts = {}\n",
    "    for sample in training_samples:\n",
    "        book_id = sample['metadata']['book_id']\n",
    "        book_sample_counts[book_id] = book_sample_counts.get(book_id, 0) + 1\n",
    "    \n",
    "    print(f\"\\n✓ 도서별 샘플 수:\")\n",
    "    for book_id, count in sorted(book_sample_counts.items()):\n",
    "        print(f\"  - Book {book_id}: {count} 샘플\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✅ 데이터셋 생성 완료!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n📁 생성된 파일:\")\n",
    "    print(f\"  - train_data.json\")\n",
    "    print(f\"  - val_data.json\")\n",
    "    print(f\"  - test_data.json\")\n",
    "    print(f\"\\n🚀 이제 모델 학습을 시작할 수 있습니다!\")\n",
    "else:\n",
    "    print(\"⚠️  먼저 training_samples를 생성해주세요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 데이터셋 통계 및 검증\n",
    "\n",
    "생성된 데이터셋의 품질을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset Builder initialized\n"
     ]
    }
   ],
   "source": [
    "class DatasetBuilder:\n",
    "    \"\"\"학습 데이터셋 구축 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_training_samples(self, processed_results):\n",
    "        \"\"\"\n",
    "        처리된 결과를 학습 샘플로 변환\n",
    "        \n",
    "        Args:\n",
    "            processed_results (list): process_multiple_books의 결과\n",
    "            \n",
    "        Returns:\n",
    "            list: 학습 샘플 리스트\n",
    "        \"\"\"\n",
    "        training_samples = []\n",
    "        \n",
    "        for result in processed_results:\n",
    "            book_id = result['book_id']\n",
    "            \n",
    "            for chapter in result['processed_chapters']:\n",
    "                # 입력: 원본 챕터 텍스트 + 프롬프트\n",
    "                input_text = f\"\"\"Convert the following book chapter into a video script format. \n",
    "Extract key elements including characters, locations, dialogues, and narrative descriptions.\n",
    "\n",
    "Chapter: {chapter['chapter_title']}\n",
    "\n",
    "Text:\n",
    "{chapter['original_text'][:2000]}\"\"\"  # 처음 2000자만 사용 (토큰 제한 고려)\n",
    "                \n",
    "                # 출력: 구조화된 스크립트 정보\n",
    "                scene_data = chapter['scene_data']\n",
    "                entities = chapter['entities']\n",
    "                \n",
    "                output_script = {\n",
    "                    'scene_title': chapter['chapter_title'],\n",
    "                    'characters': scene_data['characters'][:10],\n",
    "                    'locations': scene_data['locations'][:5],\n",
    "                    'dialogues': scene_data['dialogues'][:15],\n",
    "                    'narrative': ' '.join(scene_data['narrative_sentences'][:10]),\n",
    "                    'total_sentences': scene_data['total_sentences'],\n",
    "                    'total_dialogues': scene_data['total_dialogues']\n",
    "                }\n",
    "                \n",
    "                # JSON 문자열로 변환 (모델 학습용)\n",
    "                output_text = json.dumps(output_script, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                training_samples.append({\n",
    "                    'input': input_text,\n",
    "                    'output': output_text,\n",
    "                    'metadata': {\n",
    "                        'book_id': book_id,\n",
    "                        'chapter_number': chapter['chapter_number'],\n",
    "                        'chapter_title': chapter['chapter_title'],\n",
    "                        'input_length': len(input_text),\n",
    "                        'output_length': len(output_text)\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        return training_samples\n",
    "    \n",
    "    def split_dataset(self, samples, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "        \"\"\"\n",
    "        데이터셋을 학습/검증/테스트로 분할\n",
    "        \n",
    "        Args:\n",
    "            samples (list): 전체 샘플\n",
    "            train_ratio (float): 학습 데이터 비율\n",
    "            val_ratio (float): 검증 데이터 비율\n",
    "            test_ratio (float): 테스트 데이터 비율\n",
    "            seed (int): 랜덤 시드\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (train_data, val_data, test_data)\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # 첫 번째 분할: train vs (val + test)\n",
    "        train_data, temp_data = train_test_split(\n",
    "            samples, \n",
    "            test_size=(val_ratio + test_ratio),\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        # 두 번째 분할: val vs test\n",
    "        val_data, test_data = train_test_split(\n",
    "            temp_data,\n",
    "            test_size=test_ratio / (val_ratio + test_ratio),\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    def save_datasets(self, train_data, val_data, test_data, output_dir='./'):\n",
    "        \"\"\"\n",
    "        데이터셋을 JSON 파일로 저장\n",
    "        \n",
    "        Args:\n",
    "            train_data (list): 학습 데이터\n",
    "            val_data (list): 검증 데이터\n",
    "            test_data (list): 테스트 데이터\n",
    "            output_dir (str): 출력 디렉토리\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 저장\n",
    "        with open(os.path.join(output_dir, 'train_data.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'val_data.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'test_data.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✓ Datasets saved to {output_dir}\")\n",
    "        print(f\"  - train_data.json: {len(train_data)} samples\")\n",
    "        print(f\"  - val_data.json: {len(val_data)} samples\")\n",
    "        print(f\"  - test_data.json: {len(test_data)} samples\")\n",
    "        \n",
    "        return {\n",
    "            'train': len(train_data),\n",
    "            'val': len(val_data),\n",
    "            'test': len(test_data),\n",
    "            'total': len(train_data) + len(val_data) + len(test_data)\n",
    "        }\n",
    "\n",
    "# DatasetBuilder 초기화\n",
    "dataset_builder = DatasetBuilder()\n",
    "print(\"✓ Dataset Builder initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 학습 데이터셋 포맷 정의\n",
    "\n",
    "LLM 학습에 적합한 입력-출력 쌍 형식으로 데이터를 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🚀 전체 파이프라인 실행 시작\n",
      "======================================================================\n",
      "\n",
      "Processing book 1342...\n",
      "  ✓ Loading from cache: ./gutenberg_cache/book_1342.txt\n",
      "  ✓ Downloaded (728846 chars)\n",
      "  ✓ Cleaned (720973 chars)\n",
      "  ✓ Split into 61 chapters\n"
     ]
    }
   ],
   "source": [
    "# 전체 도서 처리\n",
    "print(\"=\" * 70)\n",
    "print(\"🚀 전체 파이프라인 실행 시작\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 결과 저장\n",
    "results = pipeline.process_multiple_books(\n",
    "    book_ids_to_process,\n",
    "    output_dir='./processed_data'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"✅ 처리 완료: {len(results)}권의 도서\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 전체 도서 처리 및 데이터셋 생성\n",
    "\n",
    "10개의 도서를 모두 처리하고 학습용 데이터셋을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 5. 성능 평가 지표 준비\n",
    "\n",
    "### 5.1 BLEU Score 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bleu_metric",
    "outputId": "5bb1816c-3c3a-4c15-bac1-39e5cba25be4"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"모델 성능 평가를 위한 메트릭 클래스\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.smooth = SmoothingFunction()\n",
    "\n",
    "    def calculate_bleu(self, reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "        \"\"\"\n",
    "        BLEU 점수 계산\n",
    "\n",
    "        Args:\n",
    "            reference (str): 참조 텍스트\n",
    "            candidate (str): 생성된 텍스트\n",
    "            weights (tuple): n-gram 가중치\n",
    "\n",
    "        Returns:\n",
    "            float: BLEU 점수\n",
    "        \"\"\"\n",
    "        # 토큰화\n",
    "        reference_tokens = reference.split()\n",
    "        candidate_tokens = candidate.split()\n",
    "\n",
    "        # BLEU 계산 (smoothing 적용)\n",
    "        bleu_score = sentence_bleu(\n",
    "            [reference_tokens],\n",
    "            candidate_tokens,\n",
    "            weights=weights,\n",
    "            smoothing_function=self.smooth.method1\n",
    "        )\n",
    "\n",
    "        return bleu_score\n",
    "\n",
    "    def calculate_bleu_variants(self, reference, candidate):\n",
    "        \"\"\"\n",
    "        다양한 BLEU 변형 계산 (BLEU-1 ~ BLEU-4)\n",
    "\n",
    "        Args:\n",
    "            reference (str): 참조 텍스트\n",
    "            candidate (str): 생성된 텍스트\n",
    "\n",
    "        Returns:\n",
    "            dict: BLEU 변형 점수\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'BLEU-1': self.calculate_bleu(reference, candidate, weights=(1, 0, 0, 0)),\n",
    "            'BLEU-2': self.calculate_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)),\n",
    "            'BLEU-3': self.calculate_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)),\n",
    "            'BLEU-4': self.calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        }\n",
    "\n",
    "# 평가 메트릭 초기화\n",
    "metrics = EvaluationMetrics()\n",
    "print(\"✓ Evaluation Metrics initialized\")\n",
    "\n",
    "# 테스트 예시\n",
    "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "candidate_text = \"The quick brown fox jumps over a lazy dog\"\n",
    "\n",
    "bleu_scores = metrics.calculate_bleu_variants(reference_text, candidate_text)\n",
    "print(\"\\nBLEU Score Examples:\")\n",
    "for metric, score in bleu_scores.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 6. 다음 단계 및 계획\n",
    "\n",
    "### 완료된 작업\n",
    "- ✅ Project Gutenberg 데이터 수집 모듈\n",
    "- ✅ 텍스트 전처리 (노이즈 제거, 정제)\n",
    "- ✅ 챕터 분할 기능\n",
    "- ✅ 개체명 추출 (인물, 장소, 시간)\n",
    "- ✅ 스크립트 변환을 위한 데이터 구조화\n",
    "- ✅ BLEU 평가 지표 구현\n",
    "\n",
    "### 향후 작업 (10월 27일까지)\n",
    "1. **데이터 수집 확대**\n",
    "   - 더 많은 도서 다운로드 및 처리\n",
    "   - 다양한 장르 확보 (소설, 드라마, 미스터리 등)\n",
    "   - 국내 디지털 도서관 데이터 수집 방법 연구\n",
    "\n",
    "2. **전처리 고도화**\n",
    "   - 대화문과 서술 분리 정확도 향상\n",
    "   - 장면 전환 감지 알고리즘 개발\n",
    "   - 감정/톤 분석 추가\n",
    "\n",
    "3. **데이터셋 품질 검증**\n",
    "   - 결측치 및 이상치 검사\n",
    "   - 데이터 통계 분석\n",
    "   - 샘플 데이터 검토\n",
    "\n",
    "### 모델링 준비 (10월 29일 이후)\n",
    "- LLM 모델 선택 (GPT-2, T5, BART 등)\n",
    "- Fine-tuning 전략 수립\n",
    "- 학습 데이터 포맷 정의\n",
    "\n",
    "### 성능 평가 계획\n",
    "- BLEU Score: 텍스트 유사도 측정\n",
    "- FVD (선택): 비디오 품질 평가 (여유가 있을 경우)\n",
    "- CLIPScore (선택): 텍스트-이미지 유사도 (여유가 있을 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_checkpoint"
   },
   "source": [
    "## 7. 진행 상황 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "save_progress",
    "outputId": "f4d36f24-986d-4d66-872b-26f4177cbbaa"
   },
   "outputs": [],
   "source": [
    "# 진행 상황 요약 저장\n",
    "progress_summary = {\n",
    "    'date': '2025-10-12',\n",
    "    'phase': 'Data Preprocessing',\n",
    "    'completed_tasks': [\n",
    "        'Data collection module (Project Gutenberg)',\n",
    "        'Text cleaning and preprocessing',\n",
    "        'Chapter segmentation',\n",
    "        'Entity extraction (characters, locations, time)',\n",
    "        'Script formatting structure',\n",
    "        'BLEU evaluation metric'\n",
    "    ],\n",
    "    'next_tasks': [\n",
    "        'Expand dataset collection',\n",
    "        'Enhance preprocessing accuracy',\n",
    "        'Data quality validation',\n",
    "        'Prepare for modeling phase'\n",
    "    ],\n",
    "    'deadline': '2025-10-27',\n",
    "    'team_notes': 'Following professor\\'s guidance - parallel work on dataset collection and framework setup'\n",
    "}\n",
    "\n",
    "# JSON으로 저장\n",
    "with open('progress_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(progress_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✓ Progress summary saved to 'progress_summary.json'\")\n",
    "print(\"\\nCurrent Status:\")\n",
    "print(f\"  Phase: {progress_summary['phase']}\")\n",
    "print(f\"  Deadline: {progress_summary['deadline']}\")\n",
    "print(f\"  Completed: {len(progress_summary['completed_tasks'])} tasks\")\n",
    "print(f\"  Remaining: {len(progress_summary['next_tasks'])} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_splitting_examples"
   },
   "source": [
    "### 3.1.1 챕터 분할 예시 및 디버깅\n",
    "\n",
    "개선된 챕터 분할 알고리즘 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test_chapter_splitting",
    "outputId": "f38aa14a-4697-4464-8e5e-efc60e8ed3d8"
   },
   "outputs": [],
   "source": [
    "# 챕터 분할 테스트 및 디버깅\n",
    "if 'cleaned_text' in globals() and cleaned_text:\n",
    "    print(\"=== 챕터 분할 디버깅 ===\")\n",
    "    print(f\"\\n1. 원본 텍스트 샘플 (처음 1000자):\")\n",
    "    print(cleaned_text[:1000])\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "    # 챕터 패턴 수동 검색\n",
    "    print(\"2. 챕터 패턴 검색 결과:\")\n",
    "\n",
    "    patterns_to_test = [\n",
    "        (\"CHAPTER/Chapter + 번호\", r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)'),\n",
    "        (\"로마자/숫자만\", r'\\n\\s*([IVXLCDM]+|\\d+)\\.?\\s*\\n'),\n",
    "        (\"BOOK/PART + 번호\", r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)'),\n",
    "    ]\n",
    "\n",
    "    for pattern_name, pattern in patterns_to_test:\n",
    "        matches = re.findall(pattern, cleaned_text)\n",
    "        print(f\"  - {pattern_name}: {len(matches)}개 발견\")\n",
    "        if matches and len(matches) <= 10:\n",
    "            print(f\"    예시: {matches[:5]}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "    # 실제 챕터 분할 수행\n",
    "    print(\"3. 챕터 분할 결과:\")\n",
    "    chapters_debug = preprocessor.split_into_chapters(cleaned_text)\n",
    "    print(f\"  총 {len(chapters_debug)}개 챕터 발견\\n\")\n",
    "\n",
    "    # 처음 3개 챕터 미리보기\n",
    "    for i, chapter in enumerate(chapters_debug[:3], 1):\n",
    "        print(f\"  Chapter {i}:\")\n",
    "        print(f\"    제목: '{chapter['title']}'\")\n",
    "        print(f\"    길이: {len(chapter['content']):,} 문자\")\n",
    "        print(f\"    내용 미리보기: {chapter['content'][:150]}...\")\n",
    "        print()\n",
    "\n",
    "    # 챕터 길이 통계\n",
    "    if len(chapters_debug) > 1:\n",
    "        chapter_lengths = [len(ch['content']) for ch in chapters_debug]\n",
    "        print(f\"\\n4. 챕터 길이 통계:\")\n",
    "        print(f\"  - 평균: {sum(chapter_lengths)/len(chapter_lengths):,.0f} 문자\")\n",
    "        print(f\"  - 최소: {min(chapter_lengths):,} 문자\")\n",
    "        print(f\"  - 최대: {max(chapter_lengths):,} 문자\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ 챕터 분할 분석 완료\")\n",
    "else:\n",
    "    print(\"⚠️  먼저 샘플 도서를 다운로드하고 정제하세요 (위의 전처리 셀을 실행하세요)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
