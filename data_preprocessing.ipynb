{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolphin1404/AI_lab/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 도서-스크립트 변환 프로젝트: 데이터 수집 및 전처리\n",
        "\n",
        "## 프로젝트 개요\n",
        "- **목표**: 도서 텍스트를 비디오 스크립트 형식으로 변환하는 LLM 모델 개발\n",
        "- **단계**: 데이터 수집 → 전처리 → 모델링 → 성능평가\n",
        "- **일정**: 데이터 전처리 마감 - 10월 27일\n",
        "\n",
        "## 데이터 전처리 목표\n",
        "1. 공개 도서 아카이브에서 텍스트 데이터 수집 (Project Gutenberg, 국내 디지털 도서관)\n",
        "2. 노이즈 제거 및 텍스트 정제\n",
        "3. 핵심 요소 추출 (인물, 장소, 시간)\n",
        "4. 도서 문체 → 스크립트 문체 변환을 위한 학습 데이터셋 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. 환경 설정 및 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_packages"
      },
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install gutenberg requests beautifulsoup4 nltk spacy transformers datasets\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "import_libraries"
      },
      "source": [
        "# 라이브러리 임포트\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# NLTK 데이터 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# SpaCy 모델 로드\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_collection"
      },
      "source": [
        "## 2. 데이터 수집 (Data Collection)\n",
        "\n",
        "### 2.1 Project Gutenberg에서 도서 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gutenberg_collection"
      },
      "source": [
        "class GutenbergCollector:\n",
        "    \"\"\"Project Gutenberg에서 도서 데이터를 수집하는 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self, base_url=\"https://www.gutenberg.org\"):\n",
        "        self.base_url = base_url\n",
        "        \n",
        "    def download_book(self, book_id):\n",
        "        \"\"\"\n",
        "        특정 책 ID로 도서 텍스트 다운로드\n",
        "        \n",
        "        Args:\n",
        "            book_id (int): Gutenberg 도서 ID\n",
        "        \n",
        "        Returns:\n",
        "            str: 도서 텍스트 내용\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/files/{book_id}/{book_id}-0.txt\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.text\n",
        "            else:\n",
        "                print(f\"Failed to download book {book_id}: Status {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading book {book_id}: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def get_popular_books(self, genre=\"fiction\", limit=10):\n",
        "        \"\"\"\n",
        "        장르별 인기 도서 ID 리스트 반환\n",
        "        \n",
        "        Args:\n",
        "            genre (str): 도서 장르\n",
        "            limit (int): 다운로드할 최대 도서 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 도서 ID 리스트\n",
        "        \"\"\"\n",
        "        # 인기 고전 소설 ID 리스트 (예시)\n",
        "        popular_fiction = [\n",
        "            1342,  # Pride and Prejudice by Jane Austen\n",
        "            84,    # Frankenstein by Mary Shelley\n",
        "            98,    # A Tale of Two Cities by Charles Dickens\n",
        "            1661,  # Sherlock Holmes by Arthur Conan Doyle\n",
        "            2701,  # Moby Dick by Herman Melville\n",
        "            11,    # Alice's Adventures in Wonderland by Lewis Carroll\n",
        "            74,    # The Adventures of Tom Sawyer by Mark Twain\n",
        "            1260,  # Jane Eyre by Charlotte Brontë\n",
        "            844,   # The Importance of Being Earnest by Oscar Wilde\n",
        "            2852   # The Hound of the Baskervilles by Arthur Conan Doyle\n",
        "        ]\n",
        "        return popular_fiction[:limit]\n",
        "\n",
        "# 사용 예시\n",
        "collector = GutenbergCollector()\n",
        "print(\"✓ Gutenberg Collector initialized\")\n",
        "print(f\"Sample book IDs: {collector.get_popular_books(limit=5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_sample"
      },
      "source": [
        "# 샘플 도서 다운로드\n",
        "sample_book_id = 1342  # Pride and Prejudice\n",
        "book_text = collector.download_book(sample_book_id)\n",
        "\n",
        "if book_text:\n",
        "    print(f\"✓ Successfully downloaded book {sample_book_id}\")\n",
        "    print(f\"Book length: {len(book_text)} characters\")\n",
        "    print(\"\\nFirst 500 characters:\")\n",
        "    print(book_text[:500])\n",
        "else:\n",
        "    print(\"Failed to download sample book\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 3. 데이터 전처리 (Data Preprocessing)\n",
        "\n",
        "### 3.1 텍스트 정제 (Text Cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "text_cleaning"
      },
      "source": [
        "class TextPreprocessor:",
        "    \"\"\"텍스트 전처리를 위한 클래스\"\"\"",
        "    ",
        "    def __init__(self):",
        "        self.nlp = nlp_en",
        "    ",
        "    def remove_gutenberg_header_footer(self, text):",
        "        \"\"\"",
        "        Project Gutenberg 헤더와 푸터 제거",
        "        ",
        "        Args:",
        "            text (str): 원본 텍스트",
        "        ",
        "        Returns:",
        "            str: 헤더/푸터가 제거된 텍스트",
        "        \"\"\"",
        "        # 시작 마커 찾기",
        "        start_markers = [",
        "            \"*** START OF THIS PROJECT GUTENBERG\",",
        "            \"*** START OF THE PROJECT GUTENBERG\",",
        "            \"***START OF THE PROJECT GUTENBERG\"",
        "        ]",
        "        ",
        "        # 종료 마커 찾기",
        "        end_markers = [",
        "            \"*** END OF THIS PROJECT GUTENBERG\",",
        "            \"*** END OF THE PROJECT GUTENBERG\",",
        "            \"***END OF THE PROJECT GUTENBERG\"",
        "        ]",
        "        ",
        "        start_idx = 0",
        "        for marker in start_markers:",
        "            idx = text.find(marker)",
        "            if idx != -1:",
        "                start_idx = text.find('\\n', idx) + 1",
        "                break",
        "        ",
        "        end_idx = len(text)",
        "        for marker in end_markers:",
        "            idx = text.find(marker)",
        "            if idx != -1:",
        "                end_idx = idx",
        "                break",
        "        ",
        "        return text[start_idx:end_idx].strip()",
        "    ",
        "    def clean_text(self, text):",
        "        \"\"\"",
        "        기본 텍스트 정제",
        "        - 과도한 공백 제거",
        "        - 특수 문자 정규화",
        "        - 줄바꿈 정규화",
        "        ",
        "        Args:",
        "            text (str): 원본 텍스트",
        "        ",
        "        Returns:",
        "            str: 정제된 텍스트",
        "        \"\"\"",
        "        # 여러 줄바꿈을 단일 줄바꿈으로",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)",
        "        ",
        "        # 여러 공백을 단일 공백으로",
        "        text = re.sub(r' +', ' ', text)",
        "        ",
        "        # 줄 시작/끝 공백 제거",
        "        text = '\\n'.join(line.strip() for line in text.split('\\n'))",
        "        ",
        "        return text.strip()",
        "    ",
        "    def split_into_chapters(self, text):",
        "        \"\"\"",
        "        텍스트를 챕터별로 분할 (개선된 버전)",
        "        ",
        "        다양한 챕터 형식 지원:",
        "        - \"CHAPTER I\", \"CHAPTER 1\", \"CHAPTER ONE\"",
        "        - \"Chapter I.\", \"Chapter 1.\", \"Chapter One.\"",
        "        - \"I.\", \"1.\", \"ONE.\"",
        "        - \"I\\n\", \"1\\n\" (숫자/로마자만 있는 줄)",
        "        ",
        "        Args:",
        "            text (str): 전체 텍스트",
        "        ",
        "        Returns:",
        "            list: 챕터별 텍스트 리스트",
        "        \"\"\"",
        "        # 다양한 챕터 패턴을 시도",
        "        patterns = [",
        "            # Pattern 1: \"CHAPTER I\" or \"Chapter 1\" with optional period/colon",
        "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)[\\s\\.:\\n]',",
        "            # Pattern 2: Roman numerals or numbers at start of line (standalone)",
        "            r'\\n\\s*([IVXLCDM]+|\\d+)\\.?\\s*\\n',",
        "            # Pattern 3: \"BOOK I\", \"PART I\", etc.",
        "            r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)[\\s\\.:\\n]',",
        "        ]",
        "        ",
        "        best_split = None",
        "        best_count = 0",
        "        ",
        "        # Try each pattern and use the one that gives the most reasonable splits",
        "        for pattern in patterns:",
        "            splits = re.split(pattern, text)",
        "            # Count potential chapters (excluding first element which is pre-chapter content)",
        "            chapter_count = (len(splits) - 1) // 3 if len(splits) > 1 else 0",
        "            ",
        "            # Prefer pattern that splits into 3-50 chapters (reasonable range)",
        "            if 3 <= chapter_count <= 50 and chapter_count > best_count:",
        "                best_count = chapter_count",
        "                best_split = (pattern, splits)",
        "        ",
        "        # If we found a good split, use it",
        "        if best_split:",
        "            pattern, chapters = best_split",
        "            result = []",
        "            ",
        "            # Skip first element (pre-chapter content) and process in groups of 3",
        "            for i in range(1, len(chapters), 3):",
        "                if i + 2 <= len(chapters):",
        "                    chapter_prefix = chapters[i].strip()",
        "                    chapter_number = chapters[i+1].strip()",
        "                    chapter_content = chapters[i+2].strip() if i+2 < len(chapters) else \"\"",
        "                    ",
        "                    # Create chapter title",
        "                    if chapter_prefix:",
        "                        chapter_title = f\"{chapter_prefix} {chapter_number}\"",
        "                    else:",
        "                        chapter_title = f\"Chapter {chapter_number}\"",
        "                    ",
        "                    if chapter_content:  # Only add if there's content",
        "                        result.append({",
        "                            'title': chapter_title,",
        "                            'content': chapter_content",
        "                        })",
        "            ",
        "            return result if result else [{'title': 'Full Text', 'content': text}]",
        "        ",
        "        # If no pattern worked, try manual line-by-line analysis",
        "        return self._fallback_chapter_split(text)",
        "    ",
        "    def _fallback_chapter_split(self, text):",
        "        \"\"\"",
        "        Fallback method: Analyze text line-by-line to find chapter markers",
        "        \"\"\"",
        "        lines = text.split('\\n')",
        "        chapters = []",
        "        current_chapter = None",
        "        current_content = []",
        "        ",
        "        for i, line in enumerate(lines):",
        "            line_stripped = line.strip()",
        "            line_upper = line_stripped.upper()",
        "            ",
        "            # Check if this line looks like a chapter header",
        "            is_chapter = False",
        "            chapter_title = None",
        "            ",
        "            # Very short line with numbers or roman numerals",
        "            if len(line_stripped) <= 30:",
        "                # Check for \"CHAPTER X\" format",
        "                if 'CHAPTER' in line_upper:",
        "                    is_chapter = True",
        "                    chapter_title = line_stripped",
        "                # Check for standalone numbers/roman numerals",
        "                elif re.match(r'^[IVXLCDM]+\\.?$', line_upper):",
        "                    is_chapter = True",
        "                    chapter_title = f\"Chapter {line_stripped}\"",
        "                elif re.match(r'^\\d+\\.?$', line_stripped):",
        "                    is_chapter = True",
        "                    chapter_title = f\"Chapter {line_stripped}\"",
        "                # Check for \"BOOK X\", \"PART X\"",
        "                elif any(word in line_upper for word in ['BOOK', 'PART']) and len(line_stripped.split()) <= 3:",
        "                    is_chapter = True",
        "                    chapter_title = line_stripped",
        "            ",
        "            if is_chapter and chapter_title:",
        "                # Save previous chapter if exists",
        "                if current_chapter and current_content:",
        "                    chapters.append({",
        "                        'title': current_chapter,",
        "                        'content': '\\n'.join(current_content).strip()",
        "                    })",
        "                ",
        "                # Start new chapter",
        "                current_chapter = chapter_title",
        "                current_content = []",
        "            else:",
        "                # Add to current chapter content",
        "                if line_stripped:  # Don't add empty lines at start",
        "                    current_content.append(line)",
        "        ",
        "        # Add last chapter",
        "        if current_chapter and current_content:",
        "            chapters.append({",
        "                'title': current_chapter,",
        "                'content': '\\n'.join(current_content).strip()",
        "            })",
        "        ",
        "        # If we found chapters, return them; otherwise return full text",
        "        if len(chapters) >= 2:  # At least 2 chapters to be valid",
        "            return chapters",
        "        else:",
        "            return [{'title': 'Full Text', 'content': text}]",
        "",
        "# 사용 예시",
        "preprocessor = TextPreprocessor()",
        "print(\"✓ Text Preprocessor initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apply_preprocessing"
      },
      "source": [
        "# 샘플 도서에 전처리 적용\n",
        "if book_text:\n",
        "    # 헤더/푸터 제거\n",
        "    cleaned_text = preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "    cleaned_text = preprocessor.clean_text(cleaned_text)\n",
        "    \n",
        "    print(f\"✓ Original length: {len(book_text)} characters\")\n",
        "    print(f\"✓ Cleaned length: {len(cleaned_text)} characters\")\n",
        "    print(f\"✓ Removed: {len(book_text) - len(cleaned_text)} characters\")\n",
        "    \n",
        "    # 챕터 분할\n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"\\n✓ Found {len(chapters)} chapters\")\n",
        "    \n",
        "    if chapters:\n",
        "        print(f\"\\nFirst chapter: {chapters[0]['title']}\")\n",
        "        print(f\"Content preview: {chapters[0]['content'][:300]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entity_extraction"
      },
      "source": [
        "### 3.2 핵심 요소 추출 (Entity Extraction)\n",
        "\n",
        "인물, 장소, 시간 등 핵심 요소를 추출하여 스크립트 변환에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entity_extractor"
      },
      "source": [
        "class EntityExtractor:\n",
        "    \"\"\"Named Entity Recognition을 통한 핵심 요소 추출 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "    \n",
        "    def extract_entities(self, text, max_length=1000000):\n",
        "        \"\"\"\n",
        "        텍스트에서 개체명 추출\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "            max_length (int): 처리할 최대 텍스트 길이\n",
        "        \n",
        "        Returns:\n",
        "            dict: 카테고리별 개체명 사전\n",
        "        \"\"\"\n",
        "        # SpaCy의 max_length 설정\n",
        "        self.nlp.max_length = max_length\n",
        "        \n",
        "        # 텍스트 분석\n",
        "        doc = self.nlp(text[:max_length])\n",
        "        \n",
        "        # 개체명 분류\n",
        "        entities = {\n",
        "            'PERSON': [],      # 인물\n",
        "            'GPE': [],          # 지정학적 개체 (도시, 국가 등)\n",
        "            'LOC': [],          # 위치\n",
        "            'DATE': [],         # 날짜\n",
        "            'TIME': [],         # 시간\n",
        "            'ORG': [],          # 조직\n",
        "            'EVENT': []         # 이벤트\n",
        "        }\n",
        "        \n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in entities:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "        \n",
        "        # 중복 제거 및 빈도수 계산\n",
        "        for key in entities:\n",
        "            entity_counts = defaultdict(int)\n",
        "            for entity in entities[key]:\n",
        "                entity_counts[entity] += 1\n",
        "            entities[key] = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    def get_main_characters(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        주요 인물 추출\n",
        "        \n",
        "        Args:\n",
        "            entities (dict): extract_entities의 결과\n",
        "            top_n (int): 반환할 주요 인물 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 주요 인물 리스트\n",
        "        \"\"\"\n",
        "        return entities['PERSON'][:top_n]\n",
        "    \n",
        "    def get_main_locations(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        주요 장소 추출\n",
        "        \n",
        "        Args:\n",
        "            entities (dict): extract_entities의 결과\n",
        "            top_n (int): 반환할 주요 장소 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 주요 장소 리스트\n",
        "        \"\"\"\n",
        "        locations = entities['GPE'] + entities['LOC']\n",
        "        # 빈도수로 재정렬\n",
        "        location_dict = defaultdict(int)\n",
        "        for loc, count in locations:\n",
        "            location_dict[loc] += count\n",
        "        return sorted(location_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# 사용 예시\n",
        "extractor = EntityExtractor()\n",
        "print(\"✓ Entity Extractor initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extract_sample_entities"
      },
      "source": [
        "# 샘플 챕터에서 개체명 추출\n",
        "if chapters:\n",
        "    # 첫 번째 챕터 분석\n",
        "    sample_chapter = chapters[0]['content']\n",
        "    entities = extractor.extract_entities(sample_chapter)\n",
        "    \n",
        "    print(\"✓ Entity extraction completed\\n\")\n",
        "    \n",
        "    # 주요 인물\n",
        "    main_characters = extractor.get_main_characters(entities, top_n=5)\n",
        "    print(\"Main Characters:\")\n",
        "    for char, count in main_characters:\n",
        "        print(f\"  - {char}: {count} mentions\")\n",
        "    \n",
        "    # 주요 장소\n",
        "    main_locations = extractor.get_main_locations(entities, top_n=5)\n",
        "    print(\"\\nMain Locations:\")\n",
        "    for loc, count in main_locations:\n",
        "        print(f\"  - {loc}: {count} mentions\")\n",
        "    \n",
        "    # 시간 정보\n",
        "    if entities['DATE']:\n",
        "        print(\"\\nTemporal Information:\")\n",
        "        for date, count in entities['DATE'][:5]:\n",
        "            print(f\"  - {date}: {count} mentions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "script_conversion"
      },
      "source": [
        "### 3.3 스크립트 변환을 위한 데이터 구조화\n",
        "\n",
        "도서 텍스트를 스크립트 학습에 적합한 형태로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "script_formatter"
      },
      "source": [
        "class ScriptFormatter:\n",
        "    \"\"\"도서 텍스트를 스크립트 형식으로 변환하는 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "    \n",
        "    def extract_dialogues(self, text):\n",
        "        \"\"\"\n",
        "        텍스트에서 대화문 추출\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            list: 대화문 리스트\n",
        "        \"\"\"\n",
        "        # 따옴표로 둘러싸인 대화문 추출\n",
        "        dialogue_pattern = r'[\"\\']([^\"\\']+)[\"\\']'\n",
        "        dialogues = re.findall(dialogue_pattern, text)\n",
        "        \n",
        "        # 짧은 대화 필터링 (3단어 이상)\n",
        "        dialogues = [d for d in dialogues if len(d.split()) >= 3]\n",
        "        \n",
        "        return dialogues\n",
        "    \n",
        "    def extract_narrative(self, text):\n",
        "        \"\"\"\n",
        "        서술 부분 추출 (대화가 아닌 부분)\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            str: 서술 텍스트\n",
        "        \"\"\"\n",
        "        # 대화문 제거\n",
        "        narrative = re.sub(r'[\"\\'][^\"\\']+[\"\\']', '', text)\n",
        "        \n",
        "        # 정제\n",
        "        narrative = re.sub(r' +', ' ', narrative)\n",
        "        narrative = re.sub(r'\\n\\s*\\n', '\\n\\n', narrative)\n",
        "        \n",
        "        return narrative.strip()\n",
        "    \n",
        "    def create_scene_structure(self, chapter_text, entities):\n",
        "        \"\"\"\n",
        "        챕터를 씬 구조로 변환\n",
        "        \n",
        "        Args:\n",
        "            chapter_text (str): 챕터 텍스트\n",
        "            entities (dict): 추출된 개체명\n",
        "        \n",
        "        Returns:\n",
        "            dict: 씬 구조 정보\n",
        "        \"\"\"\n",
        "        # 문장 단위로 분할\n",
        "        doc = self.nlp(chapter_text[:100000])  # 처리 속도를 위해 제한\n",
        "        sentences = [sent.text for sent in doc.sents]\n",
        "        \n",
        "        # 대화문과 서술 분리\n",
        "        dialogues = self.extract_dialogues(chapter_text)\n",
        "        narrative = self.extract_narrative(chapter_text)\n",
        "        \n",
        "        scene_data = {\n",
        "            'characters': [char for char, _ in entities.get('PERSON', [])[:5]],\n",
        "            'locations': [loc for loc, _ in (entities.get('GPE', []) + entities.get('LOC', []))[:3]],\n",
        "            'dialogues': dialogues[:10],\n",
        "            'narrative_sentences': sentences[:20],\n",
        "            'total_sentences': len(sentences),\n",
        "            'total_dialogues': len(dialogues)\n",
        "        }\n",
        "        \n",
        "        return scene_data\n",
        "\n",
        "# 사용 예시\n",
        "formatter = ScriptFormatter()\n",
        "print(\"✓ Script Formatter initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_scene"
      },
      "source": [
        "# 샘플 챕터를 씬 구조로 변환\n",
        "if chapters and entities:\n",
        "    scene_data = formatter.create_scene_structure(chapters[0]['content'], entities)\n",
        "    \n",
        "    print(\"✓ Scene structure created\\n\")\n",
        "    print(f\"Scene Information:\")\n",
        "    print(f\"  - Main Characters: {', '.join(scene_data['characters'])}\")\n",
        "    print(f\"  - Locations: {', '.join(scene_data['locations'])}\")\n",
        "    print(f\"  - Total Sentences: {scene_data['total_sentences']}\")\n",
        "    print(f\"  - Total Dialogues: {scene_data['total_dialogues']}\")\n",
        "    \n",
        "    print(\"\\nSample Dialogues:\")\n",
        "    for i, dialogue in enumerate(scene_data['dialogues'][:3], 1):\n",
        "        print(f\"  {i}. \\\"{dialogue}\\\"\")\n",
        "    \n",
        "    print(\"\\nSample Narrative:\")\n",
        "    for i, sentence in enumerate(scene_data['narrative_sentences'][:3], 1):\n",
        "        print(f\"  {i}. {sentence}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_creation"
      },
      "source": [
        "## 4. 학습 데이터셋 구축\n",
        "\n",
        "### 4.1 전체 파이프라인 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipeline"
      },
      "source": [
        "class BookToScriptPipeline:\n",
        "    \"\"\"도서에서 스크립트 학습 데이터까지의 전체 파이프라인\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.collector = GutenbergCollector()\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.extractor = EntityExtractor()\n",
        "        self.formatter = ScriptFormatter()\n",
        "        \n",
        "    def process_book(self, book_id):\n",
        "        \"\"\"\n",
        "        단일 도서 처리\n",
        "        \n",
        "        Args:\n",
        "            book_id (int): 도서 ID\n",
        "        \n",
        "        Returns:\n",
        "            dict: 처리된 데이터\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing book {book_id}...\")\n",
        "        \n",
        "        # 1. 도서 다운로드\n",
        "        book_text = self.collector.download_book(book_id)\n",
        "        if not book_text:\n",
        "            return None\n",
        "        print(f\"  ✓ Downloaded ({len(book_text)} chars)\")\n",
        "        \n",
        "        # 2. 전처리\n",
        "        cleaned_text = self.preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "        cleaned_text = self.preprocessor.clean_text(cleaned_text)\n",
        "        print(f\"  ✓ Cleaned ({len(cleaned_text)} chars)\")\n",
        "        \n",
        "        # 3. 챕터 분할\n",
        "        chapters = self.preprocessor.split_into_chapters(cleaned_text)\n",
        "        print(f\"  ✓ Split into {len(chapters)} chapters\")\n",
        "        \n",
        "        # 4. 각 챕터별 처리\n",
        "        processed_chapters = []\n",
        "        for i, chapter in enumerate(chapters[:5]):  # 처음 5개 챕터만 처리 (예시)\n",
        "            # 개체명 추출\n",
        "            entities = self.extractor.extract_entities(chapter['content'])\n",
        "            \n",
        "            # 씬 구조 생성\n",
        "            scene_data = self.formatter.create_scene_structure(chapter['content'], entities)\n",
        "            \n",
        "            processed_chapters.append({\n",
        "                'chapter_title': chapter['title'],\n",
        "                'chapter_number': i + 1,\n",
        "                'original_text': chapter['content'],\n",
        "                'entities': entities,\n",
        "                'scene_data': scene_data\n",
        "            })\n",
        "        \n",
        "        print(f\"  ✓ Processed {len(processed_chapters)} chapters\")\n",
        "        \n",
        "        return {\n",
        "            'book_id': book_id,\n",
        "            'total_length': len(cleaned_text),\n",
        "            'total_chapters': len(chapters),\n",
        "            'processed_chapters': processed_chapters\n",
        "        }\n",
        "    \n",
        "    def process_multiple_books(self, book_ids, output_dir='./processed_data'):\n",
        "        \"\"\"\n",
        "        여러 도서 처리 및 저장\n",
        "        \n",
        "        Args:\n",
        "            book_ids (list): 도서 ID 리스트\n",
        "            output_dir (str): 출력 디렉토리\n",
        "        \n",
        "        Returns:\n",
        "            list: 처리 결과 리스트\n",
        "        \"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        results = []\n",
        "        \n",
        "        for book_id in book_ids:\n",
        "            result = self.process_book(book_id)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "                \n",
        "                # JSON으로 저장\n",
        "                output_file = os.path.join(output_dir, f'book_{book_id}.json')\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "                print(f\"  ✓ Saved to {output_file}\")\n",
        "        \n",
        "        print(f\"\\n✓ Completed processing {len(results)} books\")\n",
        "        return results\n",
        "\n",
        "# 파이프라인 초기화\n",
        "pipeline = BookToScriptPipeline()\n",
        "print(\"✓ Pipeline initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_pipeline"
      },
      "source": [
        "# 샘플 도서 처리\n",
        "sample_books = [1342]  # Pride and Prejudice\n",
        "results = pipeline.process_multiple_books(sample_books, output_dir='./processed_data')\n",
        "\n",
        "print(\"\\n=== Processing Summary ===\")\n",
        "for result in results:\n",
        "    print(f\"\\nBook ID: {result['book_id']}\")\n",
        "    print(f\"  Total Length: {result['total_length']:,} characters\")\n",
        "    print(f\"  Total Chapters: {result['total_chapters']}\")\n",
        "    print(f\"  Processed Chapters: {len(result['processed_chapters'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 5. 성능 평가 지표 준비\n",
        "\n",
        "### 5.1 BLEU Score 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bleu_metric"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"모델 성능 평가를 위한 메트릭 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.smooth = SmoothingFunction()\n",
        "    \n",
        "    def calculate_bleu(self, reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "        \"\"\"\n",
        "        BLEU 점수 계산\n",
        "        \n",
        "        Args:\n",
        "            reference (str): 참조 텍스트\n",
        "            candidate (str): 생성된 텍스트\n",
        "            weights (tuple): n-gram 가중치\n",
        "        \n",
        "        Returns:\n",
        "            float: BLEU 점수\n",
        "        \"\"\"\n",
        "        # 토큰화\n",
        "        reference_tokens = reference.split()\n",
        "        candidate_tokens = candidate.split()\n",
        "        \n",
        "        # BLEU 계산 (smoothing 적용)\n",
        "        bleu_score = sentence_bleu(\n",
        "            [reference_tokens],\n",
        "            candidate_tokens,\n",
        "            weights=weights,\n",
        "            smoothing_function=self.smooth.method1\n",
        "        )\n",
        "        \n",
        "        return bleu_score\n",
        "    \n",
        "    def calculate_bleu_variants(self, reference, candidate):\n",
        "        \"\"\"\n",
        "        다양한 BLEU 변형 계산 (BLEU-1 ~ BLEU-4)\n",
        "        \n",
        "        Args:\n",
        "            reference (str): 참조 텍스트\n",
        "            candidate (str): 생성된 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            dict: BLEU 변형 점수\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'BLEU-1': self.calculate_bleu(reference, candidate, weights=(1, 0, 0, 0)),\n",
        "            'BLEU-2': self.calculate_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)),\n",
        "            'BLEU-3': self.calculate_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)),\n",
        "            'BLEU-4': self.calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        }\n",
        "\n",
        "# 평가 메트릭 초기화\n",
        "metrics = EvaluationMetrics()\n",
        "print(\"✓ Evaluation Metrics initialized\")\n",
        "\n",
        "# 테스트 예시\n",
        "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "candidate_text = \"The quick brown fox jumps over a lazy dog\"\n",
        "\n",
        "bleu_scores = metrics.calculate_bleu_variants(reference_text, candidate_text)\n",
        "print(\"\\nBLEU Score Examples:\")\n",
        "for metric, score in bleu_scores.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## 6. 다음 단계 및 계획\n",
        "\n",
        "### 완료된 작업\n",
        "- ✅ Project Gutenberg 데이터 수집 모듈\n",
        "- ✅ 텍스트 전처리 (노이즈 제거, 정제)\n",
        "- ✅ 챕터 분할 기능\n",
        "- ✅ 개체명 추출 (인물, 장소, 시간)\n",
        "- ✅ 스크립트 변환을 위한 데이터 구조화\n",
        "- ✅ BLEU 평가 지표 구현\n",
        "\n",
        "### 향후 작업 (10월 27일까지)\n",
        "1. **데이터 수집 확대**\n",
        "   - 더 많은 도서 다운로드 및 처리\n",
        "   - 다양한 장르 확보 (소설, 드라마, 미스터리 등)\n",
        "   - 국내 디지털 도서관 데이터 수집 방법 연구\n",
        "\n",
        "2. **전처리 고도화**\n",
        "   - 대화문과 서술 분리 정확도 향상\n",
        "   - 장면 전환 감지 알고리즘 개발\n",
        "   - 감정/톤 분석 추가\n",
        "\n",
        "3. **데이터셋 품질 검증**\n",
        "   - 결측치 및 이상치 검사\n",
        "   - 데이터 통계 분석\n",
        "   - 샘플 데이터 검토\n",
        "\n",
        "### 모델링 준비 (10월 29일 이후)\n",
        "- LLM 모델 선택 (GPT-2, T5, BART 등)\n",
        "- Fine-tuning 전략 수립\n",
        "- 학습 데이터 포맷 정의\n",
        "\n",
        "### 성능 평가 계획\n",
        "- BLEU Score: 텍스트 유사도 측정\n",
        "- FVD (선택): 비디오 품질 평가 (여유가 있을 경우)\n",
        "- CLIPScore (선택): 텍스트-이미지 유사도 (여유가 있을 경우)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_checkpoint"
      },
      "source": [
        "## 7. 진행 상황 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_progress"
      },
      "source": [
        "# 진행 상황 요약 저장\n",
        "progress_summary = {\n",
        "    'date': '2025-10-12',\n",
        "    'phase': 'Data Preprocessing',\n",
        "    'completed_tasks': [\n",
        "        'Data collection module (Project Gutenberg)',\n",
        "        'Text cleaning and preprocessing',\n",
        "        'Chapter segmentation',\n",
        "        'Entity extraction (characters, locations, time)',\n",
        "        'Script formatting structure',\n",
        "        'BLEU evaluation metric'\n",
        "    ],\n",
        "    'next_tasks': [\n",
        "        'Expand dataset collection',\n",
        "        'Enhance preprocessing accuracy',\n",
        "        'Data quality validation',\n",
        "        'Prepare for modeling phase'\n",
        "    ],\n",
        "    'deadline': '2025-10-27',\n",
        "    'team_notes': 'Following professor\\'s guidance - parallel work on dataset collection and framework setup'\n",
        "}\n",
        "\n",
        "# JSON으로 저장\n",
        "with open('progress_summary.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(progress_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✓ Progress summary saved to 'progress_summary.json'\")\n",
        "print(\"\\nCurrent Status:\")\n",
        "print(f\"  Phase: {progress_summary['phase']}\")\n",
        "print(f\"  Deadline: {progress_summary['deadline']}\")\n",
        "print(f\"  Completed: {len(progress_summary['completed_tasks'])} tasks\")\n",
        "print(f\"  Remaining: {len(progress_summary['next_tasks'])} tasks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chapter_splitting_examples"
      },
      "source": [
        "### 3.1.1 챕터 분할 예시 및 디버깅\n",
        "\n",
        "개선된 챕터 분할 알고리즘 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_chapter_splitting"
      },
      "source": [
        "# 챕터 분할 테스트 및 디버깅\n",
        "if 'book_text' in dir() and 'cleaned_text' in dir():\n",
        "    print(\"=== 챕터 분할 디버깅 ===\")\n",
        "    print(f\"\\n1. 원본 텍스트 샘플 (처음 1000자):\")\n",
        "    print(cleaned_text[:1000])\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # 챕터 패턴 수동 검색\n",
        "    print(\"2. 챕터 패턴 검색 결과:\")\n",
        "    \n",
        "    patterns_to_test = [\n",
        "        (\"CHAPTER/Chapter + 번호\", r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)'),\n",
        "        (\"로마자/숫자만\", r'\\n\\s*([IVXLCDM]+|\\d+)\\.?\\s*\\n'),\n",
        "        (\"BOOK/PART + 번호\", r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)'),\n",
        "    ]\n",
        "    \n",
        "    for pattern_name, pattern in patterns_to_test:\n",
        "        matches = re.findall(pattern, cleaned_text)\n",
        "        print(f\"  - {pattern_name}: {len(matches)}개 발견\")\n",
        "        if matches and len(matches) <= 10:\n",
        "            print(f\"    예시: {matches[:5]}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # 실제 챕터 분할 수행\n",
        "    print(\"3. 챕터 분할 결과:\")\n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"  총 {len(chapters)}개 챕터 발견\\n\")\n",
        "    \n",
        "    # 처음 3개 챕터 미리보기\n",
        "    for i, chapter in enumerate(chapters[:3], 1):\n",
        "        print(f\"  Chapter {i}:\")\n",
        "        print(f\"    제목: '{chapter['title']}'\")\n",
        "        print(f\"    길이: {len(chapter['content'])} 문자\")\n",
        "        print(f\"    내용 미리보기: {chapter['content'][:150]}...\")\n",
        "        print()\n",
        "    \n",
        "    # 챕터 길이 통계\n",
        "    if len(chapters) > 1:\n",
        "        chapter_lengths = [len(ch['content']) for ch in chapters]\n",
        "        print(f\"\\n4. 챕터 길이 통계:\")\n",
        "        print(f\"  - 평균: {sum(chapter_lengths)/len(chapter_lengths):.0f} 문자\")\n",
        "        print(f\"  - 최소: {min(chapter_lengths)} 문자\")\n",
        "        print(f\"  - 최대: {max(chapter_lengths)} 문자\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ 챕터 분할 분석 완료\")\n",
        "else:\n",
        "    print(\"먼저 샘플 도서를 다운로드하고 정제하세요.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}