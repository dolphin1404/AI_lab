{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolphin1404/AI_lab/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 도서-스크립트 변환 프로젝트: 데이터 수집 및 전처리\n",
        "\n",
        "## 프로젝트 개요\n",
        "- **목표**: 도서 텍스트를 비디오 스크립트 형식으로 변환하는 LLM 모델 개발\n",
        "- **단계**: 데이터 수집 → 전처리 → 모델링 → 성능평가\n",
        "- **일정**: 데이터 전처리 마감 - 10월 27일\n",
        "\n",
        "## 데이터 전처리 목표\n",
        "1. 공개 도서 아카이브에서 텍스트 데이터 수집 (Project Gutenberg, 국내 디지털 도서관)\n",
        "2. 노이즈 제거 및 텍스트 정제\n",
        "3. 핵심 요소 추출 (인물, 장소, 시간)\n",
        "4. 도서 문체 → 스크립트 문체 변환을 위한 학습 데이터셋 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. 환경 설정 및 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_packages"
      },
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install gutenberg requests beautifulsoup4 nltk spacy transformers datasets\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "import_libraries"
      },
      "source": [
        "# 라이브러리 임포트\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# NLTK 데이터 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# SpaCy 모델 로드\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_collection"
      },
      "source": [
        "## 2. 데이터 수집 (Data Collection)\n",
        "\n",
        "### 2.1 Project Gutenberg에서 도서 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gutenberg_collection"
      },
      "source": [
        "class GutenbergCollector:\n",
        "    \"\"\"Project Gutenberg에서 도서 데이터를 수집하는 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self, base_url=\"https://www.gutenberg.org\"):\n",
        "        self.base_url = base_url\n",
        "        \n",
        "    def download_book(self, book_id):\n",
        "        \"\"\"\n",
        "        특정 책 ID로 도서 텍스트 다운로드\n",
        "        \n",
        "        Args:\n",
        "            book_id (int): Gutenberg 도서 ID\n",
        "        \n",
        "        Returns:\n",
        "            str: 도서 텍스트 내용\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/files/{book_id}/{book_id}-0.txt\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.text\n",
        "            else:\n",
        "                print(f\"Failed to download book {book_id}: Status {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading book {book_id}: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def get_popular_books(self, genre=\"fiction\", limit=10):\n",
        "        \"\"\"\n",
        "        장르별 인기 도서 ID 리스트 반환\n",
        "        \n",
        "        Args:\n",
        "            genre (str): 도서 장르\n",
        "            limit (int): 다운로드할 최대 도서 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 도서 ID 리스트\n",
        "        \"\"\"\n",
        "        # 인기 고전 소설 ID 리스트 (예시)\n",
        "        popular_fiction = [\n",
        "            1342,  # Pride and Prejudice by Jane Austen\n",
        "            84,    # Frankenstein by Mary Shelley\n",
        "            98,    # A Tale of Two Cities by Charles Dickens\n",
        "            1661,  # Sherlock Holmes by Arthur Conan Doyle\n",
        "            2701,  # Moby Dick by Herman Melville\n",
        "            11,    # Alice's Adventures in Wonderland by Lewis Carroll\n",
        "            74,    # The Adventures of Tom Sawyer by Mark Twain\n",
        "            1260,  # Jane Eyre by Charlotte Brontë\n",
        "            844,   # The Importance of Being Earnest by Oscar Wilde\n",
        "            2852   # The Hound of the Baskervilles by Arthur Conan Doyle\n",
        "        ]\n",
        "        return popular_fiction[:limit]\n",
        "\n",
        "# 사용 예시\n",
        "collector = GutenbergCollector()\n",
        "print(\"✓ Gutenberg Collector initialized\")\n",
        "print(f\"Sample book IDs: {collector.get_popular_books(limit=5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_sample"
      },
      "source": [
        "# 샘플 도서 다운로드\n",
        "sample_book_id = 1342  # Pride and Prejudice\n",
        "book_text = collector.download_book(sample_book_id)\n",
        "\n",
        "if book_text:\n",
        "    print(f\"✓ Successfully downloaded book {sample_book_id}\")\n",
        "    print(f\"Book length: {len(book_text)} characters\")\n",
        "    print(\"\\nFirst 500 characters:\")\n",
        "    print(book_text[:500])\n",
        "else:\n",
        "    print(\"Failed to download sample book\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 3. 데이터 전처리 (Data Preprocessing)\n",
        "\n",
        "### 3.1 텍스트 정제 (Text Cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "text_cleaning"
      },
      "source": [
        "class TextPreprocessor:",
        "    \"\"\"텍스트 전처리를 위한 클래스\"\"\"",
        "    ",
        "    def __init__(self):",
        "        self.nlp = nlp_en",
        "    ",
        "    def remove_gutenberg_header_footer(self, text):",
        "        \"\"\"",
        "        Project Gutenberg 헤더와 푸터 제거",
        "        ",
        "        Args:",
        "            text (str): 원본 텍스트",
        "        ",
        "        Returns:",
        "            str: 헤더/푸터가 제거된 텍스트",
        "        \"\"\"",
        "        # 시작 마커 찾기",
        "        start_markers = [",
        "            \"*** START OF THIS PROJECT GUTENBERG\",",
        "            \"*** START OF THE PROJECT GUTENBERG\",",
        "            \"***START OF THE PROJECT GUTENBERG\"",
        "        ]",
        "        ",
        "        # 종료 마커 찾기",
        "        end_markers = [",
        "            \"*** END OF THIS PROJECT GUTENBERG\",",
        "            \"*** END OF THE PROJECT GUTENBERG\",",
        "            \"***END OF THE PROJECT GUTENBERG\"",
        "        ]",
        "        ",
        "        start_idx = 0",
        "        for marker in start_markers:",
        "            idx = text.find(marker)",
        "            if idx != -1:",
        "                start_idx = text.find('\\n', idx) + 1",
        "                break",
        "        ",
        "        end_idx = len(text)",
        "        for marker in end_markers:",
        "            idx = text.find(marker)",
        "            if idx != -1:",
        "                end_idx = idx",
        "                break",
        "        ",
        "        return text[start_idx:end_idx].strip()",
        "    ",
        "    def remove_table_of_contents(self, text):",
        "        \"\"\"",
        "        목차(Table of Contents) 섹션 제거 (v4 - 개선된 정확도)",
        "        ",
        "        개선 사항:",
        "        - 실제 챕터와 목차 항목 구분 강화",
        "        - Chapter I부터 정확히 보존",
        "        - 목차 패턴 더 정밀하게 감지",
        "        ",
        "        Args:",
        "            text (str): 원본 텍스트",
        "        ",
        "        Returns:",
        "            str: 목차가 제거된 텍스트",
        "        \"\"\"",
        "        lines = text.split('\\n')",
        "        result_lines = []",
        "        in_toc = False",
        "        toc_start_idx = -1",
        "        toc_line_count = 0",
        "        consecutive_heading_lines = 0",
        "        found_first_chapter = False",
        "        ",
        "        for i, line in enumerate(lines):",
        "            line_stripped = line.strip()",
        "            line_lower = line_stripped.lower()",
        "            ",
        "            # 목차 시작 감지 (더 정확한 패턴)",
        "            if not in_toc and not found_first_chapter:",
        "                # \"CONTENTS\" 단독으로 나오는 경우 (일반적인 목차 시작)",
        "                if line_stripped.upper() in ['CONTENTS', 'TABLE OF CONTENTS', 'LIST OF CHAPTERS']:",
        "                    in_toc = True",
        "                    toc_start_idx = i",
        "                    toc_line_count = 0",
        "                    continue",
        "                ",
        "                # \"Heading to Chapter\" 패턴 여러 줄 연속 (Pride and Prejudice 스타일)",
        "                if 'heading to chapter' in line_lower or 'heading to CHAPTER' in line_lower:",
        "                    consecutive_heading_lines += 1",
        "                    # 2줄 이상 연속으로 나오면 목차",
        "                    if consecutive_heading_lines >= 2:",
        "                        in_toc = True",
        "                        toc_start_idx = i - consecutive_heading_lines",
        "                    continue",
        "                else:",
        "                    if consecutive_heading_lines > 0 and consecutive_heading_lines < 2:",
        "                        # 1줄만 있었으면 실제 내용일 수 있음",
        "                        consecutive_heading_lines = 0",
        "            ",
        "            # 실제 챕터 시작 확인 (목차 종료 또는 첫 챕터 발견)",
        "            # 더 엄격한 조건: 줄이 짧고(80자 이하), 패턴이 명확하고, 다음 줄에 내용이 있어야 함",
        "            is_real_chapter = False",
        "            if len(line_stripped) <= 80:",
        "                # \"CHAPTER I\", \"Chapter 1\", \"CHAPTER ONE\" 등의 패턴",
        "                chapter_match = re.match(r'^\\s*(CHAPTER|Chapter)\\s+(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX|XXI|XXII|XXIII|XXIV|XXV|XXVI|XXVII|XXVIII|XXIX|XXX|XXXI|XXXII|XXXIII|XXXIV|XXXV|XXXVI|XXXVII|XXXVIII|XXXIX|XL|XLI|XLII|XLIII|XLIV|XLV|XLVI|XLVII|XLVIII|XLIX|L|LI|LII|LIII|LIV|LV|LVI|LVII|LVIII|LIX|LX|LXI|LXII|LXIII|LXIV|LXV|LXVI|LXVII|LXVIII|LXIX|LXX|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty)\\.?\\s*$', line_stripped)",
        "                ",
        "                if chapter_match:",
        "                    # 다음 몇 줄을 확인하여 실제 챕터 내용이 있는지 검증",
        "                    has_content_after = False",
        "                    for j in range(i+1, min(i+10, len(lines))):",
        "                        next_line = lines[j].strip()",
        "                        # 빈 줄이 아니고, 40자 이상의 실제 문장이 있으면 챕터로 간주",
        "                        if next_line and len(next_line) > 40:",
        "                            # 목차 키워드가 없어야 함",
        "                            if 'heading to' not in next_line.lower() and 'page' not in next_line.lower():",
        "                                has_content_after = True",
        "                                break",
        "                    ",
        "                    if has_content_after:",
        "                        is_real_chapter = True",
        "                        found_first_chapter = True",
        "                        ",
        "                        # 목차 중이었다면 목차 종료",
        "                        if in_toc and toc_line_count > 3:",
        "                            in_toc = False",
        "                            # 목차 부분 스킵 (이미 추가 안 함)",
        "            ",
        "            # 목차 중이면 줄 카운트만 증가하고 추가 안 함",
        "            if in_toc:",
        "                toc_line_count += 1",
        "                # 목차가 너무 길면 (150줄 이상) 종료",
        "                if toc_line_count > 150:",
        "                    in_toc = False",
        "                    result_lines.append(line)",
        "                # 실제 챕터를 만났으면 추가",
        "                elif is_real_chapter:",
        "                    in_toc = False",
        "                    result_lines.append(line)",
        "                continue",
        "            ",
        "            # 일반 라인 또는 실제 챕터 추가",
        "            result_lines.append(line)",
        "        ",
        "        return '\\n'.join(result_lines)",
        "    ",
        "    def clean_text(self, text):",
        "        \"\"\"",
        "        기본 텍스트 정제",
        "        - 과도한 공백 제거",
        "        - 특수 문자 정규화",
        "        - 줄바꿈 정규화",
        "        ",
        "        Args:",
        "            text (str): 원본 텍스트",
        "        ",
        "        Returns:",
        "            str: 정제된 텍스트",
        "        \"\"\"",
        "        # 여러 줄바꿈을 단일 줄바꿈으로",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)",
        "        ",
        "        # 여러 공백을 단일 공백으로",
        "        text = re.sub(r' +', ' ', text)",
        "        ",
        "        # 줄 시작/끝 공백 제거",
        "        text = '\\n'.join(line.strip() for line in text.split('\\n'))",
        "        ",
        "        return text.strip()",
        "    ",
        "    def split_into_chapters(self, text):",
        "        \"\"\"",
        "        텍스트를 챕터별로 분할 (개선된 버전 v2)",
        "        ",
        "        개선 사항:",
        "        - 목차(Table of Contents) 자동 제거",
        "        - \"Heading to Chapter\" 같은 목차 항목 필터링",
        "        - 실제 챕터 내용만 추출",
        "        - 더 정확한 챕터 감지",
        "        ",
        "        지원하는 형식:",
        "        - \"CHAPTER I\", \"CHAPTER 1\", \"CHAPTER ONE\"",
        "        - \"Chapter I.\", \"Chapter 1.\", \"Chapter One.\"",
        "        - 실제 챕터 제목이 있는 경우 (e.g., \"CHAPTER I. The Beginning\")",
        "        ",
        "        Args:",
        "            text (str): 전체 텍스트",
        "        ",
        "        Returns:",
        "            list: 챕터별 텍스트 리스트",
        "        \"\"\"",
        "        # 1단계: 목차 제거",
        "        text_without_toc = self.remove_table_of_contents(text)",
        "        ",
        "        # 2단계: 챕터 패턴 매칭",
        "        # 안전한 패턴 사용 (catastrophic backtracking 방지)",
        "        patterns = [",
        "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty|Twenty-one|Twenty-two|Twenty-three|Twenty-four|Twenty-five|Twenty-six|Twenty-seven|Twenty-eight|Twenty-nine|Thirty|Thirty-one|Thirty-two|Thirty-three|Thirty-four|Thirty-five|Thirty-six|Thirty-seven|Thirty-eight|Thirty-nine|Forty|Forty-one|Forty-two|Forty-three|Forty-four|Forty-five|Forty-six|Forty-seven|Forty-eight|Forty-nine|Fifty|Fifty-one|Fifty-two|Fifty-three|Fifty-four|Fifty-five|Fifty-six|Fifty-seven|Fifty-eight|Fifty-nine|Sixty|Sixty-one)(?:\\.\\s*|\\s+|\\n)',\n",
        "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve)(?:\\.\\s*|\\s+|\\n)',",
        "            # Pattern 2: \"BOOK I\", \"PART I\" ",
        "            r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)(?:\\.\\s*|\\s+|\\n)',",
        "        ]",
        "        ",
        "        best_split = None",
        "        best_count = 0",
        "        best_pattern_idx = -1",
        "        ",
        "        # 각 패턴 시도 (타임아웃 추가)",
        "        import time",
        "        for idx, pattern in enumerate(patterns):",
        "            try:",
        "                start_time = time.time()",
        "                splits = re.split(pattern, text_without_toc)",
        "                ",
        "                # 타임아웃 체크 (3초)",
        "                if time.time() - start_time > 3:",
        "                    print(f\"Warning: Pattern {idx} took too long, skipping\")",
        "                    continue",
        "                ",
        "                # 챕터 수 계산 (3개 요소가 1개 챕터: prefix, number, content)",
        "                chapter_count = (len(splits) - 1) // 3 if len(splits) > 1 else 0",
        "                ",
        "                # 합리적인 범위의 챕터 수 (3-150개)",
        "                if 2 <= chapter_count <= 150:\n",
        "                    # 챕터 평균 길이 확인 (너무 짧으면 목차일 가능성)",
        "                    total_length = sum(len(splits[i]) for i in range(2, len(splits), 3) if i < len(splits))",
        "                    avg_length = total_length / chapter_count if chapter_count > 0 else 0",
        "                    ",
        "                    # 평균 길이가 500자 이상이어야 실제 챕터",
        "                    if avg_length >= 300 and chapter_count > best_count:\n",
        "                        best_count = chapter_count",
        "                        best_split = (pattern, splits)",
        "                        best_pattern_idx = idx",
        "            except Exception as e:",
        "                print(f\"Warning: Pattern {idx} failed with error: {e}\")",
        "                continue",
        "        ",
        "        # 3단계: 챕터 추출",
        "        if best_split:",
        "            pattern, chapters = best_split",
        "            result = []",
        "            ",
        "            for i in range(1, len(chapters), 3):",
        "                if i + 2 <= len(chapters):",
        "                    chapter_prefix = chapters[i].strip()  # \"CHAPTER\" or \"Chapter\"",
        "                    chapter_number = chapters[i+1].strip()  # \"I\", \"1\", etc.",
        "                    chapter_content = chapters[i+2].strip() if i+2 < len(chapters) else \"\"",
        "                    ",
        "                    # 챕터 내용이 충분히 긴지 확인 (최소 200자)",
        "                    if len(chapter_content) < 200:",
        "                        continue",
        "                    ",
        "                    # 챕터 제목에서 실제 제목 추출",
        "                    content_lines = chapter_content.split('\\n', 2)",
        "                    chapter_title_suffix = \"\"",
        "                    ",
        "                    # 첫 줄이 짧으면 (60자 이하) 챕터 제목으로 간주",
        "                    if content_lines and len(content_lines[0]) <= 60 and content_lines[0].strip():",
        "                        chapter_title_suffix = content_lines[0].strip()",
        "                        # 제목을 제외한 나머지가 내용",
        "                        if len(content_lines) > 1:",
        "                            chapter_content = '\\n'.join(content_lines[1:]).strip()",
        "                    ",
        "                    # 최종 챕터 제목 생성",
        "                    if chapter_prefix:",
        "                        if chapter_title_suffix:",
        "                            chapter_title = f\"{chapter_prefix} {chapter_number}. {chapter_title_suffix}\"",
        "                        else:",
        "                            chapter_title = f\"{chapter_prefix} {chapter_number}\"",
        "                    else:",
        "                        chapter_title = f\"Chapter {chapter_number}\"",
        "                    ",
        "                    if chapter_content:  # 내용이 있을 때만 추가",
        "                        result.append({",
        "                            'title': chapter_title,",
        "                            'content': chapter_content",
        "                        })",
        "            ",
        "            # 결과 검증: 최소 2개 이상의 챕터",
        "            if len(result) >= 2:",
        "                return result",
        "        ",
        "        # 4단계: Fallback - 수동 라인 분석",
        "        return self._fallback_chapter_split(text_without_toc)",
        "    ",
        "    def _fallback_chapter_split(self, text):",
        "        \"\"\"",
        "        Fallback 방법: 줄 단위로 챕터 마커 찾기",
        "        목차 항목은 제외하고 실제 챕터만 추출",
        "        \"\"\"",
        "        lines = text.split('\\n')",
        "        chapters = []",
        "        current_chapter = None",
        "        current_content = []",
        "        skip_toc = True  # 처음 몇 챕터는 목차일 수 있으므로 건너뛰기",
        "        toc_count = 0",
        "        ",
        "        for i, line in enumerate(lines):",
        "            line_stripped = line.strip()",
        "            line_upper = line_stripped.upper()",
        "            ",
        "            # 목차 항목 건너뛰기",
        "            if 'HEADING TO' in line_upper or 'CONTENTS' in line_upper:",
        "                continue",
        "            ",
        "            # 챕터 헤더 감지",
        "            is_chapter = False",
        "            chapter_title = None",
        "            ",
        "            # 짧은 줄 (60자 이하)에서만 챕터 검사",
        "            if len(line_stripped) <= 60 and line_stripped:",
        "                # \"CHAPTER X\" 형식",
        "                chapter_match = re.match(r'^(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)', line_stripped)",
        "                if chapter_match:",
        "                    is_chapter = True",
        "                    chapter_title = line_stripped",
        "                # \"BOOK X\", \"PART X\" 형식",
        "                elif re.match(r'^(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)', line_stripped):",
        "                    is_chapter = True",
        "                    chapter_title = line_stripped",
        "            ",
        "            if is_chapter and chapter_title:",
        "                # 이전 챕터 저장",
        "                if current_chapter and current_content:",
        "                    content_text = '\\n'.join(current_content).strip()",
        "                    # 내용이 충분히 긴 경우만 (최소 500자)",
        "                    if len(content_text) >= 300:",
        "                        chapters.append({",
        "                            'title': current_chapter,",
        "                            'content': content_text",
        "                        })",
        "                        toc_count = 0",
        "                        skip_toc = False",
        "                    else:",
        "                        # 너무 짧으면 목차일 가능성",
        "                        toc_count += 1",
        "                        if toc_count > 10:",
        "                            skip_toc = False",
        "                ",
        "                # 새 챕터 시작",
        "                current_chapter = chapter_title",
        "                current_content = []",
        "            else:",
        "                # 현재 챕터에 내용 추가",
        "                if current_chapter:  # 챕터가 시작된 후에만",
        "                    current_content.append(line)",
        "        ",
        "        # 마지막 챕터 저장",
        "        if current_chapter and current_content:",
        "            content_text = '\\n'.join(current_content).strip()",
        "            if len(content_text) >= 500:",
        "                chapters.append({",
        "                    'title': current_chapter,",
        "                    'content': content_text",
        "                })",
        "        ",
        "        # 최소 2개 이상의 챕터가 있어야 유효",
        "        if len(chapters) >= 2:",
        "            return chapters",
        "        else:",
        "            return [{'title': 'Full Text', 'content': text}]",
        "",
        "# 사용 예시",
        "preprocessor = TextPreprocessor()",
        "print(\"✓ Text Preprocessor initialized (v2 - safe regex, no infinite loops)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apply_preprocessing"
      },
      "source": [
        "# 샘플 도서에 전처리 적용\n",
        "if book_text:\n",
        "    # 헤더/푸터 제거\n",
        "    cleaned_text = preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "    cleaned_text = preprocessor.clean_text(cleaned_text)\n",
        "    \n",
        "    print(f\"✓ Original length: {len(book_text)} characters\")\n",
        "    print(f\"✓ Cleaned length: {len(cleaned_text)} characters\")\n",
        "    print(f\"✓ Removed: {len(book_text) - len(cleaned_text)} characters\")\n",
        "    \n",
        "    # 챕터 분할\n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"\\n✓ Found {len(chapters)} chapters\")\n",
        "    \n",
        "    if chapters:\n",
        "        print(f\"\\nFirst chapter: {chapters[0]['title']}\")\n",
        "        print(f\"Content preview: {chapters[0]['content'][:300]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entity_extraction"
      },
      "source": [
        "### 3.2 핵심 요소 추출 (Entity Extraction)\n",
        "\n",
        "인물, 장소, 시간 등 핵심 요소를 추출하여 스크립트 변환에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entity_extractor"
      },
      "source": [
        "class EntityExtractor:\n",
        "    \"\"\"Named Entity Recognition을 통한 핵심 요소 추출 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "    \n",
        "    def extract_entities(self, text, max_length=1000000):\n",
        "        \"\"\"\n",
        "        텍스트에서 개체명 추출\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "            max_length (int): 처리할 최대 텍스트 길이\n",
        "        \n",
        "        Returns:\n",
        "            dict: 카테고리별 개체명 사전\n",
        "        \"\"\"\n",
        "        # SpaCy의 max_length 설정\n",
        "        self.nlp.max_length = max_length\n",
        "        \n",
        "        # 텍스트 분석\n",
        "        doc = self.nlp(text[:max_length])\n",
        "        \n",
        "        # 개체명 분류\n",
        "        entities = {\n",
        "            'PERSON': [],      # 인물\n",
        "            'GPE': [],          # 지정학적 개체 (도시, 국가 등)\n",
        "            'LOC': [],          # 위치\n",
        "            'DATE': [],         # 날짜\n",
        "            'TIME': [],         # 시간\n",
        "            'ORG': [],          # 조직\n",
        "            'EVENT': []         # 이벤트\n",
        "        }\n",
        "        \n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in entities:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "        \n",
        "        # 중복 제거 및 빈도수 계산\n",
        "        for key in entities:\n",
        "            entity_counts = defaultdict(int)\n",
        "            for entity in entities[key]:\n",
        "                entity_counts[entity] += 1\n",
        "            entities[key] = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    def get_main_characters(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        주요 인물 추출\n",
        "        \n",
        "        Args:\n",
        "            entities (dict): extract_entities의 결과\n",
        "            top_n (int): 반환할 주요 인물 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 주요 인물 리스트\n",
        "        \"\"\"\n",
        "        return entities['PERSON'][:top_n]\n",
        "    \n",
        "    def get_main_locations(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        주요 장소 추출\n",
        "        \n",
        "        Args:\n",
        "            entities (dict): extract_entities의 결과\n",
        "            top_n (int): 반환할 주요 장소 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 주요 장소 리스트\n",
        "        \"\"\"\n",
        "        locations = entities['GPE'] + entities['LOC']\n",
        "        # 빈도수로 재정렬\n",
        "        location_dict = defaultdict(int)\n",
        "        for loc, count in locations:\n",
        "            location_dict[loc] += count\n",
        "        return sorted(location_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# 사용 예시\n",
        "extractor = EntityExtractor()\n",
        "print(\"✓ Entity Extractor initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extract_sample_entities"
      },
      "source": [
        "# 샘플 챕터에서 개체명 추출\n",
        "if chapters:\n",
        "    # 첫 번째 챕터 분석\n",
        "    sample_chapter = chapters[0]['content']\n",
        "    entities = extractor.extract_entities(sample_chapter)\n",
        "    \n",
        "    print(\"✓ Entity extraction completed\\n\")\n",
        "    \n",
        "    # 주요 인물\n",
        "    main_characters = extractor.get_main_characters(entities, top_n=5)\n",
        "    print(\"Main Characters:\")\n",
        "    for char, count in main_characters:\n",
        "        print(f\"  - {char}: {count} mentions\")\n",
        "    \n",
        "    # 주요 장소\n",
        "    main_locations = extractor.get_main_locations(entities, top_n=5)\n",
        "    print(\"\\nMain Locations:\")\n",
        "    for loc, count in main_locations:\n",
        "        print(f\"  - {loc}: {count} mentions\")\n",
        "    \n",
        "    # 시간 정보\n",
        "    if entities['DATE']:\n",
        "        print(\"\\nTemporal Information:\")\n",
        "        for date, count in entities['DATE'][:5]:\n",
        "            print(f\"  - {date}: {count} mentions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "script_conversion"
      },
      "source": [
        "### 3.3 스크립트 변환을 위한 데이터 구조화\n",
        "\n",
        "도서 텍스트를 스크립트 학습에 적합한 형태로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "script_formatter"
      },
      "source": [
        "class ScriptFormatter:\n",
        "    \"\"\"도서 텍스트를 스크립트 형식으로 변환하는 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "    \n",
        "    def extract_dialogues(self, text):\n",
        "        \"\"\"\n",
        "        텍스트에서 대화문 추출\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            list: 대화문 리스트\n",
        "        \"\"\"\n",
        "        # 따옴표로 둘러싸인 대화문 추출\n",
        "        dialogue_pattern = r'[\"\\']([^\"\\']+)[\"\\']'\n",
        "        dialogues = re.findall(dialogue_pattern, text)\n",
        "        \n",
        "        # 짧은 대화 필터링 (3단어 이상)\n",
        "        dialogues = [d for d in dialogues if len(d.split()) >= 3]\n",
        "        \n",
        "        return dialogues\n",
        "    \n",
        "    def extract_narrative(self, text):\n",
        "        \"\"\"\n",
        "        서술 부분 추출 (대화가 아닌 부분)\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            str: 서술 텍스트\n",
        "        \"\"\"\n",
        "        # 대화문 제거\n",
        "        narrative = re.sub(r'[\"\\'][^\"\\']+[\"\\']', '', text)\n",
        "        \n",
        "        # 정제\n",
        "        narrative = re.sub(r' +', ' ', narrative)\n",
        "        narrative = re.sub(r'\\n\\s*\\n', '\\n\\n', narrative)\n",
        "        \n",
        "        return narrative.strip()\n",
        "    \n",
        "    def create_scene_structure(self, chapter_text, entities):\n",
        "        \"\"\"\n",
        "        챕터를 씬 구조로 변환\n",
        "        \n",
        "        Args:\n",
        "            chapter_text (str): 챕터 텍스트\n",
        "            entities (dict): 추출된 개체명\n",
        "        \n",
        "        Returns:\n",
        "            dict: 씬 구조 정보\n",
        "        \"\"\"\n",
        "        # 문장 단위로 분할\n",
        "        doc = self.nlp(chapter_text[:100000])  # 처리 속도를 위해 제한\n",
        "        sentences = [sent.text for sent in doc.sents]\n",
        "        \n",
        "        # 대화문과 서술 분리\n",
        "        dialogues = self.extract_dialogues(chapter_text)\n",
        "        narrative = self.extract_narrative(chapter_text)\n",
        "        \n",
        "        scene_data = {\n",
        "            'characters': [char for char, _ in entities.get('PERSON', [])[:5]],\n",
        "            'locations': [loc for loc, _ in (entities.get('GPE', []) + entities.get('LOC', []))[:3]],\n",
        "            'dialogues': dialogues[:10],\n",
        "            'narrative_sentences': sentences[:20],\n",
        "            'total_sentences': len(sentences),\n",
        "            'total_dialogues': len(dialogues)\n",
        "        }\n",
        "        \n",
        "        return scene_data\n",
        "\n",
        "# 사용 예시\n",
        "formatter = ScriptFormatter()\n",
        "print(\"✓ Script Formatter initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_scene"
      },
      "source": [
        "# 샘플 챕터를 씬 구조로 변환\n",
        "if chapters and entities:\n",
        "    scene_data = formatter.create_scene_structure(chapters[0]['content'], entities)\n",
        "    \n",
        "    print(\"✓ Scene structure created\\n\")\n",
        "    print(f\"Scene Information:\")\n",
        "    print(f\"  - Main Characters: {', '.join(scene_data['characters'])}\")\n",
        "    print(f\"  - Locations: {', '.join(scene_data['locations'])}\")\n",
        "    print(f\"  - Total Sentences: {scene_data['total_sentences']}\")\n",
        "    print(f\"  - Total Dialogues: {scene_data['total_dialogues']}\")\n",
        "    \n",
        "    print(\"\\nSample Dialogues:\")\n",
        "    for i, dialogue in enumerate(scene_data['dialogues'][:3], 1):\n",
        "        print(f\"  {i}. \\\"{dialogue}\\\"\")\n",
        "    \n",
        "    print(\"\\nSample Narrative:\")\n",
        "    for i, sentence in enumerate(scene_data['narrative_sentences'][:3], 1):\n",
        "        print(f\"  {i}. {sentence}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_creation"
      },
      "source": [
        "## 4. 학습 데이터셋 구축\n",
        "\n",
        "### 4.1 전체 파이프라인 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipeline"
      },
      "source": [
        "class BookToScriptPipeline:\n",
        "    \"\"\"도서에서 스크립트 학습 데이터까지의 전체 파이프라인\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.collector = GutenbergCollector()\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.extractor = EntityExtractor()\n",
        "        self.formatter = ScriptFormatter()\n",
        "        \n",
        "    def process_book(self, book_id):\n",
        "        \"\"\"\n",
        "        단일 도서 처리\n",
        "        \n",
        "        Args:\n",
        "            book_id (int): 도서 ID\n",
        "        \n",
        "        Returns:\n",
        "            dict: 처리된 데이터\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing book {book_id}...\")\n",
        "        \n",
        "        # 1. 도서 다운로드\n",
        "        book_text = self.collector.download_book(book_id)\n",
        "        if not book_text:\n",
        "            return None\n",
        "        print(f\"  ✓ Downloaded ({len(book_text)} chars)\")\n",
        "        \n",
        "        # 2. 전처리\n",
        "        cleaned_text = self.preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "        cleaned_text = self.preprocessor.clean_text(cleaned_text)\n",
        "        print(f\"  ✓ Cleaned ({len(cleaned_text)} chars)\")\n",
        "        \n",
        "        # 3. 챕터 분할\n",
        "        chapters = self.preprocessor.split_into_chapters(cleaned_text)\n",
        "        print(f\"  ✓ Split into {len(chapters)} chapters\")\n",
        "        \n",
        "        # 4. 각 챕터별 처리\n",
        "        processed_chapters = []\n",
        "        for i, chapter in enumerate(chapters[:5]):  # 처음 5개 챕터만 처리 (예시)\n",
        "            # 개체명 추출\n",
        "            entities = self.extractor.extract_entities(chapter['content'])\n",
        "            \n",
        "            # 씬 구조 생성\n",
        "            scene_data = self.formatter.create_scene_structure(chapter['content'], entities)\n",
        "            \n",
        "            processed_chapters.append({\n",
        "                'chapter_title': chapter['title'],\n",
        "                'chapter_number': i + 1,\n",
        "                'original_text': chapter['content'],\n",
        "                'entities': entities,\n",
        "                'scene_data': scene_data\n",
        "            })\n",
        "        \n",
        "        print(f\"  ✓ Processed {len(processed_chapters)} chapters\")\n",
        "        \n",
        "        return {\n",
        "            'book_id': book_id,\n",
        "            'total_length': len(cleaned_text),\n",
        "            'total_chapters': len(chapters),\n",
        "            'processed_chapters': processed_chapters\n",
        "        }\n",
        "    \n",
        "    def process_multiple_books(self, book_ids, output_dir='./processed_data'):\n",
        "        \"\"\"\n",
        "        여러 도서 처리 및 저장\n",
        "        \n",
        "        Args:\n",
        "            book_ids (list): 도서 ID 리스트\n",
        "            output_dir (str): 출력 디렉토리\n",
        "        \n",
        "        Returns:\n",
        "            list: 처리 결과 리스트\n",
        "        \"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        results = []\n",
        "        \n",
        "        for book_id in book_ids:\n",
        "            result = self.process_book(book_id)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "                \n",
        "                # JSON으로 저장\n",
        "                output_file = os.path.join(output_dir, f'book_{book_id}.json')\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "                print(f\"  ✓ Saved to {output_file}\")\n",
        "        \n",
        "        print(f\"\\n✓ Completed processing {len(results)} books\")\n",
        "        return results\n",
        "\n",
        "# 파이프라인 초기화\n",
        "pipeline = BookToScriptPipeline()\n",
        "print(\"✓ Pipeline initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_pipeline"
      },
      "source": [
        "# 샘플 도서 처리\n",
        "sample_books = [1342]  # Pride and Prejudice\n",
        "results = pipeline.process_multiple_books(sample_books, output_dir='./processed_data')\n",
        "\n",
        "print(\"\\n=== Processing Summary ===\")\n",
        "for result in results:\n",
        "    print(f\"\\nBook ID: {result['book_id']}\")\n",
        "    print(f\"  Total Length: {result['total_length']:,} characters\")\n",
        "    print(f\"  Total Chapters: {result['total_chapters']}\")\n",
        "    print(f\"  Processed Chapters: {len(result['processed_chapters'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 5. 성능 평가 지표 준비\n",
        "\n",
        "### 5.1 BLEU Score 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bleu_metric"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"모델 성능 평가를 위한 메트릭 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.smooth = SmoothingFunction()\n",
        "    \n",
        "    def calculate_bleu(self, reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "        \"\"\"\n",
        "        BLEU 점수 계산\n",
        "        \n",
        "        Args:\n",
        "            reference (str): 참조 텍스트\n",
        "            candidate (str): 생성된 텍스트\n",
        "            weights (tuple): n-gram 가중치\n",
        "        \n",
        "        Returns:\n",
        "            float: BLEU 점수\n",
        "        \"\"\"\n",
        "        # 토큰화\n",
        "        reference_tokens = reference.split()\n",
        "        candidate_tokens = candidate.split()\n",
        "        \n",
        "        # BLEU 계산 (smoothing 적용)\n",
        "        bleu_score = sentence_bleu(\n",
        "            [reference_tokens],\n",
        "            candidate_tokens,\n",
        "            weights=weights,\n",
        "            smoothing_function=self.smooth.method1\n",
        "        )\n",
        "        \n",
        "        return bleu_score\n",
        "    \n",
        "    def calculate_bleu_variants(self, reference, candidate):\n",
        "        \"\"\"\n",
        "        다양한 BLEU 변형 계산 (BLEU-1 ~ BLEU-4)\n",
        "        \n",
        "        Args:\n",
        "            reference (str): 참조 텍스트\n",
        "            candidate (str): 생성된 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            dict: BLEU 변형 점수\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'BLEU-1': self.calculate_bleu(reference, candidate, weights=(1, 0, 0, 0)),\n",
        "            'BLEU-2': self.calculate_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)),\n",
        "            'BLEU-3': self.calculate_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)),\n",
        "            'BLEU-4': self.calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        }\n",
        "\n",
        "# 평가 메트릭 초기화\n",
        "metrics = EvaluationMetrics()\n",
        "print(\"✓ Evaluation Metrics initialized\")\n",
        "\n",
        "# 테스트 예시\n",
        "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "candidate_text = \"The quick brown fox jumps over a lazy dog\"\n",
        "\n",
        "bleu_scores = metrics.calculate_bleu_variants(reference_text, candidate_text)\n",
        "print(\"\\nBLEU Score Examples:\")\n",
        "for metric, score in bleu_scores.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## 6. 다음 단계 및 계획\n",
        "\n",
        "### 완료된 작업\n",
        "- ✅ Project Gutenberg 데이터 수집 모듈\n",
        "- ✅ 텍스트 전처리 (노이즈 제거, 정제)\n",
        "- ✅ 챕터 분할 기능\n",
        "- ✅ 개체명 추출 (인물, 장소, 시간)\n",
        "- ✅ 스크립트 변환을 위한 데이터 구조화\n",
        "- ✅ BLEU 평가 지표 구현\n",
        "\n",
        "### 향후 작업 (10월 27일까지)\n",
        "1. **데이터 수집 확대**\n",
        "   - 더 많은 도서 다운로드 및 처리\n",
        "   - 다양한 장르 확보 (소설, 드라마, 미스터리 등)\n",
        "   - 국내 디지털 도서관 데이터 수집 방법 연구\n",
        "\n",
        "2. **전처리 고도화**\n",
        "   - 대화문과 서술 분리 정확도 향상\n",
        "   - 장면 전환 감지 알고리즘 개발\n",
        "   - 감정/톤 분석 추가\n",
        "\n",
        "3. **데이터셋 품질 검증**\n",
        "   - 결측치 및 이상치 검사\n",
        "   - 데이터 통계 분석\n",
        "   - 샘플 데이터 검토\n",
        "\n",
        "### 모델링 준비 (10월 29일 이후)\n",
        "- LLM 모델 선택 (GPT-2, T5, BART 등)\n",
        "- Fine-tuning 전략 수립\n",
        "- 학습 데이터 포맷 정의\n",
        "\n",
        "### 성능 평가 계획\n",
        "- BLEU Score: 텍스트 유사도 측정\n",
        "- FVD (선택): 비디오 품질 평가 (여유가 있을 경우)\n",
        "- CLIPScore (선택): 텍스트-이미지 유사도 (여유가 있을 경우)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_checkpoint"
      },
      "source": [
        "## 7. 진행 상황 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_progress"
      },
      "source": [
        "# 진행 상황 요약 저장\n",
        "progress_summary = {\n",
        "    'date': '2025-10-12',\n",
        "    'phase': 'Data Preprocessing',\n",
        "    'completed_tasks': [\n",
        "        'Data collection module (Project Gutenberg)',\n",
        "        'Text cleaning and preprocessing',\n",
        "        'Chapter segmentation',\n",
        "        'Entity extraction (characters, locations, time)',\n",
        "        'Script formatting structure',\n",
        "        'BLEU evaluation metric'\n",
        "    ],\n",
        "    'next_tasks': [\n",
        "        'Expand dataset collection',\n",
        "        'Enhance preprocessing accuracy',\n",
        "        'Data quality validation',\n",
        "        'Prepare for modeling phase'\n",
        "    ],\n",
        "    'deadline': '2025-10-27',\n",
        "    'team_notes': 'Following professor\\'s guidance - parallel work on dataset collection and framework setup'\n",
        "}\n",
        "\n",
        "# JSON으로 저장\n",
        "with open('progress_summary.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(progress_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✓ Progress summary saved to 'progress_summary.json'\")\n",
        "print(\"\\nCurrent Status:\")\n",
        "print(f\"  Phase: {progress_summary['phase']}\")\n",
        "print(f\"  Deadline: {progress_summary['deadline']}\")\n",
        "print(f\"  Completed: {len(progress_summary['completed_tasks'])} tasks\")\n",
        "print(f\"  Remaining: {len(progress_summary['next_tasks'])} tasks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chapter_splitting_examples"
      },
      "source": [
        "### 3.1.1 챕터 분할 예시 및 디버깅\n",
        "\n",
        "개선된 챕터 분할 알고리즘 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_chapter_splitting"
      },
      "source": [
        "# 챕터 분할 테스트 및 디버깅\n",
        "if 'book_text' in dir() and 'cleaned_text' in dir():\n",
        "    print(\"=== 챕터 분할 디버깅 ===\")\n",
        "    print(f\"\\n1. 원본 텍스트 샘플 (처음 1000자):\")\n",
        "    print(cleaned_text[:1000])\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # 챕터 패턴 수동 검색\n",
        "    print(\"2. 챕터 패턴 검색 결과:\")\n",
        "    \n",
        "    patterns_to_test = [\n",
        "        (\"CHAPTER/Chapter + 번호\", r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)'),\n",
        "        (\"로마자/숫자만\", r'\\n\\s*([IVXLCDM]+|\\d+)\\.?\\s*\\n'),\n",
        "        (\"BOOK/PART + 번호\", r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)'),\n",
        "    ]\n",
        "    \n",
        "    for pattern_name, pattern in patterns_to_test:\n",
        "        matches = re.findall(pattern, cleaned_text)\n",
        "        print(f\"  - {pattern_name}: {len(matches)}개 발견\")\n",
        "        if matches and len(matches) <= 10:\n",
        "            print(f\"    예시: {matches[:5]}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # 실제 챕터 분할 수행\n",
        "    print(\"3. 챕터 분할 결과:\")\n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"  총 {len(chapters)}개 챕터 발견\\n\")\n",
        "    \n",
        "    # 처음 3개 챕터 미리보기\n",
        "    for i, chapter in enumerate(chapters[:3], 1):\n",
        "        print(f\"  Chapter {i}:\")\n",
        "        print(f\"    제목: '{chapter['title']}'\")\n",
        "        print(f\"    길이: {len(chapter['content'])} 문자\")\n",
        "        print(f\"    내용 미리보기: {chapter['content'][:150]}...\")\n",
        "        print()\n",
        "    \n",
        "    # 챕터 길이 통계\n",
        "    if len(chapters) > 1:\n",
        "        chapter_lengths = [len(ch['content']) for ch in chapters]\n",
        "        print(f\"\\n4. 챕터 길이 통계:\")\n",
        "        print(f\"  - 평균: {sum(chapter_lengths)/len(chapter_lengths):.0f} 문자\")\n",
        "        print(f\"  - 최소: {min(chapter_lengths)} 문자\")\n",
        "        print(f\"  - 최대: {max(chapter_lengths)} 문자\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ 챕터 분할 분석 완료\")\n",
        "else:\n",
        "    print(\"먼저 샘플 도서를 다운로드하고 정제하세요.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}