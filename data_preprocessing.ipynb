{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolphin1404/AI_lab/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 도서-스크립트 변환 프로젝트: 데이터 수집 및 전처리\n",
        "\n",
        "## 프로젝트 개요\n",
        "- **목표**: 도서 텍스트를 비디오 스크립트 형식으로 변환하는 LLM 모델 개발\n",
        "- **단계**: 데이터 수집 → 전처리 → 모델링 → 성능평가\n",
        "- **일정**: 데이터 전처리 마감 - 10월 27일\n",
        "\n",
        "## 데이터 전처리 목표\n",
        "1. 공개 도서 아카이브에서 텍스트 데이터 수집 (Project Gutenberg, 국내 디지털 도서관)\n",
        "2. 노이즈 제거 및 텍스트 정제\n",
        "3. 핵심 요소 추출 (인물, 장소, 시간)\n",
        "4. 도서 문체 → 스크립트 문체 변환을 위한 학습 데이터셋 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. 환경 설정 및 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (4.13.4)\n",
            "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: spacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.8.7)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
            "Collecting datasets\n",
            "  Using cached datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting gutenberg\n",
            "  Using cached Gutenberg-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2025.9.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.3.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.12.0)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.3.1)\n",
            "Requirement already satisfied: httpx<1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Using cached aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "\u001b[33mWARNING: Package 'Gutenberg' has an invalid Requires-Python: Invalid specifier: '>=2.7.*'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting SPARQLWrapper>=1.8.2 (from gutenberg)\n",
            "  Using cached SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "  Using cached bsddb3-6.2.9.tar.gz (230 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[4 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m /tmp/pip-install-5ufuez97/bsddb3_e5f003514860494a87020d432ed117eb/setup3.py:94: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \u001b[31m   \u001b[0m   _ver_re = re.compile('^#\\s*define\\s+PY_BSDDB_VERSION\\s+\"(\\d+\\.\\d+\\.\\d+.*)\"')\n",
            "  \u001b[31m   \u001b[0m Can't find a local Berkeley DB installation.\n",
            "  \u001b[31m   \u001b[0m (suggestion: try the --berkeley-db=/path/to/bsddb option)\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting ko-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ko-core-news-sm\n",
            "Successfully installed ko-core-news-sm-3.8.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ko_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install requests beautifulsoup4 nltk spacy transformers datasets gutenberg \n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2025.9.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: spacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.3.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.12.0)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.1)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting ko-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ko_core_news_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리 임포트\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "!pip install nltk\n",
        "import nltk\n",
        "!pip install spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# NLTK 데이터 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# SpaCy 모델 로드\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_collection"
      },
      "source": [
        "## 2. 데이터 수집 (Data Collection)\n",
        "\n",
        "### 2.1 Project Gutenberg에서 도서 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gutenberg_collection"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Gutenberg Collector initialized\n",
            "Sample book IDs: [1342, 84, 98, 1661, 2701]\n"
          ]
        }
      ],
      "source": [
        "class GutenbergCollector:\n",
        "    \"\"\"Project Gutenberg에서 도서 데이터를 수집하는 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self, base_url=\"https://www.gutenberg.org\"):\n",
        "        self.base_url = base_url\n",
        "        \n",
        "    def download_book(self, book_id):\n",
        "        \"\"\"\n",
        "        특정 책 ID로 도서 텍스트 다운로드\n",
        "        \n",
        "        Args:\n",
        "            book_id (int): Gutenberg 도서 ID\n",
        "        \n",
        "        Returns:\n",
        "            str: 도서 텍스트 내용\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/files/{book_id}/{book_id}-0.txt\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.text\n",
        "            else:\n",
        "                print(f\"Failed to download book {book_id}: Status {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading book {book_id}: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def get_popular_books(self, genre=\"fiction\", limit=10):\n",
        "        \"\"\"\n",
        "        장르별 인기 도서 ID 리스트 반환\n",
        "        \n",
        "        Args:\n",
        "            genre (str): 도서 장르\n",
        "            limit (int): 다운로드할 최대 도서 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 도서 ID 리스트\n",
        "        \"\"\"\n",
        "        # 인기 고전 소설 ID 리스트 (예시)\n",
        "        popular_fiction = [\n",
        "            1342,  # Pride and Prejudice by Jane Austen\n",
        "            84,    # Frankenstein by Mary Shelley\n",
        "            98,    # A Tale of Two Cities by Charles Dickens\n",
        "            1661,  # Sherlock Holmes by Arthur Conan Doyle\n",
        "            2701,  # Moby Dick by Herman Melville\n",
        "            11,    # Alice's Adventures in Wonderland by Lewis Carroll\n",
        "            74,    # The Adventures of Tom Sawyer by Mark Twain\n",
        "            1260,  # Jane Eyre by Charlotte Brontë\n",
        "            844,   # The Importance of Being Earnest by Oscar Wilde\n",
        "            2852   # The Hound of the Baskervilles by Arthur Conan Doyle\n",
        "        ]\n",
        "        return popular_fiction[:limit]\n",
        "\n",
        "# 사용 예시\n",
        "collector = GutenbergCollector()\n",
        "print(\"✓ Gutenberg Collector initialized\")\n",
        "print(f\"Sample book IDs: {collector.get_popular_books(limit=5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "download_sample"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Successfully downloaded book 1342\n",
            "Book length: 743383 characters\n",
            "\n",
            "First 500 characters:\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            [Illustration:\n",
            "\n",
            "                             GEORGE ALLEN\n",
            "                               PUBLISHER\n",
            "\n",
            "                        156 CHARING CROSS ROAD\n",
            "                                LONDON\n",
            "\n",
            "                             RUSKIN HOUSE\n",
            "                                   ]\n",
            "\n",
            "                            [Illustration:\n",
            "\n",
            "               _Reading Jane’s Letters._      _Chap 34._\n",
            "                               \n"
          ]
        }
      ],
      "source": [
        "# 샘플 도서 다운로드\n",
        "sample_book_id = 1342  # Pride and Prejudice\n",
        "book_text = collector.download_book(sample_book_id)\n",
        "\n",
        "if book_text:\n",
        "    print(f\"✓ Successfully downloaded book {sample_book_id}\")\n",
        "    print(f\"Book length: {len(book_text)} characters\")\n",
        "    print(\"\\nFirst 500 characters:\")\n",
        "    print(book_text[:500])\n",
        "else:\n",
        "    print(\"Failed to download sample book\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 3. 데이터 전처리 (Data Preprocessing)\n",
        "\n",
        "### 3.1 텍스트 정제 (Text Cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "text_cleaning"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Text Preprocessor initialized (v2 - safe regex, no infinite loops)\n"
          ]
        }
      ],
      "source": [
        "class TextPreprocessor:\n",
        "    \"\"\"텍스트 전처리를 위한 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "    \n",
        "    def remove_gutenberg_header_footer(self, text):\n",
        "        \"\"\"\n",
        "        Project Gutenberg 헤더와 푸터 제거\n",
        "        \n",
        "        Args:\n",
        "            text (str): 원본 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            str: 헤더/푸터가 제거된 텍스트\n",
        "        \"\"\"\n",
        "        # 시작 마커 찾기\n",
        "        start_markers = [\n",
        "            \"*** START OF THIS PROJECT GUTENBERG\",\n",
        "            \"*** START OF THE PROJECT GUTENBERG\",\n",
        "            \"***START OF THE PROJECT GUTENBERG\"\n",
        "        ]\n",
        "        \n",
        "        # 종료 마커 찾기\n",
        "        end_markers = [\n",
        "            \"*** END OF THIS PROJECT GUTENBERG\",\n",
        "            \"*** END OF THE PROJECT GUTENBERG\",\n",
        "            \"***END OF THE PROJECT GUTENBERG\"\n",
        "        ]\n",
        "        \n",
        "        start_idx = 0\n",
        "        for marker in start_markers:\n",
        "            idx = text.find(marker)\n",
        "            if idx != -1:\n",
        "                start_idx = text.find('\\n', idx) + 1\n",
        "                break\n",
        "        \n",
        "        end_idx = len(text)\n",
        "        for marker in end_markers:\n",
        "            idx = text.find(marker)\n",
        "            if idx != -1:\n",
        "                end_idx = idx\n",
        "                break\n",
        "        \n",
        "        return text[start_idx:end_idx].strip()\n",
        "    \n",
        "    def remove_table_of_contents(self, text):\n",
        "        \"\"\"\n",
        "        목차(Table of Contents) 섹션 제거 (v4 - 개선된 정확도)\n",
        "        \n",
        "        개선 사항:\n",
        "        - 실제 챕터와 목차 항목 구분 강화\n",
        "        - Chapter I부터 정확히 보존\n",
        "        - 목차 패턴 더 정밀하게 감지\n",
        "        \n",
        "        Args:\n",
        "            text (str): 원본 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            str: 목차가 제거된 텍스트\n",
        "        \"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        result_lines = []\n",
        "        in_toc = False\n",
        "        toc_start_idx = -1\n",
        "        toc_line_count = 0\n",
        "        consecutive_heading_lines = 0\n",
        "        found_first_chapter = False\n",
        "        \n",
        "        for i, line in enumerate(lines):\n",
        "            line_stripped = line.strip()\n",
        "            line_lower = line_stripped.lower()\n",
        "            \n",
        "            # 목차 시작 감지 (더 정확한 패턴)\n",
        "            if not in_toc and not found_first_chapter:\n",
        "                # \"CONTENTS\" 단독으로 나오는 경우 (일반적인 목차 시작)\n",
        "                if line_stripped.upper() in ['CONTENTS', 'TABLE OF CONTENTS', 'LIST OF CHAPTERS']:\n",
        "                    in_toc = True\n",
        "                    toc_start_idx = i\n",
        "                    toc_line_count = 0\n",
        "                    continue\n",
        "                \n",
        "                # \"Heading to Chapter\" 패턴 여러 줄 연속 (Pride and Prejudice 스타일)\n",
        "                if 'heading to chapter' in line_lower or 'heading to CHAPTER' in line_lower:\n",
        "                    consecutive_heading_lines += 1\n",
        "                    # 2줄 이상 연속으로 나오면 목차\n",
        "                    if consecutive_heading_lines >= 2:\n",
        "                        in_toc = True\n",
        "                        toc_start_idx = i - consecutive_heading_lines\n",
        "                    continue\n",
        "                else:\n",
        "                    if consecutive_heading_lines > 0 and consecutive_heading_lines < 2:\n",
        "                        # 1줄만 있었으면 실제 내용일 수 있음\n",
        "                        consecutive_heading_lines = 0\n",
        "            \n",
        "            # 실제 챕터 시작 확인 (목차 종료 또는 첫 챕터 발견)\n",
        "            # 더 엄격한 조건: 줄이 짧고(80자 이하), 패턴이 명확하고, 다음 줄에 내용이 있어야 함\n",
        "            is_real_chapter = False\n",
        "            if len(line_stripped) <= 80:\n",
        "                # \"CHAPTER I\", \"Chapter 1\", \"CHAPTER ONE\" 등의 패턴\n",
        "                chapter_match = re.match(r'^\\s*(CHAPTER|Chapter)\\s+(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX|XXI|XXII|XXIII|XXIV|XXV|XXVI|XXVII|XXVIII|XXIX|XXX|XXXI|XXXII|XXXIII|XXXIV|XXXV|XXXVI|XXXVII|XXXVIII|XXXIX|XL|XLI|XLII|XLIII|XLIV|XLV|XLVI|XLVII|XLVIII|XLIX|L|LI|LII|LIII|LIV|LV|LVI|LVII|LVIII|LIX|LX|LXI|LXII|LXIII|LXIV|LXV|LXVI|LXVII|LXVIII|LXIX|LXX|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty)\\.?\\s*$', line_stripped)\n",
        "                \n",
        "                if chapter_match:\n",
        "                    # 다음 몇 줄을 확인하여 실제 챕터 내용이 있는지 검증\n",
        "                    has_content_after = False\n",
        "                    for j in range(i+1, min(i+10, len(lines))):\n",
        "                        next_line = lines[j].strip()\n",
        "                        # 빈 줄이 아니고, 40자 이상의 실제 문장이 있으면 챕터로 간주\n",
        "                        if next_line and len(next_line) > 40:\n",
        "                            # 목차 키워드가 없어야 함\n",
        "                            if 'heading to' not in next_line.lower() and 'page' not in next_line.lower():\n",
        "                                has_content_after = True\n",
        "                                break\n",
        "                    \n",
        "                    if has_content_after:\n",
        "                        is_real_chapter = True\n",
        "                        found_first_chapter = True\n",
        "                        \n",
        "                        # 목차 중이었다면 목차 종료\n",
        "                        if in_toc and toc_line_count > 3:\n",
        "                            in_toc = False\n",
        "                            # 목차 부분 스킵 (이미 추가 안 함)\n",
        "            \n",
        "            # 목차 중이면 줄 카운트만 증가하고 추가 안 함\n",
        "            if in_toc:\n",
        "                toc_line_count += 1\n",
        "                # 목차가 너무 길면 (150줄 이상) 종료\n",
        "                if toc_line_count > 150:\n",
        "                    in_toc = False\n",
        "                    result_lines.append(line)\n",
        "                # 실제 챕터를 만났으면 추가\n",
        "                elif is_real_chapter:\n",
        "                    in_toc = False\n",
        "                    result_lines.append(line)\n",
        "                continue\n",
        "            \n",
        "            # 일반 라인 또는 실제 챕터 추가\n",
        "            result_lines.append(line)\n",
        "        \n",
        "        return '\\n'.join(result_lines)\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        기본 텍스트 정제\n",
        "        - 과도한 공백 제거\n",
        "        - 특수 문자 정규화\n",
        "        - 줄바꿈 정규화\n",
        "        \n",
        "        Args:\n",
        "            text (str): 원본 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            str: 정제된 텍스트\n",
        "        \"\"\"\n",
        "        # 여러 줄바꿈을 단일 줄바꿈으로\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "        \n",
        "        # 여러 공백을 단일 공백으로\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        \n",
        "        # 줄 시작/끝 공백 제거\n",
        "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    def split_into_chapters(self, text):\n",
        "        \"\"\"\n",
        "        텍스트를 챕터별로 분할 (개선된 버전 v2)\n",
        "        \n",
        "        개선 사항:\n",
        "        - 목차(Table of Contents) 자동 제거\n",
        "        - \"Heading to Chapter\" 같은 목차 항목 필터링\n",
        "        - 실제 챕터 내용만 추출\n",
        "        - 더 정확한 챕터 감지\n",
        "        \n",
        "        지원하는 형식:\n",
        "        - \"CHAPTER I\", \"CHAPTER 1\", \"CHAPTER ONE\"\n",
        "        - \"Chapter I.\", \"Chapter 1.\", \"Chapter One.\"\n",
        "        - 실제 챕터 제목이 있는 경우 (e.g., \"CHAPTER I. The Beginning\")\n",
        "        \n",
        "        Args:\n",
        "            text (str): 전체 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            list: 챕터별 텍스트 리스트\n",
        "        \"\"\"\n",
        "        # 1단계: 목차 제거\n",
        "        text_without_toc = self.remove_table_of_contents(text)\n",
        "        \n",
        "        # 2단계: 챕터 패턴 매칭\n",
        "        # 안전한 패턴 사용 (catastrophic backtracking 방지)\n",
        "        patterns = [\n",
        "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty|Twenty-one|Twenty-two|Twenty-three|Twenty-four|Twenty-five|Twenty-six|Twenty-seven|Twenty-eight|Twenty-nine|Thirty|Thirty-one|Thirty-two|Thirty-three|Thirty-four|Thirty-five|Thirty-six|Thirty-seven|Thirty-eight|Thirty-nine|Forty|Forty-one|Forty-two|Forty-three|Forty-four|Forty-five|Forty-six|Forty-seven|Forty-eight|Forty-nine|Fifty|Fifty-one|Fifty-two|Fifty-three|Fifty-four|Fifty-five|Fifty-six|Fifty-seven|Fifty-eight|Fifty-nine|Sixty|Sixty-one)(?:\\.\\s*|\\s+|\\n)',\n",
        "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve)(?:\\.\\s*|\\s+|\\n)',\n",
        "            # Pattern 2: \"BOOK I\", \"PART I\" \n",
        "            r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)(?:\\.\\s*|\\s+|\\n)',\n",
        "        ]\n",
        "        \n",
        "        best_split = None\n",
        "        best_count = 0\n",
        "        best_pattern_idx = -1\n",
        "        \n",
        "        # 각 패턴 시도 (타임아웃 추가)\n",
        "        import time\n",
        "        for idx, pattern in enumerate(patterns):\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                splits = re.split(pattern, text_without_toc)\n",
        "                \n",
        "                # 타임아웃 체크 (3초)\n",
        "                if time.time() - start_time > 3:\n",
        "                    print(f\"Warning: Pattern {idx} took too long, skipping\")\n",
        "                    continue\n",
        "                \n",
        "                # 챕터 수 계산 (3개 요소가 1개 챕터: prefix, number, content)\n",
        "                chapter_count = (len(splits) - 1) // 3 if len(splits) > 1 else 0\n",
        "                \n",
        "                # 합리적인 범위의 챕터 수 (3-150개)\n",
        "                if 2 <= chapter_count <= 150:\n",
        "                    # 챕터 평균 길이 확인 (너무 짧으면 목차일 가능성)\n",
        "                    total_length = sum(len(splits[i]) for i in range(2, len(splits), 3) if i < len(splits))\n",
        "                    avg_length = total_length / chapter_count if chapter_count > 0 else 0\n",
        "                    \n",
        "                    # 평균 길이가 500자 이상이어야 실제 챕터\n",
        "                    if avg_length >= 300 and chapter_count > best_count:\n",
        "                        best_count = chapter_count\n",
        "                        best_split = (pattern, splits)\n",
        "                        best_pattern_idx = idx\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Pattern {idx} failed with error: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # 3단계: 챕터 추출\n",
        "        if best_split:\n",
        "            pattern, chapters = best_split\n",
        "            result = []\n",
        "            \n",
        "            for i in range(1, len(chapters), 3):\n",
        "                if i + 2 <= len(chapters):\n",
        "                    chapter_prefix = chapters[i].strip()  # \"CHAPTER\" or \"Chapter\"\n",
        "                    chapter_number = chapters[i+1].strip()  # \"I\", \"1\", etc.\n",
        "                    chapter_content = chapters[i+2].strip() if i+2 < len(chapters) else \"\"\n",
        "                    \n",
        "                    # 챕터 내용이 충분히 긴지 확인 (최소 200자)\n",
        "                    if len(chapter_content) < 200:\n",
        "                        continue\n",
        "                    \n",
        "                    # 챕터 제목에서 실제 제목 추출\n",
        "                    content_lines = chapter_content.split('\\n', 2)\n",
        "                    chapter_title_suffix = \"\"\n",
        "                    \n",
        "                    # 첫 줄이 짧으면 (60자 이하) 챕터 제목으로 간주\n",
        "                    if content_lines and len(content_lines[0]) <= 60 and content_lines[0].strip():\n",
        "                        chapter_title_suffix = content_lines[0].strip()\n",
        "                        # 제목을 제외한 나머지가 내용\n",
        "                        if len(content_lines) > 1:\n",
        "                            chapter_content = '\\n'.join(content_lines[1:]).strip()\n",
        "                    \n",
        "                    # 최종 챕터 제목 생성\n",
        "                    if chapter_prefix:\n",
        "                        if chapter_title_suffix:\n",
        "                            chapter_title = f\"{chapter_prefix} {chapter_number}. {chapter_title_suffix}\"\n",
        "                        else:\n",
        "                            chapter_title = f\"{chapter_prefix} {chapter_number}\"\n",
        "                    else:\n",
        "                        chapter_title = f\"Chapter {chapter_number}\"\n",
        "                    \n",
        "                    if chapter_content:  # 내용이 있을 때만 추가\n",
        "                        result.append({\n",
        "                            'title': chapter_title,\n",
        "                            'content': chapter_content\n",
        "                        })\n",
        "            \n",
        "            # 결과 검증: 최소 2개 이상의 챕터\n",
        "            if len(result) >= 2:\n",
        "                return result\n",
        "        \n",
        "        # 4단계: Fallback - 수동 라인 분석\n",
        "        return self._fallback_chapter_split(text_without_toc)\n",
        "    \n",
        "    def _fallback_chapter_split(self, text):\n",
        "        \"\"\"\n",
        "        Fallback 방법: 줄 단위로 챕터 마커 찾기\n",
        "        목차 항목은 제외하고 실제 챕터만 추출\n",
        "        \"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        chapters = []\n",
        "        current_chapter = None\n",
        "        current_content = []\n",
        "        skip_toc = True  # 처음 몇 챕터는 목차일 수 있으므로 건너뛰기\n",
        "        toc_count = 0\n",
        "        \n",
        "        for i, line in enumerate(lines):\n",
        "            line_stripped = line.strip()\n",
        "            line_upper = line_stripped.upper()\n",
        "            \n",
        "            # 목차 항목 건너뛰기\n",
        "            if 'HEADING TO' in line_upper or 'CONTENTS' in line_upper:\n",
        "                continue\n",
        "            \n",
        "            # 챕터 헤더 감지\n",
        "            is_chapter = False\n",
        "            chapter_title = None\n",
        "            \n",
        "            # 짧은 줄 (60자 이하)에서만 챕터 검사\n",
        "            if len(line_stripped) <= 60 and line_stripped:\n",
        "                # \"CHAPTER X\" 형식\n",
        "                chapter_match = re.match(r'^(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)', line_stripped)\n",
        "                if chapter_match:\n",
        "                    is_chapter = True\n",
        "                    chapter_title = line_stripped\n",
        "                # \"BOOK X\", \"PART X\" 형식\n",
        "                elif re.match(r'^(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)', line_stripped):\n",
        "                    is_chapter = True\n",
        "                    chapter_title = line_stripped\n",
        "            \n",
        "            if is_chapter and chapter_title:\n",
        "                # 이전 챕터 저장\n",
        "                if current_chapter and current_content:\n",
        "                    content_text = '\\n'.join(current_content).strip()\n",
        "                    # 내용이 충분히 긴 경우만 (최소 500자)\n",
        "                    if len(content_text) >= 300:\n",
        "                        chapters.append({\n",
        "                            'title': current_chapter,\n",
        "                            'content': content_text\n",
        "                        })\n",
        "                        toc_count = 0\n",
        "                        skip_toc = False\n",
        "                    else:\n",
        "                        # 너무 짧으면 목차일 가능성\n",
        "                        toc_count += 1\n",
        "                        if toc_count > 10:\n",
        "                            skip_toc = False\n",
        "                \n",
        "                # 새 챕터 시작\n",
        "                current_chapter = chapter_title\n",
        "                current_content = []\n",
        "            else:\n",
        "                # 현재 챕터에 내용 추가\n",
        "                if current_chapter:  # 챕터가 시작된 후에만\n",
        "                    current_content.append(line)\n",
        "        \n",
        "        # 마지막 챕터 저장\n",
        "        if current_chapter and current_content:\n",
        "            content_text = '\\n'.join(current_content).strip()\n",
        "            if len(content_text) >= 500:\n",
        "                chapters.append({\n",
        "                    'title': current_chapter,\n",
        "                    'content': content_text\n",
        "                })\n",
        "        \n",
        "        # 최소 2개 이상의 챕터가 있어야 유효\n",
        "        if len(chapters) >= 2:\n",
        "            return chapters\n",
        "        else:\n",
        "            return [{'title': 'Full Text', 'content': text}]\n",
        "\n",
        "# 사용 예시\n",
        "preprocessor = TextPreprocessor()\n",
        "print(\"✓ Text Preprocessor initialized (v2 - safe regex, no infinite loops)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "apply_preprocessing"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Original length: 743383 characters\n",
            "✓ Cleaned length: 720973 characters\n",
            "✓ Removed: 22410 characters\n",
            "\n",
            "✓ Found 61 chapters\n",
            "\n",
            "First chapter: Chapter I.]\n",
            "Content preview: It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "\n",
            "However little known the feelings or views of such a man may be on his\n",
            "first entering a neighbourhood, this truth is so well fixed in the minds\n",
            "of the surrounding families, that he i...\n"
          ]
        }
      ],
      "source": [
        "# 샘플 도서에 전처리 적용\n",
        "if book_text:\n",
        "    # 헤더/푸터 제거\n",
        "    cleaned_text = preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "    cleaned_text = preprocessor.clean_text(cleaned_text)\n",
        "    \n",
        "    print(f\"✓ Original length: {len(book_text)} characters\")\n",
        "    print(f\"✓ Cleaned length: {len(cleaned_text)} characters\")\n",
        "    print(f\"✓ Removed: {len(book_text) - len(cleaned_text)} characters\")\n",
        "    \n",
        "    # 챕터 분할\n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"\\n✓ Found {len(chapters)} chapters\")\n",
        "    \n",
        "    if chapters:\n",
        "        print(f\"\\nFirst chapter: {chapters[0]['title']}\")\n",
        "        print(f\"Content preview: {chapters[0]['content'][:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entity_extraction"
      },
      "source": [
        "### 3.2 핵심 요소 추출 (Entity Extraction)\n",
        "\n",
        "인물, 장소, 시간 등 핵심 요소를 추출하여 스크립트 변환에 활용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "entity_extractor"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Entity Extractor initialized\n"
          ]
        }
      ],
      "source": [
        "class EntityExtractor:\n",
        "    \"\"\"Named Entity Recognition을 통한 핵심 요소 추출 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "    \n",
        "    def extract_entities(self, text, max_length=1000000):\n",
        "        \"\"\"\n",
        "        텍스트에서 개체명 추출\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "            max_length (int): 처리할 최대 텍스트 길이\n",
        "        \n",
        "        Returns:\n",
        "            dict: 카테고리별 개체명 사전\n",
        "        \"\"\"\n",
        "        # SpaCy의 max_length 설정\n",
        "        self.nlp.max_length = max_length\n",
        "        \n",
        "        # 텍스트 분석\n",
        "        doc = self.nlp(text[:max_length])\n",
        "        \n",
        "        # 개체명 분류\n",
        "        entities = {\n",
        "            'PERSON': [],      # 인물\n",
        "            'GPE': [],          # 지정학적 개체 (도시, 국가 등)\n",
        "            'LOC': [],          # 위치\n",
        "            'DATE': [],         # 날짜\n",
        "            'TIME': [],         # 시간\n",
        "            'ORG': [],          # 조직\n",
        "            'EVENT': []         # 이벤트\n",
        "        }\n",
        "        \n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in entities:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "        \n",
        "        # 중복 제거 및 빈도수 계산\n",
        "        for key in entities:\n",
        "            entity_counts = defaultdict(int)\n",
        "            for entity in entities[key]:\n",
        "                entity_counts[entity] += 1\n",
        "            entities[key] = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    def get_main_characters(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        주요 인물 추출\n",
        "        \n",
        "        Args:\n",
        "            entities (dict): extract_entities의 결과\n",
        "            top_n (int): 반환할 주요 인물 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 주요 인물 리스트\n",
        "        \"\"\"\n",
        "        return entities['PERSON'][:top_n]\n",
        "    \n",
        "    def get_main_locations(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        주요 장소 추출\n",
        "        \n",
        "        Args:\n",
        "            entities (dict): extract_entities의 결과\n",
        "            top_n (int): 반환할 주요 장소 수\n",
        "        \n",
        "        Returns:\n",
        "            list: 주요 장소 리스트\n",
        "        \"\"\"\n",
        "        locations = entities['GPE'] + entities['LOC']\n",
        "        # 빈도수로 재정렬\n",
        "        location_dict = defaultdict(int)\n",
        "        for loc, count in locations:\n",
        "            location_dict[loc] += count\n",
        "        return sorted(location_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# 사용 예시\n",
        "extractor = EntityExtractor()\n",
        "print(\"✓ Entity Extractor initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "extract_sample_entities"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Entity extraction completed\n",
            "\n",
            "Main Characters:\n",
            "  - Bennet: 6 mentions\n",
            "  - Bingley: 4 mentions\n",
            "  - George Allen: 3 mentions\n",
            "  - Lizzy: 3 mentions\n",
            "  - Long: 2 mentions\n",
            "\n",
            "Main Locations:\n",
            "  - England: 1 mentions\n",
            "  - Lydia: 1 mentions\n",
            "\n",
            "Temporal Information:\n",
            "  - 1894: 3 mentions\n",
            "  - one day: 1 mentions\n",
            "  - Monday: 1 mentions\n",
            "  - the end of next week: 1 mentions\n",
            "  - five thousand a year: 1 mentions\n"
          ]
        }
      ],
      "source": [
        "# 샘플 챕터에서 개체명 추출\n",
        "if chapters:\n",
        "    # 첫 번째 챕터 분석\n",
        "    sample_chapter = chapters[0]['content']\n",
        "    entities = extractor.extract_entities(sample_chapter)\n",
        "    \n",
        "    print(\"✓ Entity extraction completed\\n\")\n",
        "    \n",
        "    # 주요 인물\n",
        "    main_characters = extractor.get_main_characters(entities, top_n=5)\n",
        "    print(\"Main Characters:\")\n",
        "    for char, count in main_characters:\n",
        "        print(f\"  - {char}: {count} mentions\")\n",
        "    \n",
        "    # 주요 장소\n",
        "    main_locations = extractor.get_main_locations(entities, top_n=5)\n",
        "    print(\"\\nMain Locations:\")\n",
        "    for loc, count in main_locations:\n",
        "        print(f\"  - {loc}: {count} mentions\")\n",
        "    \n",
        "    # 시간 정보\n",
        "    if entities['DATE']:\n",
        "        print(\"\\nTemporal Information:\")\n",
        "        for date, count in entities['DATE'][:5]:\n",
        "            print(f\"  - {date}: {count} mentions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "script_conversion"
      },
      "source": [
        "### 3.3 스크립트 변환을 위한 데이터 구조화\n",
        "\n",
        "도서 텍스트를 스크립트 학습에 적합한 형태로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "script_formatter"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Script Formatter initialized\n"
          ]
        }
      ],
      "source": [
        "class ScriptFormatter:\n",
        "    \"\"\"도서 텍스트를 스크립트 형식으로 변환하는 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "    \n",
        "    def extract_dialogues(self, text):\n",
        "        \"\"\"\n",
        "        텍스트에서 대화문 추출\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            list: 대화문 리스트\n",
        "        \"\"\"\n",
        "        # 따옴표로 둘러싸인 대화문 추출\n",
        "        dialogue_pattern = r'[\"\\']([^\"\\']+)[\"\\']'\n",
        "        dialogues = re.findall(dialogue_pattern, text)\n",
        "        \n",
        "        # 짧은 대화 필터링 (3단어 이상)\n",
        "        dialogues = [d for d in dialogues if len(d.split()) >= 3]\n",
        "        \n",
        "        return dialogues\n",
        "    \n",
        "    def extract_narrative(self, text):\n",
        "        \"\"\"\n",
        "        서술 부분 추출 (대화가 아닌 부분)\n",
        "        \n",
        "        Args:\n",
        "            text (str): 분석할 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            str: 서술 텍스트\n",
        "        \"\"\"\n",
        "        # 대화문 제거\n",
        "        narrative = re.sub(r'[\"\\'][^\"\\']+[\"\\']', '', text)\n",
        "        \n",
        "        # 정제\n",
        "        narrative = re.sub(r' +', ' ', narrative)\n",
        "        narrative = re.sub(r'\\n\\s*\\n', '\\n\\n', narrative)\n",
        "        \n",
        "        return narrative.strip()\n",
        "    \n",
        "    def create_scene_structure(self, chapter_text, entities):\n",
        "        \"\"\"\n",
        "        챕터를 씬 구조로 변환\n",
        "        \n",
        "        Args:\n",
        "            chapter_text (str): 챕터 텍스트\n",
        "            entities (dict): 추출된 개체명\n",
        "        \n",
        "        Returns:\n",
        "            dict: 씬 구조 정보\n",
        "        \"\"\"\n",
        "        # 문장 단위로 분할\n",
        "        doc = self.nlp(chapter_text[:100000])  # 처리 속도를 위해 제한\n",
        "        sentences = [sent.text for sent in doc.sents]\n",
        "        \n",
        "        # 대화문과 서술 분리\n",
        "        dialogues = self.extract_dialogues(chapter_text)\n",
        "        narrative = self.extract_narrative(chapter_text)\n",
        "        \n",
        "        scene_data = {\n",
        "            'characters': [char for char, _ in entities.get('PERSON', [])[:5]],\n",
        "            'locations': [loc for loc, _ in (entities.get('GPE', []) + entities.get('LOC', []))[:3]],\n",
        "            'dialogues': dialogues[:10],\n",
        "            'narrative_sentences': sentences[:20],\n",
        "            'total_sentences': len(sentences),\n",
        "            'total_dialogues': len(dialogues)\n",
        "        }\n",
        "        \n",
        "        return scene_data\n",
        "\n",
        "# 사용 예시\n",
        "formatter = ScriptFormatter()\n",
        "print(\"✓ Script Formatter initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "create_scene"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Scene structure created\n",
            "\n",
            "Scene Information:\n",
            "  - Main Characters: Bennet, Bingley, George Allen, Lizzy, Long\n",
            "  - Locations: England, Lydia\n",
            "  - Total Sentences: 53\n",
            "  - Total Dialogues: 0\n",
            "\n",
            "Sample Dialogues:\n",
            "\n",
            "Sample Narrative:\n",
            "  1. It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "\n",
            "\n",
            "  2. However little known the feelings or views of such a man may be on his\n",
            "first entering a neighbourhood, this truth is so well fixed in the minds\n",
            "of the surrounding families, that he is considered as the rightful\n",
            "property of some one or other of their daughters.\n",
            "\n",
            "\n",
            "  3. “My dear Mr. Bennet,” said his lady to him one day, “have you heard that\n",
            "Netherfield Park is let at last?”\n",
            "\n",
            "Mr. Bennet replied that he had not.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 샘플 챕터를 씬 구조로 변환\n",
        "if chapters and entities:\n",
        "    scene_data = formatter.create_scene_structure(chapters[0]['content'], entities)\n",
        "    \n",
        "    print(\"✓ Scene structure created\\n\")\n",
        "    print(f\"Scene Information:\")\n",
        "    print(f\"  - Main Characters: {', '.join(scene_data['characters'])}\")\n",
        "    print(f\"  - Locations: {', '.join(scene_data['locations'])}\")\n",
        "    print(f\"  - Total Sentences: {scene_data['total_sentences']}\")\n",
        "    print(f\"  - Total Dialogues: {scene_data['total_dialogues']}\")\n",
        "    \n",
        "    print(\"\\nSample Dialogues:\")\n",
        "    for i, dialogue in enumerate(scene_data['dialogues'][:3], 1):\n",
        "        print(f\"  {i}. \\\"{dialogue}\\\"\")\n",
        "    \n",
        "    print(\"\\nSample Narrative:\")\n",
        "    for i, sentence in enumerate(scene_data['narrative_sentences'][:3], 1):\n",
        "        print(f\"  {i}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_creation"
      },
      "source": [
        "## 4. 학습 데이터셋 구축\n",
        "\n",
        "### 4.1 전체 파이프라인 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pipeline"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Pipeline initialized\n"
          ]
        }
      ],
      "source": [
        "class BookToScriptPipeline:\n",
        "    \"\"\"도서에서 스크립트 학습 데이터까지의 전체 파이프라인\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.collector = GutenbergCollector()\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.extractor = EntityExtractor()\n",
        "        self.formatter = ScriptFormatter()\n",
        "        \n",
        "    def process_book(self, book_id):\n",
        "        \"\"\"\n",
        "        단일 도서 처리\n",
        "        \n",
        "        Args:\n",
        "            book_id (int): 도서 ID\n",
        "        \n",
        "        Returns:\n",
        "            dict: 처리된 데이터\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing book {book_id}...\")\n",
        "        \n",
        "        # 1. 도서 다운로드\n",
        "        book_text = self.collector.download_book(book_id)\n",
        "        if not book_text:\n",
        "            return None\n",
        "        print(f\"  ✓ Downloaded ({len(book_text)} chars)\")\n",
        "        \n",
        "        # 2. 전처리\n",
        "        cleaned_text = self.preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "        cleaned_text = self.preprocessor.clean_text(cleaned_text)\n",
        "        print(f\"  ✓ Cleaned ({len(cleaned_text)} chars)\")\n",
        "        \n",
        "        # 3. 챕터 분할\n",
        "        chapters = self.preprocessor.split_into_chapters(cleaned_text)\n",
        "        print(f\"  ✓ Split into {len(chapters)} chapters\")\n",
        "        \n",
        "        # 4. 각 챕터별 처리\n",
        "        processed_chapters = []\n",
        "        for i, chapter in enumerate(chapters[:5]):  # 처음 5개 챕터만 처리 (예시)\n",
        "            # 개체명 추출\n",
        "            entities = self.extractor.extract_entities(chapter['content'])\n",
        "            \n",
        "            # 씬 구조 생성\n",
        "            scene_data = self.formatter.create_scene_structure(chapter['content'], entities)\n",
        "            \n",
        "            processed_chapters.append({\n",
        "                'chapter_title': chapter['title'],\n",
        "                'chapter_number': i + 1,\n",
        "                'original_text': chapter['content'],\n",
        "                'entities': entities,\n",
        "                'scene_data': scene_data\n",
        "            })\n",
        "        \n",
        "        print(f\"  ✓ Processed {len(processed_chapters)} chapters\")\n",
        "        \n",
        "        return {\n",
        "            'book_id': book_id,\n",
        "            'total_length': len(cleaned_text),\n",
        "            'total_chapters': len(chapters),\n",
        "            'processed_chapters': processed_chapters\n",
        "        }\n",
        "    \n",
        "    def process_multiple_books(self, book_ids, output_dir='./processed_data'):\n",
        "        \"\"\"\n",
        "        여러 도서 처리 및 저장\n",
        "        \n",
        "        Args:\n",
        "            book_ids (list): 도서 ID 리스트\n",
        "            output_dir (str): 출력 디렉토리\n",
        "        \n",
        "        Returns:\n",
        "            list: 처리 결과 리스트\n",
        "        \"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        results = []\n",
        "        \n",
        "        for book_id in book_ids:\n",
        "            result = self.process_book(book_id)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "                \n",
        "                # JSON으로 저장\n",
        "                output_file = os.path.join(output_dir, f'book_{book_id}.json')\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "                print(f\"  ✓ Saved to {output_file}\")\n",
        "        \n",
        "        print(f\"\\n✓ Completed processing {len(results)} books\")\n",
        "        return results\n",
        "\n",
        "# 파이프라인 초기화\n",
        "pipeline = BookToScriptPipeline()\n",
        "print(\"✓ Pipeline initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "run_pipeline"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing book 1342...\n",
            "  ✓ Downloaded (743383 chars)\n",
            "  ✓ Cleaned (720973 chars)\n",
            "  ✓ Split into 61 chapters\n",
            "  ✓ Processed 5 chapters\n",
            "  ✓ Saved to ./processed_data/book_1342.json\n",
            "\n",
            "✓ Completed processing 1 books\n",
            "\n",
            "=== Processing Summary ===\n",
            "\n",
            "Book ID: 1342\n",
            "  Total Length: 720,973 characters\n",
            "  Total Chapters: 61\n",
            "  Processed Chapters: 5\n"
          ]
        }
      ],
      "source": [
        "# 샘플 도서 처리\n",
        "sample_books = [1342]  # Pride and Prejudice\n",
        "results = pipeline.process_multiple_books(sample_books, output_dir='./processed_data')\n",
        "\n",
        "print(\"\\n=== Processing Summary ===\")\n",
        "for result in results:\n",
        "    print(f\"\\nBook ID: {result['book_id']}\")\n",
        "    print(f\"  Total Length: {result['total_length']:,} characters\")\n",
        "    print(f\"  Total Chapters: {result['total_chapters']}\")\n",
        "    print(f\"  Processed Chapters: {len(result['processed_chapters'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 5. 성능 평가 지표 준비\n",
        "\n",
        "### 5.1 BLEU Score 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bleu_metric"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Evaluation Metrics initialized\n",
            "\n",
            "BLEU Score Examples:\n",
            "  BLEU-1: 0.8889\n",
            "  BLEU-2: 0.8165\n",
            "  BLEU-3: 0.7273\n",
            "  BLEU-4: 0.6606\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"모델 성능 평가를 위한 메트릭 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.smooth = SmoothingFunction()\n",
        "    \n",
        "    def calculate_bleu(self, reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "        \"\"\"\n",
        "        BLEU 점수 계산\n",
        "        \n",
        "        Args:\n",
        "            reference (str): 참조 텍스트\n",
        "            candidate (str): 생성된 텍스트\n",
        "            weights (tuple): n-gram 가중치\n",
        "        \n",
        "        Returns:\n",
        "            float: BLEU 점수\n",
        "        \"\"\"\n",
        "        # 토큰화\n",
        "        reference_tokens = reference.split()\n",
        "        candidate_tokens = candidate.split()\n",
        "        \n",
        "        # BLEU 계산 (smoothing 적용)\n",
        "        bleu_score = sentence_bleu(\n",
        "            [reference_tokens],\n",
        "            candidate_tokens,\n",
        "            weights=weights,\n",
        "            smoothing_function=self.smooth.method1\n",
        "        )\n",
        "        \n",
        "        return bleu_score\n",
        "    \n",
        "    def calculate_bleu_variants(self, reference, candidate):\n",
        "        \"\"\"\n",
        "        다양한 BLEU 변형 계산 (BLEU-1 ~ BLEU-4)\n",
        "        \n",
        "        Args:\n",
        "            reference (str): 참조 텍스트\n",
        "            candidate (str): 생성된 텍스트\n",
        "        \n",
        "        Returns:\n",
        "            dict: BLEU 변형 점수\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'BLEU-1': self.calculate_bleu(reference, candidate, weights=(1, 0, 0, 0)),\n",
        "            'BLEU-2': self.calculate_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)),\n",
        "            'BLEU-3': self.calculate_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)),\n",
        "            'BLEU-4': self.calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        }\n",
        "\n",
        "# 평가 메트릭 초기화\n",
        "metrics = EvaluationMetrics()\n",
        "print(\"✓ Evaluation Metrics initialized\")\n",
        "\n",
        "# 테스트 예시\n",
        "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "candidate_text = \"The quick brown fox jumps over a lazy dog\"\n",
        "\n",
        "bleu_scores = metrics.calculate_bleu_variants(reference_text, candidate_text)\n",
        "print(\"\\nBLEU Score Examples:\")\n",
        "for metric, score in bleu_scores.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## 6. 다음 단계 및 계획\n",
        "\n",
        "### 완료된 작업\n",
        "- ✅ Project Gutenberg 데이터 수집 모듈\n",
        "- ✅ 텍스트 전처리 (노이즈 제거, 정제)\n",
        "- ✅ 챕터 분할 기능\n",
        "- ✅ 개체명 추출 (인물, 장소, 시간)\n",
        "- ✅ 스크립트 변환을 위한 데이터 구조화\n",
        "- ✅ BLEU 평가 지표 구현\n",
        "\n",
        "### 향후 작업 (10월 27일까지)\n",
        "1. **데이터 수집 확대**\n",
        "   - 더 많은 도서 다운로드 및 처리\n",
        "   - 다양한 장르 확보 (소설, 드라마, 미스터리 등)\n",
        "   - 국내 디지털 도서관 데이터 수집 방법 연구\n",
        "\n",
        "2. **전처리 고도화**\n",
        "   - 대화문과 서술 분리 정확도 향상\n",
        "   - 장면 전환 감지 알고리즘 개발\n",
        "   - 감정/톤 분석 추가\n",
        "\n",
        "3. **데이터셋 품질 검증**\n",
        "   - 결측치 및 이상치 검사\n",
        "   - 데이터 통계 분석\n",
        "   - 샘플 데이터 검토\n",
        "\n",
        "### 모델링 준비 (10월 29일 이후)\n",
        "- LLM 모델 선택 (GPT-2, T5, BART 등)\n",
        "- Fine-tuning 전략 수립\n",
        "- 학습 데이터 포맷 정의\n",
        "\n",
        "### 성능 평가 계획\n",
        "- BLEU Score: 텍스트 유사도 측정\n",
        "- FVD (선택): 비디오 품질 평가 (여유가 있을 경우)\n",
        "- CLIPScore (선택): 텍스트-이미지 유사도 (여유가 있을 경우)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_checkpoint"
      },
      "source": [
        "## 7. 진행 상황 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "save_progress"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Progress summary saved to 'progress_summary.json'\n",
            "\n",
            "Current Status:\n",
            "  Phase: Data Preprocessing\n",
            "  Deadline: 2025-10-27\n",
            "  Completed: 6 tasks\n",
            "  Remaining: 4 tasks\n"
          ]
        }
      ],
      "source": [
        "# 진행 상황 요약 저장\n",
        "progress_summary = {\n",
        "    'date': '2025-10-12',\n",
        "    'phase': 'Data Preprocessing',\n",
        "    'completed_tasks': [\n",
        "        'Data collection module (Project Gutenberg)',\n",
        "        'Text cleaning and preprocessing',\n",
        "        'Chapter segmentation',\n",
        "        'Entity extraction (characters, locations, time)',\n",
        "        'Script formatting structure',\n",
        "        'BLEU evaluation metric'\n",
        "    ],\n",
        "    'next_tasks': [\n",
        "        'Expand dataset collection',\n",
        "        'Enhance preprocessing accuracy',\n",
        "        'Data quality validation',\n",
        "        'Prepare for modeling phase'\n",
        "    ],\n",
        "    'deadline': '2025-10-27',\n",
        "    'team_notes': 'Following professor\\'s guidance - parallel work on dataset collection and framework setup'\n",
        "}\n",
        "\n",
        "# JSON으로 저장\n",
        "with open('progress_summary.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(progress_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✓ Progress summary saved to 'progress_summary.json'\")\n",
        "print(\"\\nCurrent Status:\")\n",
        "print(f\"  Phase: {progress_summary['phase']}\")\n",
        "print(f\"  Deadline: {progress_summary['deadline']}\")\n",
        "print(f\"  Completed: {len(progress_summary['completed_tasks'])} tasks\")\n",
        "print(f\"  Remaining: {len(progress_summary['next_tasks'])} tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chapter_splitting_examples"
      },
      "source": [
        "### 3.1.1 챕터 분할 예시 및 디버깅\n",
        "\n",
        "개선된 챕터 분할 알고리즘 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "test_chapter_splitting"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 챕터 분할 디버깅 ===\n",
            "\n",
            "1. 원본 텍스트 샘플 (처음 1000자):\n",
            "[Illustration:\n",
            "\n",
            "GEORGE ALLEN\n",
            "PUBLISHER\n",
            "\n",
            "156 CHARING CROSS ROAD\n",
            "LONDON\n",
            "\n",
            "RUSKIN HOUSE\n",
            "]\n",
            "\n",
            "[Illustration:\n",
            "\n",
            "_Reading Jane’s Letters._ _Chap 34._\n",
            "]\n",
            "\n",
            "PRIDE.\n",
            "and\n",
            "PREJUDICE\n",
            "\n",
            "by\n",
            "Jane Austen,\n",
            "\n",
            "with a Preface by\n",
            "George Saintsbury\n",
            "and\n",
            "Illustrations by\n",
            "Hugh Thomson\n",
            "\n",
            "[Illustration: 1894]\n",
            "\n",
            "Ruskin 156. Charing\n",
            "House. Cross Road.\n",
            "\n",
            "London\n",
            "George Allen.\n",
            "\n",
            "CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
            "TOOKS COURT, CHANCERY LANE, LONDON.\n",
            "\n",
            "[Illustration:\n",
            "\n",
            "_To J. Comyns Carr\n",
            "in acknowledgment of all I\n",
            "owe to his friendship and\n",
            "advice, these illustrations are\n",
            "gratefully inscribed_\n",
            "\n",
            "_Hugh Thomson_\n",
            "]\n",
            "\n",
            "PREFACE.\n",
            "\n",
            "[Illustration]\n",
            "\n",
            "_Walt Whitman has somewhere a fine and just distinction between “loving\n",
            "by allowance” and “loving with personal love.” This distinction applies\n",
            "to books as well as to men and women; and in the case of the not very\n",
            "numerous authors who are the objects of the personal affection, it\n",
            "brings a curious consequence with it. There is much more difference as\n",
            "to their best work than in the case of tho\n",
            "\n",
            "============================================================\n",
            "\n",
            "2. 챕터 패턴 검색 결과:\n",
            "  - CHAPTER/Chapter + 번호: 61개 발견\n",
            "  - 로마자/숫자만: 0개 발견\n",
            "  - BOOK/PART + 번호: 0개 발견\n",
            "\n",
            "============================================================\n",
            "\n",
            "3. 챕터 분할 결과:\n",
            "  총 61개 챕터 발견\n",
            "\n",
            "  Chapter 1:\n",
            "    제목: 'Chapter I.]'\n",
            "    길이: 4741 문자\n",
            "    내용 미리보기: It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "\n",
            "However little known the feeling...\n",
            "\n",
            "  Chapter 2:\n",
            "    제목: 'CHAPTER II.'\n",
            "    길이: 4401 문자\n",
            "    내용 미리보기: [Illustration]\n",
            "\n",
            "Mr. Bennet was among the earliest of those who waited on Mr. Bingley. He\n",
            "had always intended to visit him, though to the last always a...\n",
            "\n",
            "  Chapter 3:\n",
            "    제목: 'CHAPTER III.'\n",
            "    길이: 9750 문자\n",
            "    내용 미리보기: [Illustration]\n",
            "\n",
            "Not all that Mrs. Bennet, however, with the assistance of her five\n",
            "daughters, could ask on the subject, was sufficient to draw from he...\n",
            "\n",
            "\n",
            "4. 챕터 길이 통계:\n",
            "  - 평균: 11319 문자\n",
            "  - 최소: 3962 문자\n",
            "  - 최대: 29373 문자\n",
            "\n",
            "============================================================\n",
            "✓ 챕터 분할 분석 완료\n"
          ]
        }
      ],
      "source": [
        "# 챕터 분할 테스트 및 디버깅\n",
        "if 'book_text' in dir() and 'cleaned_text' in dir():\n",
        "    print(\"=== 챕터 분할 디버깅 ===\")\n",
        "    print(f\"\\n1. 원본 텍스트 샘플 (처음 1000자):\")\n",
        "    print(cleaned_text[:1000])\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # 챕터 패턴 수동 검색\n",
        "    print(\"2. 챕터 패턴 검색 결과:\")\n",
        "    \n",
        "    patterns_to_test = [\n",
        "        (\"CHAPTER/Chapter + 번호\", r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)'),\n",
        "        (\"로마자/숫자만\", r'\\n\\s*([IVXLCDM]+|\\d+)\\.?\\s*\\n'),\n",
        "        (\"BOOK/PART + 번호\", r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)'),\n",
        "    ]\n",
        "    \n",
        "    for pattern_name, pattern in patterns_to_test:\n",
        "        matches = re.findall(pattern, cleaned_text)\n",
        "        print(f\"  - {pattern_name}: {len(matches)}개 발견\")\n",
        "        if matches and len(matches) <= 10:\n",
        "            print(f\"    예시: {matches[:5]}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # 실제 챕터 분할 수행\n",
        "    print(\"3. 챕터 분할 결과:\")\n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"  총 {len(chapters)}개 챕터 발견\\n\")\n",
        "    \n",
        "    # 처음 3개 챕터 미리보기\n",
        "    for i, chapter in enumerate(chapters[:3], 1):\n",
        "        print(f\"  Chapter {i}:\")\n",
        "        print(f\"    제목: '{chapter['title']}'\")\n",
        "        print(f\"    길이: {len(chapter['content'])} 문자\")\n",
        "        print(f\"    내용 미리보기: {chapter['content'][:150]}...\")\n",
        "        print()\n",
        "    \n",
        "    # 챕터 길이 통계\n",
        "    if len(chapters) > 1:\n",
        "        chapter_lengths = [len(ch['content']) for ch in chapters]\n",
        "        print(f\"\\n4. 챕터 길이 통계:\")\n",
        "        print(f\"  - 평균: {sum(chapter_lengths)/len(chapter_lengths):.0f} 문자\")\n",
        "        print(f\"  - 최소: {min(chapter_lengths)} 문자\")\n",
        "        print(f\"  - 최대: {max(chapter_lengths)} 문자\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ 챕터 분할 분석 완료\")\n",
        "else:\n",
        "    print(\"먼저 샘플 도서를 다운로드하고 정제하세요.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
