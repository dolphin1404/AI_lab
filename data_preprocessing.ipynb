{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolphin1404/AI_lab/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ë„ì„œ-ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ í”„ë¡œì íŠ¸: ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬\n",
        "\n",
        "## í”„ë¡œì íŠ¸ ê°œìš”\n",
        "- **ëª©í‘œ**: ë„ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¹„ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” LLM ëª¨ë¸ ê°œë°œ\n",
        "- **ë‹¨ê³„**: ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ë§ â†’ ì„±ëŠ¥í‰ê°€\n",
        "- **ì¼ì •**: ë°ì´í„° ì „ì²˜ë¦¬ ë§ˆê° - 10ì›” 27ì¼\n",
        "\n",
        "## ë°ì´í„° ì „ì²˜ë¦¬ ëª©í‘œ\n",
        "1. ê³µê°œ ë„ì„œ ì•„ì¹´ì´ë¸Œì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘ (Project Gutenberg, êµ­ë‚´ ë””ì§€í„¸ ë„ì„œê´€)\n",
        "2. ë…¸ì´ì¦ˆ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ\n",
        "3. í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ (ì¸ë¬¼, ì¥ì†Œ, ì‹œê°„)\n",
        "4. ë„ì„œ ë¬¸ì²´ â†’ ìŠ¤í¬ë¦½íŠ¸ ë¬¸ì²´ ë³€í™˜ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_packages",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5912e9dc-ac53-4708-a2b0-d4eb08e9abfc"
      },
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install gutenberg requests beautifulsoup4 nltk spacy transformers datasets\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gutenberg\n",
            "  Using cached Gutenberg-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "\u001b[33mWARNING: Package 'Gutenberg' has an invalid Requires-Python: Invalid specifier: '>=2.7.*'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting SPARQLWrapper>=1.8.2 (from gutenberg)\n",
            "  Using cached SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "  Using cached bsddb3-6.2.9.tar.gz (230 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m145.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ko-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ko_core_news_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "import_libraries",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67937159-6414-4abb-b09f-d32cbacb861e"
      },
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# SpaCy ëª¨ë¸ ë¡œë“œ\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_collection"
      },
      "source": [
        "## 2. ë°ì´í„° ìˆ˜ì§‘ (Data Collection)\n",
        "\n",
        "### 2.1 Project Gutenbergì—ì„œ ë„ì„œ ë‹¤ìš´ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gutenberg_collection",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f0a7e9-6dbf-4f9c-c42d-e67b4f65f24c"
      },
      "source": [
        "# êµ¬í…ë²„ê·¸ì—ì„œ ìˆ˜ì§‘í•  ë„ì„œ ID ë¦¬ìŠ¤íŠ¸ (ëœë¤ 10ê°œ)",
        "# ì¸ê¸°ìˆê³  ì±•í„° êµ¬ë¶„ì´ ëª…í™•í•œ ê³ ì „ ì†Œì„¤ë“¤ì„ ì„ íƒ",
        "# ê° IDëŠ” Project Gutenbergì˜ ì±… ë²ˆí˜¸ì…ë‹ˆë‹¤",
        "",
        "popular_book_ids = [",
        "    1342,  # Pride and Prejudice by Jane Austen",
        "    2701,  # Moby Dick by Herman Melville  ",
        "    84,    # Frankenstein by Mary Shelley",
        "    1661,  # The Adventures of Sherlock Holmes by Arthur Conan Doyle",
        "    11,    # Alice's Adventures in Wonderland by Lewis Carroll",
        "    98,    # A Tale of Two Cities by Charles Dickens",
        "    74,    # The Adventures of Tom Sawyer by Mark Twain",
        "    345,   # Dracula by Bram Stoker",
        "    46,    # A Christmas Carol by Charles Dickens",
        "    1952,  # The Yellow Wallpaper by Charlotte Perkins Gilman",
        "]",
        "",
        "# ëœë¤í•˜ê²Œ 10ê°œ ì„ íƒ (ì´ë¯¸ 10ê°œì´ë¯€ë¡œ ì „ì²´ ì‚¬ìš©)",
        "random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •",
        "book_ids_to_process = popular_book_ids[:10]",
        "",
        "print(f\"ğŸ“š ì´ {len(book_ids_to_process)}ê°œì˜ ë„ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:\")",
        "for book_id in book_ids_to_process:",
        "    print(f\"  - Book ID: {book_id}\")",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Gutenberg Collector initialized\n",
            "Sample book IDs: [1342, 84, 98, 1661, 2701]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_sample",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e527d97-bbb9-4b40-98c6-8a9295a33706"
      },
      "source": [
        "# êµ¬í…ë²„ê·¸ì—ì„œ ìˆ˜ì§‘í•  ë„ì„œ ID ë¦¬ìŠ¤íŠ¸ (ëœë¤ 10ê°œ)",
        "# ì¸ê¸°ìˆê³  ì±•í„° êµ¬ë¶„ì´ ëª…í™•í•œ ê³ ì „ ì†Œì„¤ë“¤ì„ ì„ íƒ",
        "# ê° IDëŠ” Project Gutenbergì˜ ì±… ë²ˆí˜¸ì…ë‹ˆë‹¤",
        "",
        "popular_book_ids = [",
        "    1342,  # Pride and Prejudice by Jane Austen",
        "    2701,  # Moby Dick by Herman Melville  ",
        "    84,    # Frankenstein by Mary Shelley",
        "    1661,  # The Adventures of Sherlock Holmes by Arthur Conan Doyle",
        "    11,    # Alice's Adventures in Wonderland by Lewis Carroll",
        "    98,    # A Tale of Two Cities by Charles Dickens",
        "    74,    # The Adventures of Tom Sawyer by Mark Twain",
        "    345,   # Dracula by Bram Stoker",
        "    46,    # A Christmas Carol by Charles Dickens",
        "    1952,  # The Yellow Wallpaper by Charlotte Perkins Gilman",
        "]",
        "",
        "# ëœë¤í•˜ê²Œ 10ê°œ ì„ íƒ (ì´ë¯¸ 10ê°œì´ë¯€ë¡œ ì „ì²´ ì‚¬ìš©)",
        "random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •",
        "book_ids_to_process = popular_book_ids[:10]",
        "",
        "print(f\"ğŸ“š ì´ {len(book_ids_to_process)}ê°œì˜ ë„ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:\")",
        "for book_id in book_ids_to_process:",
        "    print(f\"  - Book ID: {book_id}\")",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Successfully downloaded book 1342\n",
            "Book length: 743383 characters\n",
            "\n",
            "First 500 characters:\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "                            [Illustration:\r\n",
            "\r\n",
            "                             GEORGE ALLEN\r\n",
            "                               PUBLISHER\r\n",
            "\r\n",
            "                        156 CHARING CROSS ROAD\r\n",
            "                                LONDON\r\n",
            "\r\n",
            "                             RUSKIN HOUSE\r\n",
            "                                   ]\r\n",
            "\r\n",
            "                            [Illustration:\r\n",
            "\r\n",
            "               _Reading Janeâ€™s Letters._      _Chap 34._\r\n",
            "                               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 3. ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)\n",
        "\n",
        "### 3.1 í…ìŠ¤íŠ¸ ì •ì œ (Text Cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "text_cleaning"
      },
      "source": [
        "import re\n",
        "import time\n",
        "\n",
        "# nlp_en is not defined in the provided code, so I'm commenting it out.\n",
        "# You would typically initialize a spaCy model here, for example:\n",
        "# import spacy\n",
        "# nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # self.nlp = nlp_en\n",
        "        pass\n",
        "\n",
        "    def remove_gutenberg_header_footer(self, text):\n",
        "        \"\"\"\n",
        "        Project Gutenberg í—¤ë”ì™€ í‘¸í„° ì œê±°\n",
        "\n",
        "        Args:\n",
        "            text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "\n",
        "        Returns:\n",
        "            str: í—¤ë”/í‘¸í„°ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        # ì‹œì‘ ë§ˆì»¤ ì°¾ê¸°\n",
        "        start_markers = [\n",
        "            \"*** START OF THIS PROJECT GUTENBERG\",\n",
        "            \"*** START OF THE PROJECT GUTENBERG\",\n",
        "            \"***START OF THE PROJECT GUTENBERG\"\n",
        "        ]\n",
        "\n",
        "        # ì¢…ë£Œ ë§ˆì»¤ ì°¾ê¸°\n",
        "        end_markers = [\n",
        "            \"*** END OF THIS PROJECT GUTENBERG\",\n",
        "            \"*** END OF THE PROJECT GUTENBERG\",\n",
        "            \"***END OF THE PROJECT GUTENBERG\"\n",
        "        ]\n",
        "\n",
        "        start_idx = 0\n",
        "        for marker in start_markers:\n",
        "            idx = text.find(marker)\n",
        "            if idx != -1:\n",
        "                start_idx = text.find('\\n', idx) + 1\n",
        "                break\n",
        "\n",
        "        end_idx = len(text)\n",
        "        for marker in end_markers:\n",
        "            idx = text.find(marker)\n",
        "            if idx != -1:\n",
        "                end_idx = idx\n",
        "                break\n",
        "\n",
        "        return text[start_idx:end_idx].strip()\n",
        "\n",
        "    def remove_table_of_contents(self, text):\n",
        "        \"\"\"\n",
        "        ëª©ì°¨(Table of Contents) ì„¹ì…˜ ì œê±° (v4 - ê°œì„ ëœ ì •í™•ë„)\n",
        "\n",
        "        ê°œì„  ì‚¬í•­:\n",
        "        - ì‹¤ì œ ì±•í„°ì™€ ëª©ì°¨ í•­ëª© êµ¬ë¶„ ê°•í™”\n",
        "        - Chapter Ië¶€í„° ì •í™•íˆ ë³´ì¡´\n",
        "        - ëª©ì°¨ íŒ¨í„´ ë” ì •ë°€í•˜ê²Œ ê°ì§€\n",
        "\n",
        "        Args:\n",
        "            text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "\n",
        "        Returns:\n",
        "            str: ëª©ì°¨ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        result_lines = []\n",
        "        in_toc = False\n",
        "        toc_start_idx = -1\n",
        "        toc_line_count = 0\n",
        "        consecutive_heading_lines = 0\n",
        "        found_first_chapter = False\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_stripped = line.strip()\n",
        "            line_lower = line_stripped.lower()\n",
        "\n",
        "            # ëª©ì°¨ ì‹œì‘ ê°ì§€ (ë” ì •í™•í•œ íŒ¨í„´)\n",
        "            if not in_toc and not found_first_chapter:\n",
        "                # \"CONTENTS\" ë‹¨ë…ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²½ìš° (ì¼ë°˜ì ì¸ ëª©ì°¨ ì‹œì‘)\n",
        "                if line_stripped.upper() in ['CONTENTS', 'TABLE OF CONTENTS', 'LIST OF CHAPTERS']:\n",
        "                    in_toc = True\n",
        "                    toc_start_idx = i\n",
        "                    toc_line_count = 0\n",
        "                    continue\n",
        "\n",
        "                # \"Heading to Chapter\" íŒ¨í„´ ì—¬ëŸ¬ ì¤„ ì—°ì† (Pride and Prejudice ìŠ¤íƒ€ì¼)\n",
        "                if 'heading to chapter' in line_lower or 'heading to CHAPTER' in line_lower:\n",
        "                    consecutive_heading_lines += 1\n",
        "                    # 2ì¤„ ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ë©´ ëª©ì°¨\n",
        "                    if consecutive_heading_lines >= 2:\n",
        "                        in_toc = True\n",
        "                        toc_start_idx = i - consecutive_heading_lines\n",
        "                        continue\n",
        "                else:\n",
        "                    if 0 < consecutive_heading_lines < 2:\n",
        "                        # 1ì¤„ë§Œ ìˆì—ˆìœ¼ë©´ ì‹¤ì œ ë‚´ìš©ì¼ ìˆ˜ ìˆìŒ\n",
        "                        consecutive_heading_lines = 0\n",
        "\n",
        "            # ì‹¤ì œ ì±•í„° ì‹œì‘ í™•ì¸ (ëª©ì°¨ ì¢…ë£Œ ë˜ëŠ” ì²« ì±•í„° ë°œê²¬)\n",
        "            # ë” ì—„ê²©í•œ ì¡°ê±´: ì¤„ì´ ì§§ê³ (80ì ì´í•˜), íŒ¨í„´ì´ ëª…í™•í•˜ê³ , ë‹¤ìŒ ì¤„ì— ë‚´ìš©ì´ ìˆì–´ì•¼ í•¨\n",
        "            is_real_chapter = False\n",
        "            if len(line_stripped) <= 80:\n",
        "                # \"CHAPTER I\", \"Chapter 1\", \"CHAPTER ONE\" ë“±ì˜ íŒ¨í„´\n",
        "                chapter_pattern = r'^\\s*(CHAPTER|Chapter)\\s+(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX|XXI|XXII|XXIII|XXIV|XXV|XXVI|XXVII|XXVIII|XXIX|XXX|XXXI|XXXII|XXXIII|XXXIV|XXXV|XXXVI|XXXVII|XXXVIII|XXXIX|XL|XLI|XLII|XLIII|XLIV|XLV|XLVI|XLVII|XLVIII|XLIX|L|LI|LII|LIII|LIV|LV|LVI|LVII|LVIII|LIX|LX|LXI|LXII|LXIII|LXIV|LXV|LXVI|LXVII|LXVIII|LXIX|LXX|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty)\\.?\\s*$'\n",
        "                chapter_match = re.match(chapter_pattern, line_stripped)\n",
        "\n",
        "                if chapter_match:\n",
        "                    # ë‹¤ìŒ ëª‡ ì¤„ì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ ì±•í„° ë‚´ìš©ì´ ìˆëŠ”ì§€ ê²€ì¦\n",
        "                    has_content_after = False\n",
        "                    for j in range(i + 1, min(i + 10, len(lines))):\n",
        "                        next_line = lines[j].strip()\n",
        "                        # ë¹ˆ ì¤„ì´ ì•„ë‹ˆê³ , 40ì ì´ìƒì˜ ì‹¤ì œ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ì±•í„°ë¡œ ê°„ì£¼\n",
        "                        if next_line and len(next_line) > 40:\n",
        "                            # ëª©ì°¨ í‚¤ì›Œë“œê°€ ì—†ì–´ì•¼ í•¨\n",
        "                            if 'heading to' not in next_line.lower() and 'page' not in next_line.lower():\n",
        "                                has_content_after = True\n",
        "                                break\n",
        "\n",
        "                    if has_content_after:\n",
        "                        is_real_chapter = True\n",
        "                        found_first_chapter = True\n",
        "\n",
        "                        # ëª©ì°¨ ì¤‘ì´ì—ˆë‹¤ë©´ ëª©ì°¨ ì¢…ë£Œ\n",
        "                        if in_toc and toc_line_count > 3:\n",
        "                            in_toc = False\n",
        "                            # ëª©ì°¨ ë¶€ë¶„ ìŠ¤í‚µ (ì´ë¯¸ ì¶”ê°€ ì•ˆ í•¨)\n",
        "\n",
        "            # ëª©ì°¨ ì¤‘ì´ë©´ ì¤„ ì¹´ìš´íŠ¸ë§Œ ì¦ê°€í•˜ê³  ì¶”ê°€ ì•ˆ í•¨\n",
        "            if in_toc:\n",
        "                toc_line_count += 1\n",
        "                # ëª©ì°¨ê°€ ë„ˆë¬´ ê¸¸ë©´ (150ì¤„ ì´ìƒ) ì¢…ë£Œ\n",
        "                if toc_line_count > 150:\n",
        "                    in_toc = False\n",
        "                continue # ëª©ì°¨ ë‚´ìš©ì€ ê²°ê³¼ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ\n",
        "\n",
        "            # ì‹¤ì œ ì±•í„°ë¥¼ ë§Œë‚¬ê±°ë‚˜, ëª©ì°¨ ìƒíƒœê°€ ì•„ë‹ˆë©´ ë¼ì¸ ì¶”ê°€\n",
        "            if is_real_chapter or not in_toc:\n",
        "                result_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(result_lines)\n",
        "\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ\n",
        "        - ê³¼ë„í•œ ê³µë°± ì œê±°\n",
        "        - íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”\n",
        "        - ì¤„ë°”ê¿ˆ ì •ê·œí™”\n",
        "\n",
        "        Args:\n",
        "            text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "\n",
        "        Returns:\n",
        "            str: ì •ì œëœ í…ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ ë‹¨ì¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "        # ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # ì¤„ ì‹œì‘/ë ê³µë°± ì œê±°\n",
        "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
        "        return text.strip()\n",
        "\n",
        "    def split_into_chapters(self, text):\n",
        "        \"\"\"\n",
        "        í…ìŠ¤íŠ¸ë¥¼ ì±•í„°ë³„ë¡œ ë¶„í•  (ê°œì„ ëœ ë²„ì „ v2)\n",
        "\n",
        "        ê°œì„  ì‚¬í•­:\n",
        "        - ëª©ì°¨(Table of Contents) ìë™ ì œê±°\n",
        "        - \"Heading to Chapter\" ê°™ì€ ëª©ì°¨ í•­ëª© í•„í„°ë§\n",
        "        - ì‹¤ì œ ì±•í„° ë‚´ìš©ë§Œ ì¶”ì¶œ\n",
        "        - ë” ì •í™•í•œ ì±•í„° ê°ì§€\n",
        "\n",
        "        ì§€ì›í•˜ëŠ” í˜•ì‹:\n",
        "        - \"CHAPTER I\", \"CHAPTER 1\", \"CHAPTER ONE\"\n",
        "        - \"Chapter I.\", \"Chapter 1.\", \"Chapter One.\"\n",
        "        - ì‹¤ì œ ì±•í„° ì œëª©ì´ ìˆëŠ” ê²½ìš° (e.g., \"CHAPTER I. The Beginning\")\n",
        "\n",
        "        Args:\n",
        "            text (str): ì „ì²´ í…ìŠ¤íŠ¸\n",
        "\n",
        "        Returns:\n",
        "            list: ì±•í„°ë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        # 1ë‹¨ê³„: ëª©ì°¨ ì œê±°\n",
        "        text_without_toc = self.remove_table_of_contents(text)\n",
        "\n",
        "        # 2ë‹¨ê³„: ì±•í„° íŒ¨í„´ ë§¤ì¹­\n",
        "        # ì•ˆì „í•œ íŒ¨í„´ ì‚¬ìš© (catastrophic backtracking ë°©ì§€)\n",
        "        word_numbers = \"One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty|Twenty-one|Twenty-two|Twenty-three|Twenty-four|Twenty-five|Twenty-six|Twenty-seven|Twenty-eight|Twenty-nine|Thirty|Thirty-one|Thirty-two|Thirty-three|Thirty-four|Thirty-five|Thirty-six|Thirty-seven|Thirty-eight|Thirty-nine|Forty|Forty-one|Forty-two|Forty-three|Forty-four|Forty-five|Forty-six|Forty-seven|Forty-eight|Forty-nine|Fifty|Fifty-one|Fifty-two|Fifty-three|Fifty-four|Fifty-five|Fifty-six|Fifty-seven|Fifty-eight|Fifty-nine|Sixty|Sixty-one\"\n",
        "\n",
        "        patterns = [\n",
        "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|' + word_numbers + r')(?:\\.\\s*|\\s+|\\n)',\n",
        "            # Pattern 2: \"BOOK I\", \"PART I\"\n",
        "            r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)(?:\\.\\s*|\\s+|\\n)',\n",
        "        ]\n",
        "\n",
        "        best_split = None\n",
        "        best_count = 0\n",
        "        best_pattern_idx = -1\n",
        "\n",
        "        # ê° íŒ¨í„´ ì‹œë„ (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)\n",
        "        for idx, pattern in enumerate(patterns):\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                splits = re.split(pattern, text_without_toc)\n",
        "\n",
        "                # íƒ€ì„ì•„ì›ƒ ì²´í¬ (3ì´ˆ)\n",
        "                if time.time() - start_time > 3:\n",
        "                    print(f\"Warning: Pattern {idx} took too long, skipping\")\n",
        "                    continue\n",
        "\n",
        "                # ì±•í„° ìˆ˜ ê³„ì‚° (3ê°œ ìš”ì†Œê°€ 1ê°œ ì±•í„°: prefix, number, content)\n",
        "                chapter_count = (len(splits) - 1) // 3 if len(splits) > 1 else 0\n",
        "\n",
        "                # í•©ë¦¬ì ì¸ ë²”ìœ„ì˜ ì±•í„° ìˆ˜ (2-150ê°œ)\n",
        "                if 2 <= chapter_count <= 150:\n",
        "                    # ì±•í„° í‰ê·  ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ìœ¼ë©´ ëª©ì°¨ì¼ ê°€ëŠ¥ì„±)\n",
        "                    total_length = sum(len(splits[i]) for i in range(2, len(splits), 3) if i < len(splits))\n",
        "                    avg_length = total_length / chapter_count if chapter_count > 0 else 0\n",
        "\n",
        "                    # í‰ê·  ê¸¸ì´ê°€ 300ì ì´ìƒì´ì–´ì•¼ ì‹¤ì œ ì±•í„°\n",
        "                    if avg_length >= 300 and chapter_count > best_count:\n",
        "                        best_count = chapter_count\n",
        "                        best_split = (pattern, splits)\n",
        "                        best_pattern_idx = idx\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Pattern {idx} failed with error: {e}\")\n",
        "                continue\n",
        "\n",
        "        # 3ë‹¨ê³„: ì±•í„° ì¶”ì¶œ\n",
        "        if best_split:\n",
        "            pattern, chapters = best_split\n",
        "            result = []\n",
        "\n",
        "            for i in range(1, len(chapters), 3):\n",
        "                if i + 2 <= len(chapters):\n",
        "                    chapter_prefix = chapters[i].strip()    # \"CHAPTER\" or \"Chapter\"\n",
        "                    chapter_number = chapters[i + 1].strip()  # \"I\", \"1\", etc.\n",
        "                    chapter_content = chapters[i + 2].strip() if i + 2 < len(chapters) else \"\"\n",
        "\n",
        "                    # ì±•í„° ë‚´ìš©ì´ ì¶©ë¶„íˆ ê¸´ì§€ í™•ì¸ (ìµœì†Œ 200ì)\n",
        "                    if len(chapter_content) < 200:\n",
        "                        continue\n",
        "\n",
        "                    # ì±•í„° ì œëª©ì—ì„œ ì‹¤ì œ ì œëª© ì¶”ì¶œ\n",
        "                    content_lines = chapter_content.split('\\n', 2)\n",
        "                    chapter_title_suffix = \"\"\n",
        "\n",
        "                    # ì²« ì¤„ì´ ì§§ìœ¼ë©´ (60ì ì´í•˜) ì±•í„° ì œëª©ìœ¼ë¡œ ê°„ì£¼\n",
        "                    if content_lines and len(content_lines[0]) <= 60 and content_lines[0].strip():\n",
        "                        chapter_title_suffix = content_lines[0].strip()\n",
        "                        # ì œëª©ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ê°€ ë‚´ìš©\n",
        "                        if len(content_lines) > 1:\n",
        "                            chapter_content = '\\n'.join(content_lines[1:]).strip()\n",
        "\n",
        "                    # ìµœì¢… ì±•í„° ì œëª© ìƒì„±\n",
        "                    if chapter_prefix:\n",
        "                        if chapter_title_suffix:\n",
        "                            chapter_title = f\"{chapter_prefix} {chapter_number}. {chapter_title_suffix}\"\n",
        "                        else:\n",
        "                            chapter_title = f\"{chapter_prefix} {chapter_number}\"\n",
        "                    else:\n",
        "                        chapter_title = f\"Chapter {chapter_number}\"\n",
        "\n",
        "                    if chapter_content:  # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì¶”ê°€\n",
        "                        result.append({\n",
        "                            'title': chapter_title,\n",
        "                            'content': chapter_content\n",
        "                        })\n",
        "\n",
        "            # ê²°ê³¼ ê²€ì¦: ìµœì†Œ 2ê°œ ì´ìƒì˜ ì±•í„°\n",
        "            if len(result) >= 2:\n",
        "                return result\n",
        "\n",
        "        # 4ë‹¨ê³„: Fallback - ìˆ˜ë™ ë¼ì¸ ë¶„ì„\n",
        "        return self._fallback_chapter_split(text_without_toc)\n",
        "\n",
        "\n",
        "    def _fallback_chapter_split(self, text):\n",
        "        \"\"\"\n",
        "        Fallback ë°©ë²•: ì¤„ ë‹¨ìœ„ë¡œ ì±•í„° ë§ˆì»¤ ì°¾ê¸°\n",
        "        ëª©ì°¨ í•­ëª©ì€ ì œì™¸í•˜ê³  ì‹¤ì œ ì±•í„°ë§Œ ì¶”ì¶œ\n",
        "        \"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        chapters = []\n",
        "        current_chapter = None\n",
        "        current_content = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_stripped = line.strip()\n",
        "            line_upper = line_stripped.upper()\n",
        "\n",
        "            # ëª©ì°¨ í•­ëª© ê±´ë„ˆë›°ê¸°\n",
        "            if 'HEADING TO' in line_upper or 'CONTENTS' in line_upper:\n",
        "                continue\n",
        "\n",
        "            # ì±•í„° í—¤ë” ê°ì§€\n",
        "            is_chapter = False\n",
        "            chapter_title = None\n",
        "\n",
        "            # ì§§ì€ ì¤„ (60ì ì´í•˜)ì—ì„œë§Œ ì±•í„° ê²€ì‚¬\n",
        "            if len(line_stripped) <= 60 and line_stripped:\n",
        "                # \"CHAPTER X\" í˜•ì‹\n",
        "                chapter_match = re.match(r'^(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)', line_stripped)\n",
        "                if chapter_match:\n",
        "                    is_chapter = True\n",
        "                    chapter_title = line_stripped\n",
        "                # \"BOOK X\", \"PART X\" í˜•ì‹\n",
        "                elif re.match(r'^(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)', line_stripped):\n",
        "                    is_chapter = True\n",
        "                    chapter_title = line_stripped\n",
        "\n",
        "            if is_chapter and chapter_title:\n",
        "                # ì´ì „ ì±•í„° ì €ì¥\n",
        "                if current_chapter and current_content:\n",
        "                    content_text = '\\n'.join(current_content).strip()\n",
        "                    # ë‚´ìš©ì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°ë§Œ (ìµœì†Œ 300ì)\n",
        "                    if len(content_text) >= 300:\n",
        "                        chapters.append({\n",
        "                            'title': current_chapter,\n",
        "                            'content': content_text\n",
        "                        })\n",
        "\n",
        "                # ìƒˆ ì±•í„° ì‹œì‘\n",
        "                current_chapter = chapter_title\n",
        "                current_content = []\n",
        "            else:\n",
        "                # í˜„ì¬ ì±•í„°ì— ë‚´ìš© ì¶”ê°€\n",
        "                if current_chapter:  # ì±•í„°ê°€ ì‹œì‘ëœ í›„ì—ë§Œ\n",
        "                    current_content.append(line)\n",
        "\n",
        "        # ë§ˆì§€ë§‰ ì±•í„° ì €ì¥\n",
        "        if current_chapter and current_content:\n",
        "            content_text = '\\n'.join(current_content).strip()\n",
        "            if len(content_text) >= 300:\n",
        "                chapters.append({\n",
        "                    'title': current_chapter,\n",
        "                    'content': content_text\n",
        "                })\n",
        "\n",
        "        # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì±•í„°ê°€ ìˆì–´ì•¼ ìœ íš¨\n",
        "        if len(chapters) >= 2:\n",
        "            return chapters\n",
        "        else:\n",
        "            return [{'title': 'Full Text', 'content': text}]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apply_preprocessing",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd67467-0957-4631-be8e-b6aa2b12b929"
      },
      "source": [
        "preprocessor = TextPreprocessor()\n",
        "# ìƒ˜í”Œ ë„ì„œì— ì „ì²˜ë¦¬ ì ìš©\n",
        "if book_text:\n",
        "    # í—¤ë”/í‘¸í„° ì œê±°\n",
        "    cleaned_text = preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "    cleaned_text = preprocessor.clean_text(cleaned_text)\n",
        "\n",
        "    print(f\"âœ“ Original length: {len(book_text)} characters\")\n",
        "    print(f\"âœ“ Cleaned length: {len(cleaned_text)} characters\")\n",
        "    print(f\"âœ“ Removed: {len(book_text) - len(cleaned_text)} characters\")\n",
        "\n",
        "    # ì±•í„° ë¶„í• \n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"\\nâœ“ Found {len(chapters)} chapters\")\n",
        "\n",
        "    if chapters:\n",
        "        print(f\"\\nFirst chapter: {chapters[0]['title']}\")\n",
        "        print(f\"Content preview: {chapters[0]['content'][:300]}...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Original length: 743383 characters\n",
            "âœ“ Cleaned length: 720973 characters\n",
            "âœ“ Removed: 22410 characters\n",
            "\n",
            "âœ“ Found 61 chapters\n",
            "\n",
            "First chapter: Chapter I.]\n",
            "Content preview: It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "\n",
            "However little known the feelings or views of such a man may be on his\n",
            "first entering a neighbourhood, this truth is so well fixed in the minds\n",
            "of the surrounding families, that he i...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entity_extraction"
      },
      "source": [
        "### 3.2 í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ (Entity Extraction)\n",
        "\n",
        "ì¸ë¬¼, ì¥ì†Œ, ì‹œê°„ ë“± í•µì‹¬ ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ì—¬ ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ì— í™œìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entity_extractor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3e3515-6b38-4798-9bdd-a7175709dd9a"
      },
      "source": [
        "class EntityExtractor:\n",
        "    \"\"\"Named Entity Recognitionì„ í†µí•œ í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "\n",
        "    def extract_entities(self, text, max_length=1000000):\n",
        "        \"\"\"\n",
        "        í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª… ì¶”ì¶œ\n",
        "\n",
        "        Args:\n",
        "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
        "            max_length (int): ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´\n",
        "\n",
        "        Returns:\n",
        "            dict: ì¹´í…Œê³ ë¦¬ë³„ ê°œì²´ëª… ì‚¬ì „\n",
        "        \"\"\"\n",
        "        # SpaCyì˜ max_length ì„¤ì •\n",
        "        self.nlp.max_length = max_length\n",
        "\n",
        "        # í…ìŠ¤íŠ¸ ë¶„ì„\n",
        "        doc = self.nlp(text[:max_length])\n",
        "\n",
        "        # ê°œì²´ëª… ë¶„ë¥˜\n",
        "        entities = {\n",
        "            'PERSON': [],      # ì¸ë¬¼\n",
        "            'GPE': [],          # ì§€ì •í•™ì  ê°œì²´ (ë„ì‹œ, êµ­ê°€ ë“±)\n",
        "            'LOC': [],          # ìœ„ì¹˜\n",
        "            'DATE': [],         # ë‚ ì§œ\n",
        "            'TIME': [],         # ì‹œê°„\n",
        "            'ORG': [],          # ì¡°ì§\n",
        "            'EVENT': []         # ì´ë²¤íŠ¸\n",
        "        }\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in entities:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "\n",
        "        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "        for key in entities:\n",
        "            entity_counts = defaultdict(int)\n",
        "            for entity in entities[key]:\n",
        "                entity_counts[entity] += 1\n",
        "            entities[key] = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def get_main_characters(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        ì£¼ìš” ì¸ë¬¼ ì¶”ì¶œ\n",
        "\n",
        "        Args:\n",
        "            entities (dict): extract_entitiesì˜ ê²°ê³¼\n",
        "            top_n (int): ë°˜í™˜í•  ì£¼ìš” ì¸ë¬¼ ìˆ˜\n",
        "\n",
        "        Returns:\n",
        "            list: ì£¼ìš” ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        return entities['PERSON'][:top_n]\n",
        "\n",
        "    def get_main_locations(self, entities, top_n=10):\n",
        "        \"\"\"\n",
        "        ì£¼ìš” ì¥ì†Œ ì¶”ì¶œ\n",
        "\n",
        "        Args:\n",
        "            entities (dict): extract_entitiesì˜ ê²°ê³¼\n",
        "            top_n (int): ë°˜í™˜í•  ì£¼ìš” ì¥ì†Œ ìˆ˜\n",
        "\n",
        "        Returns:\n",
        "            list: ì£¼ìš” ì¥ì†Œ ë¦¬ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        locations = entities['GPE'] + entities['LOC']\n",
        "        # ë¹ˆë„ìˆ˜ë¡œ ì¬ì •ë ¬\n",
        "        location_dict = defaultdict(int)\n",
        "        for loc, count in locations:\n",
        "            location_dict[loc] += count\n",
        "        return sorted(location_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "extractor = EntityExtractor()\n",
        "print(\"âœ“ Entity Extractor initialized\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Entity Extractor initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extract_sample_entities",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b167395d-c00e-4047-ab13-cccdfcdaafdf"
      },
      "source": [
        "# ìƒ˜í”Œ ì±•í„°ì—ì„œ ê°œì²´ëª… ì¶”ì¶œ\n",
        "if chapters:\n",
        "    # ì²« ë²ˆì§¸ ì±•í„° ë¶„ì„\n",
        "    sample_chapter = chapters[0]['content']\n",
        "    entities = extractor.extract_entities(sample_chapter)\n",
        "\n",
        "    print(\"âœ“ Entity extraction completed\\n\")\n",
        "\n",
        "    # ì£¼ìš” ì¸ë¬¼\n",
        "    main_characters = extractor.get_main_characters(entities, top_n=5)\n",
        "    print(\"Main Characters:\")\n",
        "    for char, count in main_characters:\n",
        "        print(f\"  - {char}: {count} mentions\")\n",
        "\n",
        "    # ì£¼ìš” ì¥ì†Œ\n",
        "    main_locations = extractor.get_main_locations(entities, top_n=5)\n",
        "    print(\"\\nMain Locations:\")\n",
        "    for loc, count in main_locations:\n",
        "        print(f\"  - {loc}: {count} mentions\")\n",
        "\n",
        "    # ì‹œê°„ ì •ë³´\n",
        "    if entities['DATE']:\n",
        "        print(\"\\nTemporal Information:\")\n",
        "        for date, count in entities['DATE'][:5]:\n",
        "            print(f\"  - {date}: {count} mentions\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Entity extraction completed\n",
            "\n",
            "Main Characters:\n",
            "  - Bennet: 6 mentions\n",
            "  - Bingley: 4 mentions\n",
            "  - George Allen: 3 mentions\n",
            "  - Lizzy: 3 mentions\n",
            "  - Long: 2 mentions\n",
            "\n",
            "Main Locations:\n",
            "  - England: 1 mentions\n",
            "  - Lydia: 1 mentions\n",
            "\n",
            "Temporal Information:\n",
            "  - 1894: 3 mentions\n",
            "  - one day: 1 mentions\n",
            "  - Monday: 1 mentions\n",
            "  - the end of next week: 1 mentions\n",
            "  - five thousand a year: 1 mentions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "script_conversion"
      },
      "source": [
        "### 3.3 ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°í™”\n",
        "\n",
        "ë„ì„œ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë¦½íŠ¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "script_formatter",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2157de3-bfe9-4185-d737-5a7a25d576ac"
      },
      "source": [
        "class ScriptFormatter:\n",
        "    \"\"\"ë„ì„œ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë¦½íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp_en\n",
        "\n",
        "    def extract_dialogues(self, text):\n",
        "        \"\"\"\n",
        "        í…ìŠ¤íŠ¸ì—ì„œ ëŒ€í™”ë¬¸ ì¶”ì¶œ\n",
        "\n",
        "        Args:\n",
        "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
        "\n",
        "        Returns:\n",
        "            list: ëŒ€í™”ë¬¸ ë¦¬ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ëŒ€í™”ë¬¸ ì¶”ì¶œ\n",
        "        dialogue_pattern = r'[\"\\']([^\"\\']+)[\"\\']'\n",
        "        dialogues = re.findall(dialogue_pattern, text)\n",
        "\n",
        "        # ì§§ì€ ëŒ€í™” í•„í„°ë§ (3ë‹¨ì–´ ì´ìƒ)\n",
        "        dialogues = [d for d in dialogues if len(d.split()) >= 3]\n",
        "\n",
        "        return dialogues\n",
        "\n",
        "    def extract_narrative(self, text):\n",
        "        \"\"\"\n",
        "        ì„œìˆ  ë¶€ë¶„ ì¶”ì¶œ (ëŒ€í™”ê°€ ì•„ë‹Œ ë¶€ë¶„)\n",
        "\n",
        "        Args:\n",
        "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
        "\n",
        "        Returns:\n",
        "            str: ì„œìˆ  í…ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        # ëŒ€í™”ë¬¸ ì œê±°\n",
        "        narrative = re.sub(r'[\"\\'][^\"\\']+[\"\\']', '', text)\n",
        "\n",
        "        # ì •ì œ\n",
        "        narrative = re.sub(r' +', ' ', narrative)\n",
        "        narrative = re.sub(r'\\n\\s*\\n', '\\n\\n', narrative)\n",
        "\n",
        "        return narrative.strip()\n",
        "\n",
        "    def create_scene_structure(self, chapter_text, entities):\n",
        "        \"\"\"\n",
        "        ì±•í„°ë¥¼ ì”¬ êµ¬ì¡°ë¡œ ë³€í™˜\n",
        "\n",
        "        Args:\n",
        "            chapter_text (str): ì±•í„° í…ìŠ¤íŠ¸\n",
        "            entities (dict): ì¶”ì¶œëœ ê°œì²´ëª…\n",
        "\n",
        "        Returns:\n",
        "            dict: ì”¬ êµ¬ì¡° ì •ë³´\n",
        "        \"\"\"\n",
        "        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• \n",
        "        doc = self.nlp(chapter_text[:100000])  # ì²˜ë¦¬ ì†ë„ë¥¼ ìœ„í•´ ì œí•œ\n",
        "        sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "        # ëŒ€í™”ë¬¸ê³¼ ì„œìˆ  ë¶„ë¦¬\n",
        "        dialogues = self.extract_dialogues(chapter_text)\n",
        "        narrative = self.extract_narrative(chapter_text)\n",
        "\n",
        "        scene_data = {\n",
        "            'characters': [char for char, _ in entities.get('PERSON', [])[:5]],\n",
        "            'locations': [loc for loc, _ in (entities.get('GPE', []) + entities.get('LOC', []))[:3]],\n",
        "            'dialogues': dialogues[:10],\n",
        "            'narrative_sentences': sentences[:20],\n",
        "            'total_sentences': len(sentences),\n",
        "            'total_dialogues': len(dialogues)\n",
        "        }\n",
        "\n",
        "        return scene_data\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "formatter = ScriptFormatter()\n",
        "print(\"âœ“ Script Formatter initialized\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Script Formatter initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_scene",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c30fb2-d76e-49e3-a2da-57b39b8a0fa4"
      },
      "source": [
        "# ìƒ˜í”Œ ì±•í„°ë¥¼ ì”¬ êµ¬ì¡°ë¡œ ë³€í™˜\n",
        "if chapters and entities:\n",
        "    scene_data = formatter.create_scene_structure(chapters[0]['content'], entities)\n",
        "\n",
        "    print(\"âœ“ Scene structure created\\n\")\n",
        "    print(f\"Scene Information:\")\n",
        "    print(f\"  - Main Characters: {', '.join(scene_data['characters'])}\")\n",
        "    print(f\"  - Locations: {', '.join(scene_data['locations'])}\")\n",
        "    print(f\"  - Total Sentences: {scene_data['total_sentences']}\")\n",
        "    print(f\"  - Total Dialogues: {scene_data['total_dialogues']}\")\n",
        "\n",
        "    print(\"\\nSample Dialogues:\")\n",
        "    for i, dialogue in enumerate(scene_data['dialogues'][:3], 1):\n",
        "        print(f\"  {i}. \\\"{dialogue}\\\"\")\n",
        "\n",
        "    print(\"\\nSample Narrative:\")\n",
        "    for i, sentence in enumerate(scene_data['narrative_sentences'][:3], 1):\n",
        "        print(f\"  {i}. {sentence}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Scene structure created\n",
            "\n",
            "Scene Information:\n",
            "  - Main Characters: Bennet, Bingley, George Allen, Lizzy, Long\n",
            "  - Locations: England, Lydia\n",
            "  - Total Sentences: 53\n",
            "  - Total Dialogues: 0\n",
            "\n",
            "Sample Dialogues:\n",
            "\n",
            "Sample Narrative:\n",
            "  1. It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "\n",
            "\n",
            "  2. However little known the feelings or views of such a man may be on his\n",
            "first entering a neighbourhood, this truth is so well fixed in the minds\n",
            "of the surrounding families, that he is considered as the rightful\n",
            "property of some one or other of their daughters.\n",
            "\n",
            "\n",
            "  3. â€œMy dear Mr. Bennet,â€ said his lady to him one day, â€œhave you heard that\n",
            "Netherfield Park is let at last?â€\n",
            "\n",
            "Mr. Bennet replied that he had not.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_creation"
      },
      "source": [
        "## 4. í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•\n",
        "\n",
        "### 4.1 ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipeline",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143dcc35-e6d5-43be-c1dc-5d5a7f642e9e"
      },
      "source": [
        "class BookToScriptPipeline:\n",
        "    \"\"\"ë„ì„œì—ì„œ ìŠ¤í¬ë¦½íŠ¸ í•™ìŠµ ë°ì´í„°ê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.collector = GutenbergCollector()\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.extractor = EntityExtractor()\n",
        "        self.formatter = ScriptFormatter()\n",
        "\n",
        "    def process_book(self, book_id):\n",
        "        \"\"\"\n",
        "        ë‹¨ì¼ ë„ì„œ ì²˜ë¦¬\n",
        "\n",
        "        Args:\n",
        "            book_id (int): ë„ì„œ ID\n",
        "\n",
        "        Returns:\n",
        "            dict: ì²˜ë¦¬ëœ ë°ì´í„°\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing book {book_id}...\")\n",
        "\n",
        "        # 1. ë„ì„œ ë‹¤ìš´ë¡œë“œ\n",
        "        book_text = self.collector.download_book(book_id)\n",
        "        if not book_text:\n",
        "            return None\n",
        "        print(f\"  âœ“ Downloaded ({len(book_text)} chars)\")\n",
        "\n",
        "        # 2. ì „ì²˜ë¦¬\n",
        "        cleaned_text = self.preprocessor.remove_gutenberg_header_footer(book_text)\n",
        "        cleaned_text = self.preprocessor.clean_text(cleaned_text)\n",
        "        print(f\"  âœ“ Cleaned ({len(cleaned_text)} chars)\")\n",
        "\n",
        "        # 3. ì±•í„° ë¶„í• \n",
        "        chapters = self.preprocessor.split_into_chapters(cleaned_text)\n",
        "        print(f\"  âœ“ Split into {len(chapters)} chapters\")\n",
        "\n",
        "        # 4. ê° ì±•í„°ë³„ ì²˜ë¦¬\n",
        "        processed_chapters = []\n",
        "        for i, chapter in enumerate(chapters[:5]):  # ì²˜ìŒ 5ê°œ ì±•í„°ë§Œ ì²˜ë¦¬ (ì˜ˆì‹œ)\n",
        "            # ê°œì²´ëª… ì¶”ì¶œ\n",
        "            entities = self.extractor.extract_entities(chapter['content'])\n",
        "\n",
        "            # ì”¬ êµ¬ì¡° ìƒì„±\n",
        "            scene_data = self.formatter.create_scene_structure(chapter['content'], entities)\n",
        "\n",
        "            processed_chapters.append({\n",
        "                'chapter_title': chapter['title'],\n",
        "                'chapter_number': i + 1,\n",
        "                'original_text': chapter['content'],\n",
        "                'entities': entities,\n",
        "                'scene_data': scene_data\n",
        "            })\n",
        "\n",
        "        print(f\"  âœ“ Processed {len(processed_chapters)} chapters\")\n",
        "\n",
        "        return {\n",
        "            'book_id': book_id,\n",
        "            'total_length': len(cleaned_text),\n",
        "            'total_chapters': len(chapters),\n",
        "            'processed_chapters': processed_chapters\n",
        "        }\n",
        "\n",
        "    def process_multiple_books(self, book_ids, output_dir='./processed_data'):\n",
        "        \"\"\"\n",
        "        ì—¬ëŸ¬ ë„ì„œ ì²˜ë¦¬ ë° ì €ì¥\n",
        "\n",
        "        Args:\n",
        "            book_ids (list): ë„ì„œ ID ë¦¬ìŠ¤íŠ¸\n",
        "            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
        "\n",
        "        Returns:\n",
        "            list: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        results = []\n",
        "\n",
        "        for book_id in book_ids:\n",
        "            result = self.process_book(book_id)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "\n",
        "                # JSONìœ¼ë¡œ ì €ì¥\n",
        "                output_file = os.path.join(output_dir, f'book_{book_id}.json')\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "                print(f\"  âœ“ Saved to {output_file}\")\n",
        "\n",
        "        print(f\"\\nâœ“ Completed processing {len(results)} books\")\n",
        "        return results\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
        "pipeline = BookToScriptPipeline()\n",
        "print(\"âœ“ Pipeline initialized\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Pipeline initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_pipeline",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68db36ed-9663-480e-e79a-2ab7e27bbe58"
      },
      "source": [
        "# êµ¬í…ë²„ê·¸ì—ì„œ ìˆ˜ì§‘í•  ë„ì„œ ID ë¦¬ìŠ¤íŠ¸ (ëœë¤ 10ê°œ)",
        "# ì¸ê¸°ìˆê³  ì±•í„° êµ¬ë¶„ì´ ëª…í™•í•œ ê³ ì „ ì†Œì„¤ë“¤ì„ ì„ íƒ",
        "# ê° IDëŠ” Project Gutenbergì˜ ì±… ë²ˆí˜¸ì…ë‹ˆë‹¤",
        "",
        "popular_book_ids = [",
        "    1342,  # Pride and Prejudice by Jane Austen",
        "    2701,  # Moby Dick by Herman Melville  ",
        "    84,    # Frankenstein by Mary Shelley",
        "    1661,  # The Adventures of Sherlock Holmes by Arthur Conan Doyle",
        "    11,    # Alice's Adventures in Wonderland by Lewis Carroll",
        "    98,    # A Tale of Two Cities by Charles Dickens",
        "    74,    # The Adventures of Tom Sawyer by Mark Twain",
        "    345,   # Dracula by Bram Stoker",
        "    46,    # A Christmas Carol by Charles Dickens",
        "    1952,  # The Yellow Wallpaper by Charlotte Perkins Gilman",
        "]",
        "",
        "# ëœë¤í•˜ê²Œ 10ê°œ ì„ íƒ (ì´ë¯¸ 10ê°œì´ë¯€ë¡œ ì „ì²´ ì‚¬ìš©)",
        "random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •",
        "book_ids_to_process = popular_book_ids[:10]",
        "",
        "print(f\"ğŸ“š ì´ {len(book_ids_to_process)}ê°œì˜ ë„ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:\")",
        "for book_id in book_ids_to_process:",
        "    print(f\"  - Book ID: {book_id}\")",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing book 1342...\n",
            "  âœ“ Downloaded (743383 chars)\n",
            "  âœ“ Cleaned (720973 chars)\n",
            "  âœ“ Split into 61 chapters\n",
            "  âœ“ Processed 5 chapters\n",
            "  âœ“ Saved to ./processed_data/book_1342.json\n",
            "\n",
            "âœ“ Completed processing 1 books\n",
            "\n",
            "=== Processing Summary ===\n",
            "\n",
            "Book ID: 1342\n",
            "  Total Length: 720,973 characters\n",
            "  Total Chapters: 61\n",
            "  Processed Chapters: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## 5. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì¤€ë¹„\n",
        "\n",
        "### 5.1 BLEU Score êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bleu_metric",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb1816c-3c3a-4c15-bac1-39e5cba25be4"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.smooth = SmoothingFunction()\n",
        "\n",
        "    def calculate_bleu(self, reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "        \"\"\"\n",
        "        BLEU ì ìˆ˜ ê³„ì‚°\n",
        "\n",
        "        Args:\n",
        "            reference (str): ì°¸ì¡° í…ìŠ¤íŠ¸\n",
        "            candidate (str): ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
        "            weights (tuple): n-gram ê°€ì¤‘ì¹˜\n",
        "\n",
        "        Returns:\n",
        "            float: BLEU ì ìˆ˜\n",
        "        \"\"\"\n",
        "        # í† í°í™”\n",
        "        reference_tokens = reference.split()\n",
        "        candidate_tokens = candidate.split()\n",
        "\n",
        "        # BLEU ê³„ì‚° (smoothing ì ìš©)\n",
        "        bleu_score = sentence_bleu(\n",
        "            [reference_tokens],\n",
        "            candidate_tokens,\n",
        "            weights=weights,\n",
        "            smoothing_function=self.smooth.method1\n",
        "        )\n",
        "\n",
        "        return bleu_score\n",
        "\n",
        "    def calculate_bleu_variants(self, reference, candidate):\n",
        "        \"\"\"\n",
        "        ë‹¤ì–‘í•œ BLEU ë³€í˜• ê³„ì‚° (BLEU-1 ~ BLEU-4)\n",
        "\n",
        "        Args:\n",
        "            reference (str): ì°¸ì¡° í…ìŠ¤íŠ¸\n",
        "            candidate (str): ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
        "\n",
        "        Returns:\n",
        "            dict: BLEU ë³€í˜• ì ìˆ˜\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'BLEU-1': self.calculate_bleu(reference, candidate, weights=(1, 0, 0, 0)),\n",
        "            'BLEU-2': self.calculate_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)),\n",
        "            'BLEU-3': self.calculate_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)),\n",
        "            'BLEU-4': self.calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        }\n",
        "\n",
        "# í‰ê°€ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”\n",
        "metrics = EvaluationMetrics()\n",
        "print(\"âœ“ Evaluation Metrics initialized\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ\n",
        "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "candidate_text = \"The quick brown fox jumps over a lazy dog\"\n",
        "\n",
        "bleu_scores = metrics.calculate_bleu_variants(reference_text, candidate_text)\n",
        "print(\"\\nBLEU Score Examples:\")\n",
        "for metric, score in bleu_scores.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Evaluation Metrics initialized\n",
            "\n",
            "BLEU Score Examples:\n",
            "  BLEU-1: 0.8889\n",
            "  BLEU-2: 0.8165\n",
            "  BLEU-3: 0.7273\n",
            "  BLEU-4: 0.6606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## 6. ë‹¤ìŒ ë‹¨ê³„ ë° ê³„íš\n",
        "\n",
        "### ì™„ë£Œëœ ì‘ì—…\n",
        "- âœ… Project Gutenberg ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ\n",
        "- âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ì œê±°, ì •ì œ)\n",
        "- âœ… ì±•í„° ë¶„í•  ê¸°ëŠ¥\n",
        "- âœ… ê°œì²´ëª… ì¶”ì¶œ (ì¸ë¬¼, ì¥ì†Œ, ì‹œê°„)\n",
        "- âœ… ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°í™”\n",
        "- âœ… BLEU í‰ê°€ ì§€í‘œ êµ¬í˜„\n",
        "\n",
        "### í–¥í›„ ì‘ì—… (10ì›” 27ì¼ê¹Œì§€)\n",
        "1. **ë°ì´í„° ìˆ˜ì§‘ í™•ëŒ€**\n",
        "   - ë” ë§ì€ ë„ì„œ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬\n",
        "   - ë‹¤ì–‘í•œ ì¥ë¥´ í™•ë³´ (ì†Œì„¤, ë“œë¼ë§ˆ, ë¯¸ìŠ¤í„°ë¦¬ ë“±)\n",
        "   - êµ­ë‚´ ë””ì§€í„¸ ë„ì„œê´€ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ì—°êµ¬\n",
        "\n",
        "2. **ì „ì²˜ë¦¬ ê³ ë„í™”**\n",
        "   - ëŒ€í™”ë¬¸ê³¼ ì„œìˆ  ë¶„ë¦¬ ì •í™•ë„ í–¥ìƒ\n",
        "   - ì¥ë©´ ì „í™˜ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n",
        "   - ê°ì •/í†¤ ë¶„ì„ ì¶”ê°€\n",
        "\n",
        "3. **ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦**\n",
        "   - ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ê²€ì‚¬\n",
        "   - ë°ì´í„° í†µê³„ ë¶„ì„\n",
        "   - ìƒ˜í”Œ ë°ì´í„° ê²€í† \n",
        "\n",
        "### ëª¨ë¸ë§ ì¤€ë¹„ (10ì›” 29ì¼ ì´í›„)\n",
        "- LLM ëª¨ë¸ ì„ íƒ (GPT-2, T5, BART ë“±)\n",
        "- Fine-tuning ì „ëµ ìˆ˜ë¦½\n",
        "- í•™ìŠµ ë°ì´í„° í¬ë§· ì •ì˜\n",
        "\n",
        "### ì„±ëŠ¥ í‰ê°€ ê³„íš\n",
        "- BLEU Score: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì¸¡ì •\n",
        "- FVD (ì„ íƒ): ë¹„ë””ì˜¤ í’ˆì§ˆ í‰ê°€ (ì—¬ìœ ê°€ ìˆì„ ê²½ìš°)\n",
        "- CLIPScore (ì„ íƒ): í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ (ì—¬ìœ ê°€ ìˆì„ ê²½ìš°)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_checkpoint"
      },
      "source": [
        "## 7. ì§„í–‰ ìƒí™© ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_progress",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d36f24-986d-4d66-872b-26f4177cbbaa"
      },
      "source": [
        "# ì§„í–‰ ìƒí™© ìš”ì•½ ì €ì¥\n",
        "progress_summary = {\n",
        "    'date': '2025-10-12',\n",
        "    'phase': 'Data Preprocessing',\n",
        "    'completed_tasks': [\n",
        "        'Data collection module (Project Gutenberg)',\n",
        "        'Text cleaning and preprocessing',\n",
        "        'Chapter segmentation',\n",
        "        'Entity extraction (characters, locations, time)',\n",
        "        'Script formatting structure',\n",
        "        'BLEU evaluation metric'\n",
        "    ],\n",
        "    'next_tasks': [\n",
        "        'Expand dataset collection',\n",
        "        'Enhance preprocessing accuracy',\n",
        "        'Data quality validation',\n",
        "        'Prepare for modeling phase'\n",
        "    ],\n",
        "    'deadline': '2025-10-27',\n",
        "    'team_notes': 'Following professor\\'s guidance - parallel work on dataset collection and framework setup'\n",
        "}\n",
        "\n",
        "# JSONìœ¼ë¡œ ì €ì¥\n",
        "with open('progress_summary.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(progress_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ“ Progress summary saved to 'progress_summary.json'\")\n",
        "print(\"\\nCurrent Status:\")\n",
        "print(f\"  Phase: {progress_summary['phase']}\")\n",
        "print(f\"  Deadline: {progress_summary['deadline']}\")\n",
        "print(f\"  Completed: {len(progress_summary['completed_tasks'])} tasks\")\n",
        "print(f\"  Remaining: {len(progress_summary['next_tasks'])} tasks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Progress summary saved to 'progress_summary.json'\n",
            "\n",
            "Current Status:\n",
            "  Phase: Data Preprocessing\n",
            "  Deadline: 2025-10-27\n",
            "  Completed: 6 tasks\n",
            "  Remaining: 4 tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chapter_splitting_examples"
      },
      "source": [
        "### 3.1.1 ì±•í„° ë¶„í•  ì˜ˆì‹œ ë° ë””ë²„ê¹…\n",
        "\n",
        "ê°œì„ ëœ ì±•í„° ë¶„í•  ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_chapter_splitting",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38aa14a-4697-4464-8e5e-efc60e8ed3d8"
      },
      "source": [
        "# ì±•í„° ë¶„í•  í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…\n",
        "if 'book_text' in dir() and 'cleaned_text' in dir():\n",
        "    print(\"=== ì±•í„° ë¶„í•  ë””ë²„ê¹… ===\")\n",
        "    print(f\"\\n1. ì›ë³¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ (ì²˜ìŒ 1000ì):\")\n",
        "    print(cleaned_text[:1000])\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # ì±•í„° íŒ¨í„´ ìˆ˜ë™ ê²€ìƒ‰\n",
        "    print(\"2. ì±•í„° íŒ¨í„´ ê²€ìƒ‰ ê²°ê³¼:\")\n",
        "\n",
        "    patterns_to_test = [\n",
        "        (\"CHAPTER/Chapter + ë²ˆí˜¸\", r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)'),\n",
        "        (\"ë¡œë§ˆì/ìˆ«ìë§Œ\", r'\\n\\s*([IVXLCDM]+|\\d+)\\.?\\s*\\n'),\n",
        "        (\"BOOK/PART + ë²ˆí˜¸\", r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)'),\n",
        "    ]\n",
        "\n",
        "    for pattern_name, pattern in patterns_to_test:\n",
        "        matches = re.findall(pattern, cleaned_text)\n",
        "        print(f\"  - {pattern_name}: {len(matches)}ê°œ ë°œê²¬\")\n",
        "        if matches and len(matches) <= 10:\n",
        "            print(f\"    ì˜ˆì‹œ: {matches[:5]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # ì‹¤ì œ ì±•í„° ë¶„í•  ìˆ˜í–‰\n",
        "    print(\"3. ì±•í„° ë¶„í•  ê²°ê³¼:\")\n",
        "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
        "    print(f\"  ì´ {len(chapters)}ê°œ ì±•í„° ë°œê²¬\\n\")\n",
        "\n",
        "    # ì²˜ìŒ 3ê°œ ì±•í„° ë¯¸ë¦¬ë³´ê¸°\n",
        "    for i, chapter in enumerate(chapters[:3], 1):\n",
        "        print(f\"  Chapter {i}:\")\n",
        "        print(f\"    ì œëª©: '{chapter['title']}'\")\n",
        "        print(f\"    ê¸¸ì´: {len(chapter['content'])} ë¬¸ì\")\n",
        "        print(f\"    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chapter['content'][:150]}...\")\n",
        "        print()\n",
        "\n",
        "    # ì±•í„° ê¸¸ì´ í†µê³„\n",
        "    if len(chapters) > 1:\n",
        "        chapter_lengths = [len(ch['content']) for ch in chapters]\n",
        "        print(f\"\\n4. ì±•í„° ê¸¸ì´ í†µê³„:\")\n",
        "        print(f\"  - í‰ê· : {sum(chapter_lengths)/len(chapter_lengths):.0f} ë¬¸ì\")\n",
        "        print(f\"  - ìµœì†Œ: {min(chapter_lengths)} ë¬¸ì\")\n",
        "        print(f\"  - ìµœëŒ€: {max(chapter_lengths)} ë¬¸ì\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ“ ì±•í„° ë¶„í•  ë¶„ì„ ì™„ë£Œ\")\n",
        "else:\n",
        "    print(\"ë¨¼ì € ìƒ˜í”Œ ë„ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì •ì œí•˜ì„¸ìš”.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ì±•í„° ë¶„í•  ë””ë²„ê¹… ===\n",
            "\n",
            "1. ì›ë³¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ (ì²˜ìŒ 1000ì):\n",
            "[Illustration:\n",
            "\n",
            "GEORGE ALLEN\n",
            "PUBLISHER\n",
            "\n",
            "156 CHARING CROSS ROAD\n",
            "LONDON\n",
            "\n",
            "RUSKIN HOUSE\n",
            "]\n",
            "\n",
            "[Illustration:\n",
            "\n",
            "_Reading Janeâ€™s Letters._ _Chap 34._\n",
            "]\n",
            "\n",
            "PRIDE.\n",
            "and\n",
            "PREJUDICE\n",
            "\n",
            "by\n",
            "Jane Austen,\n",
            "\n",
            "with a Preface by\n",
            "George Saintsbury\n",
            "and\n",
            "Illustrations by\n",
            "Hugh Thomson\n",
            "\n",
            "[Illustration: 1894]\n",
            "\n",
            "Ruskin 156. Charing\n",
            "House. Cross Road.\n",
            "\n",
            "London\n",
            "George Allen.\n",
            "\n",
            "CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
            "TOOKS COURT, CHANCERY LANE, LONDON.\n",
            "\n",
            "[Illustration:\n",
            "\n",
            "_To J. Comyns Carr\n",
            "in acknowledgment of all I\n",
            "owe to his friendship and\n",
            "advice, these illustrations are\n",
            "gratefully inscribed_\n",
            "\n",
            "_Hugh Thomson_\n",
            "]\n",
            "\n",
            "PREFACE.\n",
            "\n",
            "[Illustration]\n",
            "\n",
            "_Walt Whitman has somewhere a fine and just distinction between â€œloving\n",
            "by allowanceâ€ and â€œloving with personal love.â€ This distinction applies\n",
            "to books as well as to men and women; and in the case of the not very\n",
            "numerous authors who are the objects of the personal affection, it\n",
            "brings a curious consequence with it. There is much more difference as\n",
            "to their best work than in the case of tho\n",
            "\n",
            "============================================================\n",
            "\n",
            "2. ì±•í„° íŒ¨í„´ ê²€ìƒ‰ ê²°ê³¼:\n",
            "  - CHAPTER/Chapter + ë²ˆí˜¸: 61ê°œ ë°œê²¬\n",
            "  - ë¡œë§ˆì/ìˆ«ìë§Œ: 0ê°œ ë°œê²¬\n",
            "  - BOOK/PART + ë²ˆí˜¸: 0ê°œ ë°œê²¬\n",
            "\n",
            "============================================================\n",
            "\n",
            "3. ì±•í„° ë¶„í•  ê²°ê³¼:\n",
            "  ì´ 61ê°œ ì±•í„° ë°œê²¬\n",
            "\n",
            "  Chapter 1:\n",
            "    ì œëª©: 'Chapter I.]'\n",
            "    ê¸¸ì´: 4741 ë¬¸ì\n",
            "    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune must be in want of a wife.\n",
            "\n",
            "However little known the feeling...\n",
            "\n",
            "  Chapter 2:\n",
            "    ì œëª©: 'CHAPTER II.'\n",
            "    ê¸¸ì´: 4401 ë¬¸ì\n",
            "    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: [Illustration]\n",
            "\n",
            "Mr. Bennet was among the earliest of those who waited on Mr. Bingley. He\n",
            "had always intended to visit him, though to the last always a...\n",
            "\n",
            "  Chapter 3:\n",
            "    ì œëª©: 'CHAPTER III.'\n",
            "    ê¸¸ì´: 9750 ë¬¸ì\n",
            "    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: [Illustration]\n",
            "\n",
            "Not all that Mrs. Bennet, however, with the assistance of her five\n",
            "daughters, could ask on the subject, was sufficient to draw from he...\n",
            "\n",
            "\n",
            "4. ì±•í„° ê¸¸ì´ í†µê³„:\n",
            "  - í‰ê· : 11319 ë¬¸ì\n",
            "  - ìµœì†Œ: 3962 ë¬¸ì\n",
            "  - ìµœëŒ€: 29373 ë¬¸ì\n",
            "\n",
            "============================================================\n",
            "âœ“ ì±•í„° ë¶„í•  ë¶„ì„ ì™„ë£Œ\n"
          ]
        }
      ]
    }
  ]
}