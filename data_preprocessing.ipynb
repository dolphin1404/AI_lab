{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dolphin1404/AI_lab/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ë„ì„œ-ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ í”„ë¡œì íŠ¸: ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "- **ëª©í‘œ**: ë„ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¹„ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” LLM ëª¨ë¸ ê°œë°œ\n",
    "- **ë‹¨ê³„**: ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ë§ â†’ ì„±ëŠ¥í‰ê°€\n",
    "- **ì¼ì •**: ë°ì´í„° ì „ì²˜ë¦¬ ë§ˆê° - 10ì›” 27ì¼\n",
    "\n",
    "## ë°ì´í„° ì „ì²˜ë¦¬ ëª©í‘œ\n",
    "1. ê³µê°œ ë„ì„œ ì•„ì¹´ì´ë¸Œì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘ (Project Gutenberg, êµ­ë‚´ ë””ì§€í„¸ ë„ì„œê´€)\n",
    "2. ë…¸ì´ì¦ˆ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ\n",
    "3. í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ (ì¸ë¬¼, ì¥ì†Œ, ì‹œê°„)\n",
    "4. ë„ì„œ ë¬¸ì²´ â†’ ìŠ¤í¬ë¦½íŠ¸ ë¬¸ì²´ ë³€í™˜ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "install_packages",
    "outputId": "5912e9dc-ac53-4708-a2b0-d4eb08e9abfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting ko-core-news-sm==3.8.0\n",
      "Collecting ko-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
      "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/14.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ko_core_news_sm')\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ko_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install gutenberg requests beautifulsoup4 nltk spacy transformers datasets\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ko_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "import_libraries",
    "outputId": "67937159-6414-4abb-b09f-d32cbacb861e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# SpaCy ëª¨ë¸ ë¡œë“œ\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_collection"
   },
   "source": [
    "## 2. ë°ì´í„° ìˆ˜ì§‘ (Data Collection)\n",
    "\n",
    "### 2.1 Project Gutenbergì—ì„œ ë„ì„œ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gutenberg_collection",
    "outputId": "39f0a7e9-6dbf-4f9c-c42d-e67b4f65f24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š ì´ 10ê°œì˜ ë„ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:\n",
      "  - Book ID: 1342\n",
      "  - Book ID: 2701\n",
      "  - Book ID: 84\n",
      "  - Book ID: 1661\n",
      "  - Book ID: 11\n",
      "  - Book ID: 98\n",
      "  - Book ID: 74\n",
      "  - Book ID: 345\n",
      "  - Book ID: 46\n",
      "  - Book ID: 1952\n"
     ]
    }
   ],
   "source": [
    "# êµ¬í…ë²„ê·¸ì—ì„œ ìˆ˜ì§‘í•  ë„ì„œ ID ë¦¬ìŠ¤íŠ¸ (ëœë¤ 10ê°œ)\n",
    "# ì¸ê¸°ìˆê³  ì±•í„° êµ¬ë¶„ì´ ëª…í™•í•œ ê³ ì „ ì†Œì„¤ë“¤ì„ ì„ íƒ\n",
    "# ê° IDëŠ” Project Gutenbergì˜ ì±… ë²ˆí˜¸ì…ë‹ˆë‹¤\n",
    "\n",
    "import random\n",
    "\n",
    "popular_book_ids = [\n",
    "    1342,  # Pride and Prejudice by Jane Austen\n",
    "    2701,  # Moby Dick by Herman Melville  \n",
    "    84,    # Frankenstein by Mary Shelley\n",
    "    1661,  # The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "    11,    # Alice's Adventures in Wonderland by Lewis Carroll\n",
    "    98,    # A Tale of Two Cities by Charles Dickens\n",
    "    74,    # The Adventures of Tom Sawyer by Mark Twain\n",
    "    345,   # Dracula by Bram Stoker\n",
    "    46,    # A Christmas Carol by Charles Dickens\n",
    "    1952,  # The Yellow Wallpaper by Charlotte Perkins Gilman\n",
    "]\n",
    "\n",
    "# ëœë¤í•˜ê²Œ 10ê°œ ì„ íƒ (ì´ë¯¸ 10ê°œì´ë¯€ë¡œ ì „ì²´ ì‚¬ìš©)\n",
    "random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    "book_ids_to_process = popular_book_ids[:10]\n",
    "\n",
    "print(f\"ğŸ“š ì´ {len(book_ids_to_process)}ê°œì˜ ë„ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:\")\n",
    "for book_id in book_ids_to_process:\n",
    "    print(f\"  - Book ID: {book_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gutenberg Collector initialized\n"
     ]
    }
   ],
   "source": [
    "class GutenbergCollector:\n",
    "    \"\"\"Project Gutenbergì—ì„œ ë„ì„œë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.gutenberg.org/files/\"\n",
    "        self.cache_dir = \"./gutenberg_cache\"\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    \n",
    "    def download_book(self, book_id):\n",
    "        \"\"\"\n",
    "        ë„ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "        \n",
    "        Args:\n",
    "            book_id (int): ë„ì„œ ID\n",
    "            \n",
    "        Returns:\n",
    "            str: ë„ì„œ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ None)\n",
    "        \"\"\"\n",
    "        # ìºì‹œ í™•ì¸\n",
    "        cache_file = os.path.join(self.cache_dir, f\"book_{book_id}.txt\")\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"  âœ“ Loading from cache: {cache_file}\")\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        # ë‹¤ìš´ë¡œë“œ ì‹œë„\n",
    "        url = f\"{self.base_url}{book_id}/{book_id}-0.txt\"\n",
    "        try:\n",
    "            print(f\"  Downloading from: {url}\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            text = response.text\n",
    "            \n",
    "            # ìºì‹œì— ì €ì¥\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Failed to download book {book_id}: {e}\")\n",
    "            \n",
    "            # ëŒ€ì²´ URL ì‹œë„ (-0 ì—†ì´)\n",
    "            alt_url = f\"{self.base_url}{book_id}/{book_id}.txt\"\n",
    "            try:\n",
    "                print(f\"  Trying alternative URL: {alt_url}\")\n",
    "                response = requests.get(alt_url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                text = response.text\n",
    "                \n",
    "                # ìºì‹œì— ì €ì¥\n",
    "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(text)\n",
    "                \n",
    "                return text\n",
    "            except Exception as e2:\n",
    "                print(f\"  âœ— Alternative URL also failed: {e2}\")\n",
    "                return None\n",
    "\n",
    "# GutenbergCollector ì´ˆê¸°í™”\n",
    "collector = GutenbergCollector()\n",
    "print(\"âœ“ Gutenberg Collector initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¥ ìƒ˜í”Œ ë„ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘... (Book ID: 1342)\n",
      "  âœ“ Loading from cache: ./gutenberg_cache/book_1342.txt\n",
      "âœ“ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: 728846 ë¬¸ì\n",
      "\n",
      "í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "\n",
      "                        156 CHARING CROSS ROAD\n",
      "                                LONDON\n",
      "\n",
      "                             RUSKIN HOUSE\n",
      "                                   ]\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "               _Reading Janeâ€™s Letters._      _Chap 34._\n",
      "                                   ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ë„ì„œ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ë„ì„œ)\n",
    "if book_ids_to_process:\n",
    "    book_id = book_ids_to_process[0]\n",
    "    print(f\"\\nğŸ“¥ ìƒ˜í”Œ ë„ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘... (Book ID: {book_id})\")\n",
    "    book_text = collector.download_book(book_id)\n",
    "    \n",
    "    if book_text:\n",
    "        print(f\"âœ“ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(book_text)} ë¬¸ì\")\n",
    "        print(f\"\\ní…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):\")\n",
    "        print(book_text[:500])\n",
    "    else:\n",
    "        print(\"âœ— ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "        book_text = None\n",
    "else:\n",
    "    print(\"book_ids_to_processê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## 3. ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)\n",
    "\n",
    "### 3.1 í…ìŠ¤íŠ¸ ì •ì œ (Text Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "text_cleaning"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "# nlp_en is not defined in the provided code, so I'm commenting it out.\n",
    "# You would typically initialize a spaCy model here, for example:\n",
    "# import spacy\n",
    "# nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.nlp = nlp_en\n",
    "        pass\n",
    "\n",
    "    def remove_gutenberg_header_footer(self, text):\n",
    "        \"\"\"\n",
    "        Project Gutenberg í—¤ë”ì™€ í‘¸í„° ì œê±°\n",
    "\n",
    "        Args:\n",
    "            text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "\n",
    "        Returns:\n",
    "            str: í—¤ë”/í‘¸í„°ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ì‹œì‘ ë§ˆì»¤ ì°¾ê¸°\n",
    "        start_markers = [\n",
    "            \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "            \"*** START OF THE PROJECT GUTENBERG\",\n",
    "            \"***START OF THE PROJECT GUTENBERG\"\n",
    "        ]\n",
    "\n",
    "        # ì¢…ë£Œ ë§ˆì»¤ ì°¾ê¸°\n",
    "        end_markers = [\n",
    "            \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "            \"*** END OF THE PROJECT GUTENBERG\",\n",
    "            \"***END OF THE PROJECT GUTENBERG\"\n",
    "        ]\n",
    "\n",
    "        start_idx = 0\n",
    "        for marker in start_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                start_idx = text.find('\\n', idx) + 1\n",
    "                break\n",
    "\n",
    "        end_idx = len(text)\n",
    "        for marker in end_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                end_idx = idx\n",
    "                break\n",
    "\n",
    "        return text[start_idx:end_idx].strip()\n",
    "\n",
    "    def remove_table_of_contents(self, text):\n",
    "        \"\"\"\n",
    "        ëª©ì°¨(Table of Contents) ì„¹ì…˜ ì œê±° (v4 - ê°œì„ ëœ ì •í™•ë„)\n",
    "\n",
    "        ê°œì„  ì‚¬í•­:\n",
    "        - ì‹¤ì œ ì±•í„°ì™€ ëª©ì°¨ í•­ëª© êµ¬ë¶„ ê°•í™”\n",
    "        - Chapter Ië¶€í„° ì •í™•íˆ ë³´ì¡´\n",
    "        - ëª©ì°¨ íŒ¨í„´ ë” ì •ë°€í•˜ê²Œ ê°ì§€\n",
    "\n",
    "        Args:\n",
    "            text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "\n",
    "        Returns:\n",
    "            str: ëª©ì°¨ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        result_lines = []\n",
    "        in_toc = False\n",
    "        toc_start_idx = -1\n",
    "        toc_line_count = 0\n",
    "        consecutive_heading_lines = 0\n",
    "        found_first_chapter = False\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line_stripped = line.strip()\n",
    "            line_lower = line_stripped.lower()\n",
    "\n",
    "            # ëª©ì°¨ ì‹œì‘ ê°ì§€ (ë” ì •í™•í•œ íŒ¨í„´)\n",
    "            if not in_toc and not found_first_chapter:\n",
    "                # \"CONTENTS\" ë‹¨ë…ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²½ìš° (ì¼ë°˜ì ì¸ ëª©ì°¨ ì‹œì‘)\n",
    "                if line_stripped.upper() in ['CONTENTS', 'TABLE OF CONTENTS', 'LIST OF CHAPTERS']:\n",
    "                    in_toc = True\n",
    "                    toc_start_idx = i\n",
    "                    toc_line_count = 0\n",
    "                    continue\n",
    "\n",
    "                # \"Heading to Chapter\" íŒ¨í„´ ì—¬ëŸ¬ ì¤„ ì—°ì† (Pride and Prejudice ìŠ¤íƒ€ì¼)\n",
    "                if 'heading to chapter' in line_lower or 'heading to CHAPTER' in line_lower:\n",
    "                    consecutive_heading_lines += 1\n",
    "                    # 2ì¤„ ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ë©´ ëª©ì°¨\n",
    "                    if consecutive_heading_lines >= 2:\n",
    "                        in_toc = True\n",
    "                        toc_start_idx = i - consecutive_heading_lines\n",
    "                        continue\n",
    "                else:\n",
    "                    if 0 < consecutive_heading_lines < 2:\n",
    "                        # 1ì¤„ë§Œ ìˆì—ˆìœ¼ë©´ ì‹¤ì œ ë‚´ìš©ì¼ ìˆ˜ ìˆìŒ\n",
    "                        consecutive_heading_lines = 0\n",
    "\n",
    "            # ì‹¤ì œ ì±•í„° ì‹œì‘ í™•ì¸ (ëª©ì°¨ ì¢…ë£Œ ë˜ëŠ” ì²« ì±•í„° ë°œê²¬)\n",
    "            # ë” ì—„ê²©í•œ ì¡°ê±´: ì¤„ì´ ì§§ê³ (80ì ì´í•˜), íŒ¨í„´ì´ ëª…í™•í•˜ê³ , ë‹¤ìŒ ì¤„ì— ë‚´ìš©ì´ ìˆì–´ì•¼ í•¨\n",
    "            is_real_chapter = False\n",
    "            if len(line_stripped) <= 80:\n",
    "                # \"CHAPTER I\", \"Chapter 1\", \"CHAPTER ONE\" ë“±ì˜ íŒ¨í„´\n",
    "                chapter_pattern = r'^\\s*(CHAPTER|Chapter)\\s+(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX|XXI|XXII|XXIII|XXIV|XXV|XXVI|XXVII|XXVIII|XXIX|XXX|XXXI|XXXII|XXXIII|XXXIV|XXXV|XXXVI|XXXVII|XXXVIII|XXXIX|XL|XLI|XLII|XLIII|XLIV|XLV|XLVI|XLVII|XLVIII|XLIX|L|LI|LII|LIII|LIV|LV|LVI|LVII|LVIII|LIX|LX|LXI|LXII|LXIII|LXIV|LXV|LXVI|LXVII|LXVIII|LXIX|LXX|\\d+|One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty)\\.?\\s*$'\n",
    "                chapter_match = re.match(chapter_pattern, line_stripped)\n",
    "\n",
    "                if chapter_match:\n",
    "                    # ë‹¤ìŒ ëª‡ ì¤„ì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ ì±•í„° ë‚´ìš©ì´ ìˆëŠ”ì§€ ê²€ì¦\n",
    "                    has_content_after = False\n",
    "                    for j in range(i + 1, min(i + 10, len(lines))):\n",
    "                        next_line = lines[j].strip()\n",
    "                        # ë¹ˆ ì¤„ì´ ì•„ë‹ˆê³ , 40ì ì´ìƒì˜ ì‹¤ì œ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ì±•í„°ë¡œ ê°„ì£¼\n",
    "                        if next_line and len(next_line) > 40:\n",
    "                            # ëª©ì°¨ í‚¤ì›Œë“œê°€ ì—†ì–´ì•¼ í•¨\n",
    "                            if 'heading to' not in next_line.lower() and 'page' not in next_line.lower():\n",
    "                                has_content_after = True\n",
    "                                break\n",
    "\n",
    "                    if has_content_after:\n",
    "                        is_real_chapter = True\n",
    "                        found_first_chapter = True\n",
    "\n",
    "                        # ëª©ì°¨ ì¤‘ì´ì—ˆë‹¤ë©´ ëª©ì°¨ ì¢…ë£Œ\n",
    "                        if in_toc and toc_line_count > 3:\n",
    "                            in_toc = False\n",
    "                            # ëª©ì°¨ ë¶€ë¶„ ìŠ¤í‚µ (ì´ë¯¸ ì¶”ê°€ ì•ˆ í•¨)\n",
    "\n",
    "            # ëª©ì°¨ ì¤‘ì´ë©´ ì¤„ ì¹´ìš´íŠ¸ë§Œ ì¦ê°€í•˜ê³  ì¶”ê°€ ì•ˆ í•¨\n",
    "            if in_toc:\n",
    "                toc_line_count += 1\n",
    "                # ëª©ì°¨ê°€ ë„ˆë¬´ ê¸¸ë©´ (150ì¤„ ì´ìƒ) ì¢…ë£Œ\n",
    "                if toc_line_count > 150:\n",
    "                    in_toc = False\n",
    "                continue # ëª©ì°¨ ë‚´ìš©ì€ ê²°ê³¼ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "            # ì‹¤ì œ ì±•í„°ë¥¼ ë§Œë‚¬ê±°ë‚˜, ëª©ì°¨ ìƒíƒœê°€ ì•„ë‹ˆë©´ ë¼ì¸ ì¶”ê°€\n",
    "            if is_real_chapter or not in_toc:\n",
    "                result_lines.append(line)\n",
    "\n",
    "        return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ\n",
    "        - ê³¼ë„í•œ ê³µë°± ì œê±°\n",
    "        - íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”\n",
    "        - ì¤„ë°”ê¿ˆ ì •ê·œí™”\n",
    "\n",
    "        Args:\n",
    "            text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "\n",
    "        Returns:\n",
    "            str: ì •ì œëœ í…ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ ë‹¨ì¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # ì¤„ ì‹œì‘/ë ê³µë°± ì œê±°\n",
    "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "        return text.strip()\n",
    "\n",
    "    def split_into_chapters(self, text):\n",
    "        \"\"\"\n",
    "        í…ìŠ¤íŠ¸ë¥¼ ì±•í„°ë³„ë¡œ ë¶„í•  (ê°œì„ ëœ ë²„ì „ v2)\n",
    "\n",
    "        ê°œì„  ì‚¬í•­:\n",
    "        - ëª©ì°¨(Table of Contents) ìë™ ì œê±°\n",
    "        - \"Heading to Chapter\" ê°™ì€ ëª©ì°¨ í•­ëª© í•„í„°ë§\n",
    "        - ì‹¤ì œ ì±•í„° ë‚´ìš©ë§Œ ì¶”ì¶œ\n",
    "        - ë” ì •í™•í•œ ì±•í„° ê°ì§€\n",
    "\n",
    "        ì§€ì›í•˜ëŠ” í˜•ì‹:\n",
    "        - \"CHAPTER I\", \"CHAPTER 1\", \"CHAPTER ONE\"\n",
    "        - \"Chapter I.\", \"Chapter 1.\", \"Chapter One.\"\n",
    "        - ì‹¤ì œ ì±•í„° ì œëª©ì´ ìˆëŠ” ê²½ìš° (e.g., \"CHAPTER I. The Beginning\")\n",
    "\n",
    "        Args:\n",
    "            text (str): ì „ì²´ í…ìŠ¤íŠ¸\n",
    "\n",
    "        Returns:\n",
    "            list: ì±•í„°ë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # 1ë‹¨ê³„: ëª©ì°¨ ì œê±°\n",
    "        text_without_toc = self.remove_table_of_contents(text)\n",
    "\n",
    "        # 2ë‹¨ê³„: ì±•í„° íŒ¨í„´ ë§¤ì¹­\n",
    "        # ì•ˆì „í•œ íŒ¨í„´ ì‚¬ìš© (catastrophic backtracking ë°©ì§€)\n",
    "        word_numbers = \"One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten|Eleven|Twelve|Thirteen|Fourteen|Fifteen|Sixteen|Seventeen|Eighteen|Nineteen|Twenty|Twenty-one|Twenty-two|Twenty-three|Twenty-four|Twenty-five|Twenty-six|Twenty-seven|Twenty-eight|Twenty-nine|Thirty|Thirty-one|Thirty-two|Thirty-three|Thirty-four|Thirty-five|Thirty-six|Thirty-seven|Thirty-eight|Thirty-nine|Forty|Forty-one|Forty-two|Forty-three|Forty-four|Forty-five|Forty-six|Forty-seven|Forty-eight|Forty-nine|Fifty|Fifty-one|Fifty-two|Fifty-three|Fifty-four|Fifty-five|Fifty-six|Fifty-seven|Fifty-eight|Fifty-nine|Sixty|Sixty-one\"\n",
    "\n",
    "        patterns = [\n",
    "            r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+|' + word_numbers + r')(?:\\.\\s*|\\s+|\\n)',\n",
    "            # Pattern 2: \"BOOK I\", \"PART I\"\n",
    "            r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)(?:\\.\\s*|\\s+|\\n)',\n",
    "        ]\n",
    "\n",
    "        best_split = None\n",
    "        best_count = 0\n",
    "        best_pattern_idx = -1\n",
    "\n",
    "        # ê° íŒ¨í„´ ì‹œë„ (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)\n",
    "        for idx, pattern in enumerate(patterns):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                splits = re.split(pattern, text_without_toc)\n",
    "\n",
    "                # íƒ€ì„ì•„ì›ƒ ì²´í¬ (3ì´ˆ)\n",
    "                if time.time() - start_time > 3:\n",
    "                    print(f\"Warning: Pattern {idx} took too long, skipping\")\n",
    "                    continue\n",
    "\n",
    "                # ì±•í„° ìˆ˜ ê³„ì‚° (3ê°œ ìš”ì†Œê°€ 1ê°œ ì±•í„°: prefix, number, content)\n",
    "                chapter_count = (len(splits) - 1) // 3 if len(splits) > 1 else 0\n",
    "\n",
    "                # í•©ë¦¬ì ì¸ ë²”ìœ„ì˜ ì±•í„° ìˆ˜ (2-150ê°œ)\n",
    "                if 2 <= chapter_count <= 150:\n",
    "                    # ì±•í„° í‰ê·  ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ìœ¼ë©´ ëª©ì°¨ì¼ ê°€ëŠ¥ì„±)\n",
    "                    total_length = sum(len(splits[i]) for i in range(2, len(splits), 3) if i < len(splits))\n",
    "                    avg_length = total_length / chapter_count if chapter_count > 0 else 0\n",
    "\n",
    "                    # í‰ê·  ê¸¸ì´ê°€ 300ì ì´ìƒì´ì–´ì•¼ ì‹¤ì œ ì±•í„°\n",
    "                    if avg_length >= 300 and chapter_count > best_count:\n",
    "                        best_count = chapter_count\n",
    "                        best_split = (pattern, splits)\n",
    "                        best_pattern_idx = idx\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Pattern {idx} failed with error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 3ë‹¨ê³„: ì±•í„° ì¶”ì¶œ\n",
    "        if best_split:\n",
    "            pattern, chapters = best_split\n",
    "            result = []\n",
    "\n",
    "            for i in range(1, len(chapters), 3):\n",
    "                if i + 2 <= len(chapters):\n",
    "                    chapter_prefix = chapters[i].strip()    # \"CHAPTER\" or \"Chapter\"\n",
    "                    chapter_number = chapters[i + 1].strip()  # \"I\", \"1\", etc.\n",
    "                    chapter_content = chapters[i + 2].strip() if i + 2 < len(chapters) else \"\"\n",
    "\n",
    "                    # ì±•í„° ë‚´ìš©ì´ ì¶©ë¶„íˆ ê¸´ì§€ í™•ì¸ (ìµœì†Œ 200ì)\n",
    "                    if len(chapter_content) < 200:\n",
    "                        continue\n",
    "\n",
    "                    # ì±•í„° ì œëª©ì—ì„œ ì‹¤ì œ ì œëª© ì¶”ì¶œ\n",
    "                    content_lines = chapter_content.split('\\n', 2)\n",
    "                    chapter_title_suffix = \"\"\n",
    "\n",
    "                    # ì²« ì¤„ì´ ì§§ìœ¼ë©´ (60ì ì´í•˜) ì±•í„° ì œëª©ìœ¼ë¡œ ê°„ì£¼\n",
    "                    if content_lines and len(content_lines[0]) <= 60 and content_lines[0].strip():\n",
    "                        chapter_title_suffix = content_lines[0].strip()\n",
    "                        # ì œëª©ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ê°€ ë‚´ìš©\n",
    "                        if len(content_lines) > 1:\n",
    "                            chapter_content = '\\n'.join(content_lines[1:]).strip()\n",
    "\n",
    "                    # ìµœì¢… ì±•í„° ì œëª© ìƒì„±\n",
    "                    if chapter_prefix:\n",
    "                        if chapter_title_suffix:\n",
    "                            chapter_title = f\"{chapter_prefix} {chapter_number}. {chapter_title_suffix}\"\n",
    "                        else:\n",
    "                            chapter_title = f\"{chapter_prefix} {chapter_number}\"\n",
    "                    else:\n",
    "                        chapter_title = f\"Chapter {chapter_number}\"\n",
    "\n",
    "                    if chapter_content:  # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì¶”ê°€\n",
    "                        result.append({\n",
    "                            'title': chapter_title,\n",
    "                            'content': chapter_content\n",
    "                        })\n",
    "\n",
    "            # ê²°ê³¼ ê²€ì¦: ìµœì†Œ 2ê°œ ì´ìƒì˜ ì±•í„°\n",
    "            if len(result) >= 2:\n",
    "                return result\n",
    "\n",
    "        # 4ë‹¨ê³„: Fallback - ìˆ˜ë™ ë¼ì¸ ë¶„ì„\n",
    "        return self._fallback_chapter_split(text_without_toc)\n",
    "\n",
    "\n",
    "    def _fallback_chapter_split(self, text):\n",
    "        \"\"\"\n",
    "        Fallback ë°©ë²•: ì¤„ ë‹¨ìœ„ë¡œ ì±•í„° ë§ˆì»¤ ì°¾ê¸°\n",
    "        ëª©ì°¨ í•­ëª©ì€ ì œì™¸í•˜ê³  ì‹¤ì œ ì±•í„°ë§Œ ì¶”ì¶œ\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        chapters = []\n",
    "        current_chapter = None\n",
    "        current_content = []\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line_stripped = line.strip()\n",
    "            line_upper = line_stripped.upper()\n",
    "\n",
    "            # ëª©ì°¨ í•­ëª© ê±´ë„ˆë›°ê¸°\n",
    "            if 'HEADING TO' in line_upper or 'CONTENTS' in line_upper:\n",
    "                continue\n",
    "\n",
    "            # ì±•í„° í—¤ë” ê°ì§€\n",
    "            is_chapter = False\n",
    "            chapter_title = None\n",
    "\n",
    "            # ì§§ì€ ì¤„ (60ì ì´í•˜)ì—ì„œë§Œ ì±•í„° ê²€ì‚¬\n",
    "            if len(line_stripped) <= 60 and line_stripped:\n",
    "                # \"CHAPTER X\" í˜•ì‹\n",
    "                chapter_match = re.match(r'^(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)', line_stripped)\n",
    "                if chapter_match:\n",
    "                    is_chapter = True\n",
    "                    chapter_title = line_stripped\n",
    "                # \"BOOK X\", \"PART X\" í˜•ì‹\n",
    "                elif re.match(r'^(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)', line_stripped):\n",
    "                    is_chapter = True\n",
    "                    chapter_title = line_stripped\n",
    "\n",
    "            if is_chapter and chapter_title:\n",
    "                # ì´ì „ ì±•í„° ì €ì¥\n",
    "                if current_chapter and current_content:\n",
    "                    content_text = '\\n'.join(current_content).strip()\n",
    "                    # ë‚´ìš©ì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°ë§Œ (ìµœì†Œ 300ì)\n",
    "                    if len(content_text) >= 300:\n",
    "                        chapters.append({\n",
    "                            'title': current_chapter,\n",
    "                            'content': content_text\n",
    "                        })\n",
    "\n",
    "                # ìƒˆ ì±•í„° ì‹œì‘\n",
    "                current_chapter = chapter_title\n",
    "                current_content = []\n",
    "            else:\n",
    "                # í˜„ì¬ ì±•í„°ì— ë‚´ìš© ì¶”ê°€\n",
    "                if current_chapter:  # ì±•í„°ê°€ ì‹œì‘ëœ í›„ì—ë§Œ\n",
    "                    current_content.append(line)\n",
    "\n",
    "        # ë§ˆì§€ë§‰ ì±•í„° ì €ì¥\n",
    "        if current_chapter and current_content:\n",
    "            content_text = '\\n'.join(current_content).strip()\n",
    "            if len(content_text) >= 300:\n",
    "                chapters.append({\n",
    "                    'title': current_chapter,\n",
    "                    'content': content_text\n",
    "                })\n",
    "\n",
    "        # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì±•í„°ê°€ ìˆì–´ì•¼ ìœ íš¨\n",
    "        if len(chapters) >= 2:\n",
    "            return chapters\n",
    "        else:\n",
    "            return [{'title': 'Full Text', 'content': text}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apply_preprocessing",
    "outputId": "5fd67467-0957-4631-be8e-b6aa2b12b929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì „ì²˜ë¦¬ ì‹œì‘ ===\n",
      "\n",
      "âœ“ Original length: 728,846 characters\n",
      "âœ“ Cleaned length: 720,973 characters\n",
      "âœ“ Removed: 7,873 characters\n",
      "\n",
      "ì±•í„° ë¶„í•  ì¤‘...\n",
      "âœ“ Found 61 chapters\n",
      "\n",
      "ì²« ë²ˆì§¸ ì±•í„°:\n",
      "  ì œëª©: Chapter I.]\n",
      "  ê¸¸ì´: 4,741 ë¬¸ì\n",
      "  ë¯¸ë¦¬ë³´ê¸°: It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first enter...\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# ìƒ˜í”Œ ë„ì„œì— ì „ì²˜ë¦¬ ì ìš©\n",
    "if 'book_text' in globals() and book_text:\n",
    "    print(\"=== ì „ì²˜ë¦¬ ì‹œì‘ ===\\n\")\n",
    "    \n",
    "    # í—¤ë”/í‘¸í„° ì œê±°\n",
    "    cleaned_text = preprocessor.remove_gutenberg_header_footer(book_text)\n",
    "    cleaned_text = preprocessor.clean_text(cleaned_text)\n",
    "\n",
    "    print(f\"âœ“ Original length: {len(book_text):,} characters\")\n",
    "    print(f\"âœ“ Cleaned length: {len(cleaned_text):,} characters\")\n",
    "    print(f\"âœ“ Removed: {len(book_text) - len(cleaned_text):,} characters\")\n",
    "\n",
    "    # ì±•í„° ë¶„í• \n",
    "    print(f\"\\nì±•í„° ë¶„í•  ì¤‘...\")\n",
    "    chapters = preprocessor.split_into_chapters(cleaned_text)\n",
    "    print(f\"âœ“ Found {len(chapters)} chapters\")\n",
    "\n",
    "    if chapters:\n",
    "        print(f\"\\nì²« ë²ˆì§¸ ì±•í„°:\")\n",
    "        print(f\"  ì œëª©: {chapters[0]['title']}\")\n",
    "        print(f\"  ê¸¸ì´: {len(chapters[0]['content']):,} ë¬¸ì\")\n",
    "        print(f\"  ë¯¸ë¦¬ë³´ê¸°: {chapters[0]['content'][:200]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë¨¼ì € book_textë¥¼ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš” (ìœ„ì˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "entity_extraction"
   },
   "source": [
    "### 3.2 í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ (Entity Extraction)\n",
    "\n",
    "ì¸ë¬¼, ì¥ì†Œ, ì‹œê°„ ë“± í•µì‹¬ ìš”ì†Œë¥¼ ì¶”ì¶œí•˜ì—¬ ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ì— í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "entity_extractor",
    "outputId": "cc3e3515-6b38-4798-9bdd-a7175709dd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Entity Extractor initialized\n"
     ]
    }
   ],
   "source": [
    "class EntityExtractor:\n",
    "    \"\"\"Named Entity Recognitionì„ í†µí•œ í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp_en\n",
    "\n",
    "    def extract_entities(self, text, max_length=1000000):\n",
    "        \"\"\"\n",
    "        í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª… ì¶”ì¶œ\n",
    "\n",
    "        Args:\n",
    "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "            max_length (int): ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "\n",
    "        Returns:\n",
    "            dict: ì¹´í…Œê³ ë¦¬ë³„ ê°œì²´ëª… ì‚¬ì „\n",
    "        \"\"\"\n",
    "        # SpaCyì˜ max_length ì„¤ì •\n",
    "        self.nlp.max_length = max_length\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ë¶„ì„\n",
    "        doc = self.nlp(text[:max_length])\n",
    "\n",
    "        # ê°œì²´ëª… ë¶„ë¥˜\n",
    "        entities = {\n",
    "            'PERSON': [],      # ì¸ë¬¼\n",
    "            'GPE': [],          # ì§€ì •í•™ì  ê°œì²´ (ë„ì‹œ, êµ­ê°€ ë“±)\n",
    "            'LOC': [],          # ìœ„ì¹˜\n",
    "            'DATE': [],         # ë‚ ì§œ\n",
    "            'TIME': [],         # ì‹œê°„\n",
    "            'ORG': [],          # ì¡°ì§\n",
    "            'EVENT': []         # ì´ë²¤íŠ¸\n",
    "        }\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in entities:\n",
    "                entities[ent.label_].append(ent.text)\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "        for key in entities:\n",
    "            entity_counts = defaultdict(int)\n",
    "            for entity in entities[key]:\n",
    "                entity_counts[entity] += 1\n",
    "            entities[key] = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def get_main_characters(self, entities, top_n=10):\n",
    "        \"\"\"\n",
    "        ì£¼ìš” ì¸ë¬¼ ì¶”ì¶œ\n",
    "\n",
    "        Args:\n",
    "            entities (dict): extract_entitiesì˜ ê²°ê³¼\n",
    "            top_n (int): ë°˜í™˜í•  ì£¼ìš” ì¸ë¬¼ ìˆ˜\n",
    "\n",
    "        Returns:\n",
    "            list: ì£¼ìš” ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        return entities['PERSON'][:top_n]\n",
    "\n",
    "    def get_main_locations(self, entities, top_n=10):\n",
    "        \"\"\"\n",
    "        ì£¼ìš” ì¥ì†Œ ì¶”ì¶œ\n",
    "\n",
    "        Args:\n",
    "            entities (dict): extract_entitiesì˜ ê²°ê³¼\n",
    "            top_n (int): ë°˜í™˜í•  ì£¼ìš” ì¥ì†Œ ìˆ˜\n",
    "\n",
    "        Returns:\n",
    "            list: ì£¼ìš” ì¥ì†Œ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        locations = entities['GPE'] + entities['LOC']\n",
    "        # ë¹ˆë„ìˆ˜ë¡œ ì¬ì •ë ¬\n",
    "        location_dict = defaultdict(int)\n",
    "        for loc, count in locations:\n",
    "            location_dict[loc] += count\n",
    "        return sorted(location_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "extractor = EntityExtractor()\n",
    "print(\"âœ“ Entity Extractor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "extract_sample_entities",
    "outputId": "b167395d-c00e-4047-ab13-cccdfcdaafdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ê°œì²´ëª… ì¶”ì¶œ ì¤‘ ===\n",
      "\n",
      "âœ“ Entity extraction completed\n",
      "\n",
      "ğŸ“ Main Characters:\n",
      "  - Bennet: 6 mentions\n",
      "  - Bingley: 4 mentions\n",
      "  - George Allen: 3 mentions\n",
      "  - Lizzy: 3 mentions\n",
      "  - Long: 2 mentions\n",
      "\n",
      "ğŸ—ºï¸  Main Locations:\n",
      "  - England: 1 mentions\n",
      "  - Lydia: 1 mentions\n",
      "\n",
      "ğŸ“… Temporal Information:\n",
      "  - 1894: 3 mentions\n",
      "  - one day: 1 mentions\n",
      "  - Monday: 1 mentions\n",
      "  - the end of next week: 1 mentions\n",
      "  - five thousand a year: 1 mentions\n",
      "âœ“ Entity extraction completed\n",
      "\n",
      "ğŸ“ Main Characters:\n",
      "  - Bennet: 6 mentions\n",
      "  - Bingley: 4 mentions\n",
      "  - George Allen: 3 mentions\n",
      "  - Lizzy: 3 mentions\n",
      "  - Long: 2 mentions\n",
      "\n",
      "ğŸ—ºï¸  Main Locations:\n",
      "  - England: 1 mentions\n",
      "  - Lydia: 1 mentions\n",
      "\n",
      "ğŸ“… Temporal Information:\n",
      "  - 1894: 3 mentions\n",
      "  - one day: 1 mentions\n",
      "  - Monday: 1 mentions\n",
      "  - the end of next week: 1 mentions\n",
      "  - five thousand a year: 1 mentions\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ì±•í„°ì—ì„œ ê°œì²´ëª… ì¶”ì¶œ\n",
    "if 'chapters' in globals() and chapters:\n",
    "    # ì²« ë²ˆì§¸ ì±•í„° ë¶„ì„\n",
    "    sample_chapter = chapters[0]['content']\n",
    "    print(\"=== ê°œì²´ëª… ì¶”ì¶œ ì¤‘ ===\\n\")\n",
    "    entities = extractor.extract_entities(sample_chapter)\n",
    "\n",
    "    print(\"âœ“ Entity extraction completed\\n\")\n",
    "\n",
    "    # ì£¼ìš” ì¸ë¬¼\n",
    "    main_characters = extractor.get_main_characters(entities, top_n=5)\n",
    "    print(\"ğŸ“ Main Characters:\")\n",
    "    for char, count in main_characters:\n",
    "        print(f\"  - {char}: {count} mentions\")\n",
    "\n",
    "    # ì£¼ìš” ì¥ì†Œ\n",
    "    main_locations = extractor.get_main_locations(entities, top_n=5)\n",
    "    print(\"\\nğŸ—ºï¸  Main Locations:\")\n",
    "    for loc, count in main_locations:\n",
    "        print(f\"  - {loc}: {count} mentions\")\n",
    "\n",
    "    # ì‹œê°„ ì •ë³´\n",
    "    if entities['DATE']:\n",
    "        print(\"\\nğŸ“… Temporal Information:\")\n",
    "        for date, count in entities['DATE'][:5]:\n",
    "            print(f\"  - {date}: {count} mentions\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë¨¼ì € chaptersë¥¼ ìƒì„±í•´ì£¼ì„¸ìš” (ìœ„ì˜ ì „ì²˜ë¦¬ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "script_conversion"
   },
   "source": [
    "### 3.3 ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°í™”\n",
    "\n",
    "ë„ì„œ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë¦½íŠ¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "script_formatter",
    "outputId": "f2157de3-bfe9-4185-d737-5a7a25d576ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Script Formatter initialized\n"
     ]
    }
   ],
   "source": [
    "class ScriptFormatter:\n",
    "    \"\"\"ë„ì„œ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë¦½íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp_en\n",
    "\n",
    "    def extract_dialogues(self, text):\n",
    "        \"\"\"\n",
    "        í…ìŠ¤íŠ¸ì—ì„œ ëŒ€í™”ë¬¸ ì¶”ì¶œ\n",
    "\n",
    "        Args:\n",
    "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "\n",
    "        Returns:\n",
    "            list: ëŒ€í™”ë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ëŒ€í™”ë¬¸ ì¶”ì¶œ\n",
    "        dialogue_pattern = r'[\"\\']([^\"\\']+)[\"\\']'\n",
    "        dialogues = re.findall(dialogue_pattern, text)\n",
    "\n",
    "        # ì§§ì€ ëŒ€í™” í•„í„°ë§ (3ë‹¨ì–´ ì´ìƒ)\n",
    "        dialogues = [d for d in dialogues if len(d.split()) >= 3]\n",
    "\n",
    "        return dialogues\n",
    "\n",
    "    def extract_narrative(self, text):\n",
    "        \"\"\"\n",
    "        ì„œìˆ  ë¶€ë¶„ ì¶”ì¶œ (ëŒ€í™”ê°€ ì•„ë‹Œ ë¶€ë¶„)\n",
    "\n",
    "        Args:\n",
    "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "\n",
    "        Returns:\n",
    "            str: ì„œìˆ  í…ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ëŒ€í™”ë¬¸ ì œê±°\n",
    "        narrative = re.sub(r'[\"\\'][^\"\\']+[\"\\']', '', text)\n",
    "\n",
    "        # ì •ì œ\n",
    "        narrative = re.sub(r' +', ' ', narrative)\n",
    "        narrative = re.sub(r'\\n\\s*\\n', '\\n\\n', narrative)\n",
    "\n",
    "        return narrative.strip()\n",
    "\n",
    "    def create_scene_structure(self, chapter_text, entities):\n",
    "        \"\"\"\n",
    "        ì±•í„°ë¥¼ ì”¬ êµ¬ì¡°ë¡œ ë³€í™˜\n",
    "\n",
    "        Args:\n",
    "            chapter_text (str): ì±•í„° í…ìŠ¤íŠ¸\n",
    "            entities (dict): ì¶”ì¶œëœ ê°œì²´ëª…\n",
    "\n",
    "        Returns:\n",
    "            dict: ì”¬ êµ¬ì¡° ì •ë³´\n",
    "        \"\"\"\n",
    "        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "        doc = self.nlp(chapter_text[:100000])  # ì²˜ë¦¬ ì†ë„ë¥¼ ìœ„í•´ ì œí•œ\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # ëŒ€í™”ë¬¸ê³¼ ì„œìˆ  ë¶„ë¦¬\n",
    "        dialogues = self.extract_dialogues(chapter_text)\n",
    "        narrative = self.extract_narrative(chapter_text)\n",
    "\n",
    "        scene_data = {\n",
    "            'characters': [char for char, _ in entities.get('PERSON', [])[:5]],\n",
    "            'locations': [loc for loc, _ in (entities.get('GPE', []) + entities.get('LOC', []))[:3]],\n",
    "            'dialogues': dialogues[:10],\n",
    "            'narrative_sentences': sentences[:20],\n",
    "            'total_sentences': len(sentences),\n",
    "            'total_dialogues': len(dialogues)\n",
    "        }\n",
    "\n",
    "        return scene_data\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "formatter = ScriptFormatter()\n",
    "print(\"âœ“ Script Formatter initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "create_scene",
    "outputId": "d5c30fb2-d76e-49e3-a2da-57b39b8a0fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì”¬ êµ¬ì¡° ìƒì„± ì¤‘ ===\n",
      "\n",
      "âœ“ Scene structure created\n",
      "\n",
      "ğŸ¬ Scene Information:\n",
      "  - Main Characters: Bennet, Bingley, George Allen, Lizzy, Long\n",
      "  - Locations: England, Lydia\n",
      "  - Total Sentences: 53\n",
      "  - Total Dialogues: 0\n",
      "\n",
      "ğŸ“ Sample Narrative:\n",
      "  1. It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in...\n",
      "  2. However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhoo...\n",
      "  3. â€œMy dear Mr. Bennet,â€ said his lady to him one day, â€œhave you heard that\n",
      "Netherfield Park is let at ...\n",
      "âœ“ Scene structure created\n",
      "\n",
      "ğŸ¬ Scene Information:\n",
      "  - Main Characters: Bennet, Bingley, George Allen, Lizzy, Long\n",
      "  - Locations: England, Lydia\n",
      "  - Total Sentences: 53\n",
      "  - Total Dialogues: 0\n",
      "\n",
      "ğŸ“ Sample Narrative:\n",
      "  1. It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune must be in...\n",
      "  2. However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhoo...\n",
      "  3. â€œMy dear Mr. Bennet,â€ said his lady to him one day, â€œhave you heard that\n",
      "Netherfield Park is let at ...\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ì±•í„°ë¥¼ ì”¬ êµ¬ì¡°ë¡œ ë³€í™˜\n",
    "if 'chapters' in globals() and 'entities' in globals() and chapters and entities:\n",
    "    print(\"=== ì”¬ êµ¬ì¡° ìƒì„± ì¤‘ ===\\n\")\n",
    "    scene_data = formatter.create_scene_structure(chapters[0]['content'], entities)\n",
    "\n",
    "    print(\"âœ“ Scene structure created\\n\")\n",
    "    print(f\"ğŸ¬ Scene Information:\")\n",
    "    print(f\"  - Main Characters: {', '.join(scene_data['characters'][:5])}\")\n",
    "    print(f\"  - Locations: {', '.join(scene_data['locations'][:3])}\")\n",
    "    print(f\"  - Total Sentences: {scene_data['total_sentences']}\")\n",
    "    print(f\"  - Total Dialogues: {scene_data['total_dialogues']}\")\n",
    "\n",
    "    if scene_data['dialogues']:\n",
    "        print(\"\\nğŸ’¬ Sample Dialogues:\")\n",
    "        for i, dialogue in enumerate(scene_data['dialogues'][:3], 1):\n",
    "            print(f\"  {i}. \\\"{dialogue[:80]}...\\\"\" if len(dialogue) > 80 else f\"  {i}. \\\"{dialogue}\\\"\")\n",
    "\n",
    "    if scene_data['narrative_sentences']:\n",
    "        print(\"\\nğŸ“ Sample Narrative:\")\n",
    "        for i, sentence in enumerate(scene_data['narrative_sentences'][:3], 1):\n",
    "            preview = sentence[:100] + \"...\" if len(sentence) > 100 else sentence\n",
    "            print(f\"  {i}. {preview}\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë¨¼ì € chaptersì™€ entitiesë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_creation"
   },
   "source": [
    "## 4. í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "\n",
    "### 4.1 ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pipeline",
    "outputId": "143dcc35-e6d5-43be-c1dc-5d5a7f642e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "class BookToScriptPipeline:\n",
    "    \"\"\"ë„ì„œì—ì„œ ìŠ¤í¬ë¦½íŠ¸ í•™ìŠµ ë°ì´í„°ê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.collector = GutenbergCollector()\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.extractor = EntityExtractor()\n",
    "        self.formatter = ScriptFormatter()\n",
    "\n",
    "    def process_book(self, book_id):\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ë„ì„œ ì²˜ë¦¬\n",
    "\n",
    "        Args:\n",
    "            book_id (int): ë„ì„œ ID\n",
    "\n",
    "        Returns:\n",
    "            dict: ì²˜ë¦¬ëœ ë°ì´í„°\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing book {book_id}...\")\n",
    "\n",
    "        # 1. ë„ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "        book_text = self.collector.download_book(book_id)\n",
    "        if not book_text:\n",
    "            return None\n",
    "        print(f\"  âœ“ Downloaded ({len(book_text)} chars)\")\n",
    "\n",
    "        # 2. ì „ì²˜ë¦¬\n",
    "        cleaned_text = self.preprocessor.remove_gutenberg_header_footer(book_text)\n",
    "        cleaned_text = self.preprocessor.clean_text(cleaned_text)\n",
    "        print(f\"  âœ“ Cleaned ({len(cleaned_text)} chars)\")\n",
    "\n",
    "        # 3. ì±•í„° ë¶„í• \n",
    "        chapters = self.preprocessor.split_into_chapters(cleaned_text)\n",
    "        print(f\"  âœ“ Split into {len(chapters)} chapters\")\n",
    "\n",
    "        # 4. ê° ì±•í„°ë³„ ì²˜ë¦¬ (ì „ì²´ ì±•í„°)\n",
    "        processed_chapters = []\n",
    "        for i, chapter in enumerate(chapters):  # ì „ì²´ ì±•í„° ì²˜ë¦¬\n",
    "            # ê°œì²´ëª… ì¶”ì¶œ\n",
    "            entities = self.extractor.extract_entities(chapter['content'])\n",
    "\n",
    "            # ì”¬ êµ¬ì¡° ìƒì„±\n",
    "            scene_data = self.formatter.create_scene_structure(chapter['content'], entities)\n",
    "\n",
    "            processed_chapters.append({\n",
    "                'chapter_title': chapter['title'],\n",
    "                'chapter_number': i + 1,\n",
    "                'original_text': chapter['content'],\n",
    "                'entities': entities,\n",
    "                'scene_data': scene_data\n",
    "            })\n",
    "\n",
    "        print(f\"  âœ“ Processed {len(processed_chapters)} chapters\")\n",
    "\n",
    "        return {\n",
    "            'book_id': book_id,\n",
    "            'total_length': len(cleaned_text),\n",
    "            'total_chapters': len(chapters),\n",
    "            'processed_chapters': processed_chapters\n",
    "        }\n",
    "\n",
    "    def process_multiple_books(self, book_ids, output_dir='./processed_data'):\n",
    "        \"\"\"\n",
    "        ì—¬ëŸ¬ ë„ì„œ ì²˜ë¦¬ ë° ì €ì¥\n",
    "\n",
    "        Args:\n",
    "            book_ids (list): ë„ì„œ ID ë¦¬ìŠ¤íŠ¸\n",
    "            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "\n",
    "        Returns:\n",
    "            list: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        results = []\n",
    "\n",
    "        for book_id in book_ids:\n",
    "            result = self.process_book(book_id)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "\n",
    "                # JSONìœ¼ë¡œ ì €ì¥\n",
    "                output_file = os.path.join(output_dir, f'book_{book_id}.json')\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"  âœ“ Saved to {output_file}\")\n",
    "\n",
    "        print(f\"\\nâœ“ Completed processing {len(results)} books\")\n",
    "        return results\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
    "pipeline = BookToScriptPipeline()\n",
    "print(\"âœ“ Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  ë¨¼ì € resultsë¥¼ ìƒì„±í•´ì£¼ì„¸ìš” (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•„ìš”)\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ìƒ˜í”Œ ìƒì„±\n",
    "if 'results' in globals() and results:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ìƒ˜í”Œ ìƒì„±\n",
    "    training_samples = dataset_builder.create_training_samples(results)\n",
    "    print(f\"\\nâœ“ ìƒì„±ëœ í•™ìŠµ ìƒ˜í”Œ: {len(training_samples)}ê°œ\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ë¶„í• \n",
    "    train_data, val_data, test_data = dataset_builder.split_dataset(training_samples)\n",
    "    print(f\"\\nâœ“ ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:\")\n",
    "    print(f\"  - í•™ìŠµ(Train): {len(train_data)} ìƒ˜í”Œ ({len(train_data)/len(training_samples)*100:.1f}%)\")\n",
    "    print(f\"  - ê²€ì¦(Val): {len(val_data)} ìƒ˜í”Œ ({len(val_data)/len(training_samples)*100:.1f}%)\")\n",
    "    print(f\"  - í…ŒìŠ¤íŠ¸(Test): {len(test_data)} ìƒ˜í”Œ ({len(test_data)/len(training_samples)*100:.1f}%)\")\n",
    "    \n",
    "    # ì €ì¥\n",
    "    stats = dataset_builder.save_datasets(train_data, val_data, test_data, output_dir='./')\n",
    "    \n",
    "    # ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“ ì²« ë²ˆì§¸ í•™ìŠµ ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(\"=\" * 70)\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\n[ì…ë ¥ (Input)]\")\n",
    "    print(sample['input'][:300] + \"...\")\n",
    "    print(f\"\\n[ì¶œë ¥ (Output)]\")\n",
    "    print(sample['output'][:300] + \"...\")\n",
    "    print(f\"\\n[ë©”íƒ€ë°ì´í„°]\")\n",
    "    print(f\"  - Book ID: {sample['metadata']['book_id']}\")\n",
    "    print(f\"  - Chapter: {sample['metadata']['chapter_title']}\")\n",
    "    print(f\"  - Input Length: {sample['metadata']['input_length']} chars\")\n",
    "    print(f\"  - Output Length: {sample['metadata']['output_length']} chars\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  ë¨¼ì € resultsë¥¼ ìƒì„±í•´ì£¼ì„¸ìš” (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•„ìš”)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  ë¨¼ì € training_samplesë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ í†µê³„\n",
    "if 'training_samples' in globals() and training_samples:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“ˆ ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ê¸¸ì´ í†µê³„\n",
    "    input_lengths = [s['metadata']['input_length'] for s in training_samples]\n",
    "    output_lengths = [s['metadata']['output_length'] for s in training_samples]\n",
    "    \n",
    "    print(f\"\\nâœ“ ì…ë ¥(Input) í…ìŠ¤íŠ¸ ê¸¸ì´:\")\n",
    "    print(f\"  - í‰ê· : {sum(input_lengths)/len(input_lengths):.0f} ë¬¸ì\")\n",
    "    print(f\"  - ìµœì†Œ: {min(input_lengths)} ë¬¸ì\")\n",
    "    print(f\"  - ìµœëŒ€: {max(input_lengths)} ë¬¸ì\")\n",
    "    \n",
    "    print(f\"\\nâœ“ ì¶œë ¥(Output) í…ìŠ¤íŠ¸ ê¸¸ì´:\")\n",
    "    print(f\"  - í‰ê· : {sum(output_lengths)/len(output_lengths):.0f} ë¬¸ì\")\n",
    "    print(f\"  - ìµœì†Œ: {min(output_lengths)} ë¬¸ì\")\n",
    "    print(f\"  - ìµœëŒ€: {max(output_lengths)} ë¬¸ì\")\n",
    "    \n",
    "    # ë„ì„œë³„ ìƒ˜í”Œ ìˆ˜\n",
    "    book_sample_counts = {}\n",
    "    for sample in training_samples:\n",
    "        book_id = sample['metadata']['book_id']\n",
    "        book_sample_counts[book_id] = book_sample_counts.get(book_id, 0) + 1\n",
    "    \n",
    "    print(f\"\\nâœ“ ë„ì„œë³„ ìƒ˜í”Œ ìˆ˜:\")\n",
    "    for book_id, count in sorted(book_sample_counts.items()):\n",
    "        print(f\"  - Book {book_id}: {count} ìƒ˜í”Œ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼:\")\n",
    "    print(f\"  - train_data.json\")\n",
    "    print(f\"  - val_data.json\")\n",
    "    print(f\"  - test_data.json\")\n",
    "    print(f\"\\nğŸš€ ì´ì œ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë¨¼ì € training_samplesë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ë°ì´í„°ì…‹ í†µê³„ ë° ê²€ì¦\n",
    "\n",
    "ìƒì„±ëœ ë°ì´í„°ì…‹ì˜ í’ˆì§ˆì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset Builder initialized\n"
     ]
    }
   ],
   "source": [
    "class DatasetBuilder:\n",
    "    \"\"\"í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶• í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_training_samples(self, processed_results):\n",
    "        \"\"\"\n",
    "        ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ í•™ìŠµ ìƒ˜í”Œë¡œ ë³€í™˜\n",
    "        \n",
    "        Args:\n",
    "            processed_results (list): process_multiple_booksì˜ ê²°ê³¼\n",
    "            \n",
    "        Returns:\n",
    "            list: í•™ìŠµ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        training_samples = []\n",
    "        \n",
    "        for result in processed_results:\n",
    "            book_id = result['book_id']\n",
    "            \n",
    "            for chapter in result['processed_chapters']:\n",
    "                # ì…ë ¥: ì›ë³¸ ì±•í„° í…ìŠ¤íŠ¸ + í”„ë¡¬í”„íŠ¸\n",
    "                input_text = f\"\"\"Convert the following book chapter into a video script format. \n",
    "Extract key elements including characters, locations, dialogues, and narrative descriptions.\n",
    "\n",
    "Chapter: {chapter['chapter_title']}\n",
    "\n",
    "Text:\n",
    "{chapter['original_text'][:2000]}\"\"\"  # ì²˜ìŒ 2000ìë§Œ ì‚¬ìš© (í† í° ì œí•œ ê³ ë ¤)\n",
    "                \n",
    "                # ì¶œë ¥: êµ¬ì¡°í™”ëœ ìŠ¤í¬ë¦½íŠ¸ ì •ë³´\n",
    "                scene_data = chapter['scene_data']\n",
    "                entities = chapter['entities']\n",
    "                \n",
    "                output_script = {\n",
    "                    'scene_title': chapter['chapter_title'],\n",
    "                    'characters': scene_data['characters'][:10],\n",
    "                    'locations': scene_data['locations'][:5],\n",
    "                    'dialogues': scene_data['dialogues'][:15],\n",
    "                    'narrative': ' '.join(scene_data['narrative_sentences'][:10]),\n",
    "                    'total_sentences': scene_data['total_sentences'],\n",
    "                    'total_dialogues': scene_data['total_dialogues']\n",
    "                }\n",
    "                \n",
    "                # JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (ëª¨ë¸ í•™ìŠµìš©)\n",
    "                output_text = json.dumps(output_script, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                training_samples.append({\n",
    "                    'input': input_text,\n",
    "                    'output': output_text,\n",
    "                    'metadata': {\n",
    "                        'book_id': book_id,\n",
    "                        'chapter_number': chapter['chapter_number'],\n",
    "                        'chapter_title': chapter['chapter_title'],\n",
    "                        'input_length': len(input_text),\n",
    "                        'output_length': len(output_text)\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        return training_samples\n",
    "    \n",
    "    def split_dataset(self, samples, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "        \"\"\"\n",
    "        ë°ì´í„°ì…‹ì„ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• \n",
    "        \n",
    "        Args:\n",
    "            samples (list): ì „ì²´ ìƒ˜í”Œ\n",
    "            train_ratio (float): í•™ìŠµ ë°ì´í„° ë¹„ìœ¨\n",
    "            val_ratio (float): ê²€ì¦ ë°ì´í„° ë¹„ìœ¨\n",
    "            test_ratio (float): í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨\n",
    "            seed (int): ëœë¤ ì‹œë“œ\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (train_data, val_data, test_data)\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ë¶„í• : train vs (val + test)\n",
    "        train_data, temp_data = train_test_split(\n",
    "            samples, \n",
    "            test_size=(val_ratio + test_ratio),\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        # ë‘ ë²ˆì§¸ ë¶„í• : val vs test\n",
    "        val_data, test_data = train_test_split(\n",
    "            temp_data,\n",
    "            test_size=test_ratio / (val_ratio + test_ratio),\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    def save_datasets(self, train_data, val_data, test_data, output_dir='./'):\n",
    "        \"\"\"\n",
    "        ë°ì´í„°ì…‹ì„ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "        \n",
    "        Args:\n",
    "            train_data (list): í•™ìŠµ ë°ì´í„°\n",
    "            val_data (list): ê²€ì¦ ë°ì´í„°\n",
    "            test_data (list): í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # ì €ì¥\n",
    "        with open(os.path.join(output_dir, 'train_data.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'val_data.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'test_data.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Datasets saved to {output_dir}\")\n",
    "        print(f\"  - train_data.json: {len(train_data)} samples\")\n",
    "        print(f\"  - val_data.json: {len(val_data)} samples\")\n",
    "        print(f\"  - test_data.json: {len(test_data)} samples\")\n",
    "        \n",
    "        return {\n",
    "            'train': len(train_data),\n",
    "            'val': len(val_data),\n",
    "            'test': len(test_data),\n",
    "            'total': len(train_data) + len(val_data) + len(test_data)\n",
    "        }\n",
    "\n",
    "# DatasetBuilder ì´ˆê¸°í™”\n",
    "dataset_builder = DatasetBuilder()\n",
    "print(\"âœ“ Dataset Builder initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 í•™ìŠµ ë°ì´í„°ì…‹ í¬ë§· ì •ì˜\n",
    "\n",
    "LLM í•™ìŠµì— ì í•©í•œ ì…ë ¥-ì¶œë ¥ ìŒ í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘\n",
      "======================================================================\n",
      "\n",
      "Processing book 1342...\n",
      "  âœ“ Loading from cache: ./gutenberg_cache/book_1342.txt\n",
      "  âœ“ Downloaded (728846 chars)\n",
      "  âœ“ Cleaned (720973 chars)\n",
      "  âœ“ Split into 61 chapters\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë„ì„œ ì²˜ë¦¬\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "results = pipeline.process_multiple_books(\n",
    "    book_ids_to_process,\n",
    "    output_dir='./processed_data'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê¶Œì˜ ë„ì„œ\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ì „ì²´ ë„ì„œ ì²˜ë¦¬ ë° ë°ì´í„°ì…‹ ìƒì„±\n",
    "\n",
    "10ê°œì˜ ë„ì„œë¥¼ ëª¨ë‘ ì²˜ë¦¬í•˜ê³  í•™ìŠµìš© ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 5. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì¤€ë¹„\n",
    "\n",
    "### 5.1 BLEU Score êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bleu_metric",
    "outputId": "5bb1816c-3c3a-4c15-bac1-39e5cba25be4"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.smooth = SmoothingFunction()\n",
    "\n",
    "    def calculate_bleu(self, reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "        \"\"\"\n",
    "        BLEU ì ìˆ˜ ê³„ì‚°\n",
    "\n",
    "        Args:\n",
    "            reference (str): ì°¸ì¡° í…ìŠ¤íŠ¸\n",
    "            candidate (str): ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "            weights (tuple): n-gram ê°€ì¤‘ì¹˜\n",
    "\n",
    "        Returns:\n",
    "            float: BLEU ì ìˆ˜\n",
    "        \"\"\"\n",
    "        # í† í°í™”\n",
    "        reference_tokens = reference.split()\n",
    "        candidate_tokens = candidate.split()\n",
    "\n",
    "        # BLEU ê³„ì‚° (smoothing ì ìš©)\n",
    "        bleu_score = sentence_bleu(\n",
    "            [reference_tokens],\n",
    "            candidate_tokens,\n",
    "            weights=weights,\n",
    "            smoothing_function=self.smooth.method1\n",
    "        )\n",
    "\n",
    "        return bleu_score\n",
    "\n",
    "    def calculate_bleu_variants(self, reference, candidate):\n",
    "        \"\"\"\n",
    "        ë‹¤ì–‘í•œ BLEU ë³€í˜• ê³„ì‚° (BLEU-1 ~ BLEU-4)\n",
    "\n",
    "        Args:\n",
    "            reference (str): ì°¸ì¡° í…ìŠ¤íŠ¸\n",
    "            candidate (str): ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "\n",
    "        Returns:\n",
    "            dict: BLEU ë³€í˜• ì ìˆ˜\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'BLEU-1': self.calculate_bleu(reference, candidate, weights=(1, 0, 0, 0)),\n",
    "            'BLEU-2': self.calculate_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)),\n",
    "            'BLEU-3': self.calculate_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)),\n",
    "            'BLEU-4': self.calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        }\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”\n",
    "metrics = EvaluationMetrics()\n",
    "print(\"âœ“ Evaluation Metrics initialized\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ\n",
    "reference_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "candidate_text = \"The quick brown fox jumps over a lazy dog\"\n",
    "\n",
    "bleu_scores = metrics.calculate_bleu_variants(reference_text, candidate_text)\n",
    "print(\"\\nBLEU Score Examples:\")\n",
    "for metric, score in bleu_scores.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 6. ë‹¤ìŒ ë‹¨ê³„ ë° ê³„íš\n",
    "\n",
    "### ì™„ë£Œëœ ì‘ì—…\n",
    "- âœ… Project Gutenberg ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ\n",
    "- âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ì œê±°, ì •ì œ)\n",
    "- âœ… ì±•í„° ë¶„í•  ê¸°ëŠ¥\n",
    "- âœ… ê°œì²´ëª… ì¶”ì¶œ (ì¸ë¬¼, ì¥ì†Œ, ì‹œê°„)\n",
    "- âœ… ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°í™”\n",
    "- âœ… BLEU í‰ê°€ ì§€í‘œ êµ¬í˜„\n",
    "\n",
    "### í–¥í›„ ì‘ì—… (10ì›” 27ì¼ê¹Œì§€)\n",
    "1. **ë°ì´í„° ìˆ˜ì§‘ í™•ëŒ€**\n",
    "   - ë” ë§ì€ ë„ì„œ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬\n",
    "   - ë‹¤ì–‘í•œ ì¥ë¥´ í™•ë³´ (ì†Œì„¤, ë“œë¼ë§ˆ, ë¯¸ìŠ¤í„°ë¦¬ ë“±)\n",
    "   - êµ­ë‚´ ë””ì§€í„¸ ë„ì„œê´€ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ì—°êµ¬\n",
    "\n",
    "2. **ì „ì²˜ë¦¬ ê³ ë„í™”**\n",
    "   - ëŒ€í™”ë¬¸ê³¼ ì„œìˆ  ë¶„ë¦¬ ì •í™•ë„ í–¥ìƒ\n",
    "   - ì¥ë©´ ì „í™˜ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n",
    "   - ê°ì •/í†¤ ë¶„ì„ ì¶”ê°€\n",
    "\n",
    "3. **ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦**\n",
    "   - ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ê²€ì‚¬\n",
    "   - ë°ì´í„° í†µê³„ ë¶„ì„\n",
    "   - ìƒ˜í”Œ ë°ì´í„° ê²€í† \n",
    "\n",
    "### ëª¨ë¸ë§ ì¤€ë¹„ (10ì›” 29ì¼ ì´í›„)\n",
    "- LLM ëª¨ë¸ ì„ íƒ (GPT-2, T5, BART ë“±)\n",
    "- Fine-tuning ì „ëµ ìˆ˜ë¦½\n",
    "- í•™ìŠµ ë°ì´í„° í¬ë§· ì •ì˜\n",
    "\n",
    "### ì„±ëŠ¥ í‰ê°€ ê³„íš\n",
    "- BLEU Score: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì¸¡ì •\n",
    "- FVD (ì„ íƒ): ë¹„ë””ì˜¤ í’ˆì§ˆ í‰ê°€ (ì—¬ìœ ê°€ ìˆì„ ê²½ìš°)\n",
    "- CLIPScore (ì„ íƒ): í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ (ì—¬ìœ ê°€ ìˆì„ ê²½ìš°)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_checkpoint"
   },
   "source": [
    "## 7. ì§„í–‰ ìƒí™© ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "save_progress",
    "outputId": "f4d36f24-986d-4d66-872b-26f4177cbbaa"
   },
   "outputs": [],
   "source": [
    "# ì§„í–‰ ìƒí™© ìš”ì•½ ì €ì¥\n",
    "progress_summary = {\n",
    "    'date': '2025-10-12',\n",
    "    'phase': 'Data Preprocessing',\n",
    "    'completed_tasks': [\n",
    "        'Data collection module (Project Gutenberg)',\n",
    "        'Text cleaning and preprocessing',\n",
    "        'Chapter segmentation',\n",
    "        'Entity extraction (characters, locations, time)',\n",
    "        'Script formatting structure',\n",
    "        'BLEU evaluation metric'\n",
    "    ],\n",
    "    'next_tasks': [\n",
    "        'Expand dataset collection',\n",
    "        'Enhance preprocessing accuracy',\n",
    "        'Data quality validation',\n",
    "        'Prepare for modeling phase'\n",
    "    ],\n",
    "    'deadline': '2025-10-27',\n",
    "    'team_notes': 'Following professor\\'s guidance - parallel work on dataset collection and framework setup'\n",
    "}\n",
    "\n",
    "# JSONìœ¼ë¡œ ì €ì¥\n",
    "with open('progress_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(progress_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ“ Progress summary saved to 'progress_summary.json'\")\n",
    "print(\"\\nCurrent Status:\")\n",
    "print(f\"  Phase: {progress_summary['phase']}\")\n",
    "print(f\"  Deadline: {progress_summary['deadline']}\")\n",
    "print(f\"  Completed: {len(progress_summary['completed_tasks'])} tasks\")\n",
    "print(f\"  Remaining: {len(progress_summary['next_tasks'])} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_splitting_examples"
   },
   "source": [
    "### 3.1.1 ì±•í„° ë¶„í•  ì˜ˆì‹œ ë° ë””ë²„ê¹…\n",
    "\n",
    "ê°œì„ ëœ ì±•í„° ë¶„í•  ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test_chapter_splitting",
    "outputId": "f38aa14a-4697-4464-8e5e-efc60e8ed3d8"
   },
   "outputs": [],
   "source": [
    "# ì±•í„° ë¶„í•  í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…\n",
    "if 'cleaned_text' in globals() and cleaned_text:\n",
    "    print(\"=== ì±•í„° ë¶„í•  ë””ë²„ê¹… ===\")\n",
    "    print(f\"\\n1. ì›ë³¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ (ì²˜ìŒ 1000ì):\")\n",
    "    print(cleaned_text[:1000])\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "    # ì±•í„° íŒ¨í„´ ìˆ˜ë™ ê²€ìƒ‰\n",
    "    print(\"2. ì±•í„° íŒ¨í„´ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "\n",
    "    patterns_to_test = [\n",
    "        (\"CHAPTER/Chapter + ë²ˆí˜¸\", r'\\n\\s*(CHAPTER|Chapter)\\s+([IVXLCDM]+|\\d+)'),\n",
    "        (\"ë¡œë§ˆì/ìˆ«ìë§Œ\", r'\\n\\s*([IVXLCDM]+|\\d+)\\.?\\s*\\n'),\n",
    "        (\"BOOK/PART + ë²ˆí˜¸\", r'\\n\\s*(BOOK|Book|PART|Part)\\s+([IVXLCDM]+|\\d+)'),\n",
    "    ]\n",
    "\n",
    "    for pattern_name, pattern in patterns_to_test:\n",
    "        matches = re.findall(pattern, cleaned_text)\n",
    "        print(f\"  - {pattern_name}: {len(matches)}ê°œ ë°œê²¬\")\n",
    "        if matches and len(matches) <= 10:\n",
    "            print(f\"    ì˜ˆì‹œ: {matches[:5]}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "    # ì‹¤ì œ ì±•í„° ë¶„í•  ìˆ˜í–‰\n",
    "    print(\"3. ì±•í„° ë¶„í•  ê²°ê³¼:\")\n",
    "    chapters_debug = preprocessor.split_into_chapters(cleaned_text)\n",
    "    print(f\"  ì´ {len(chapters_debug)}ê°œ ì±•í„° ë°œê²¬\\n\")\n",
    "\n",
    "    # ì²˜ìŒ 3ê°œ ì±•í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    for i, chapter in enumerate(chapters_debug[:3], 1):\n",
    "        print(f\"  Chapter {i}:\")\n",
    "        print(f\"    ì œëª©: '{chapter['title']}'\")\n",
    "        print(f\"    ê¸¸ì´: {len(chapter['content']):,} ë¬¸ì\")\n",
    "        print(f\"    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chapter['content'][:150]}...\")\n",
    "        print()\n",
    "\n",
    "    # ì±•í„° ê¸¸ì´ í†µê³„\n",
    "    if len(chapters_debug) > 1:\n",
    "        chapter_lengths = [len(ch['content']) for ch in chapters_debug]\n",
    "        print(f\"\\n4. ì±•í„° ê¸¸ì´ í†µê³„:\")\n",
    "        print(f\"  - í‰ê· : {sum(chapter_lengths)/len(chapter_lengths):,.0f} ë¬¸ì\")\n",
    "        print(f\"  - ìµœì†Œ: {min(chapter_lengths):,} ë¬¸ì\")\n",
    "        print(f\"  - ìµœëŒ€: {max(chapter_lengths):,} ë¬¸ì\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ“ ì±•í„° ë¶„í•  ë¶„ì„ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë¨¼ì € ìƒ˜í”Œ ë„ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì •ì œí•˜ì„¸ìš” (ìœ„ì˜ ì „ì²˜ë¦¬ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
