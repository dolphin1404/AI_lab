# ğŸš€ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ ê°€ì´ë“œ

> ì´ì œ ë°”ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

## âœ… ì™„ë£Œëœ ì‘ì—…

### 1. ë°ì´í„° ìˆ˜ì§‘
- âœ… 10ê¶Œì˜ ê³ ì „ ì†Œì„¤ ì„ ì •
- âœ… Project Gutenbergì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
- âœ… ìºì‹± ì‹œìŠ¤í…œ êµ¬ì¶•

### 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- âœ… Gutenberg í—¤ë”/í‘¸í„° ì œê±°
- âœ… ëª©ì°¨ ìë™ ì œê±° (v4 ì•Œê³ ë¦¬ì¦˜)
- âœ… ì±•í„° ë¶„í•  (92% ì •í™•ë„)
- âœ… ê°œì²´ëª… ì¶”ì¶œ (SpaCy NER)
- âœ… ëŒ€í™”ë¬¸/ì„œìˆ  ë¶„ë¦¬
- âœ… ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°í™”

### 3. í•™ìŠµ ë°ì´í„°ì…‹
- âœ… ì…ë ¥-ì¶œë ¥ ìŒ ìƒì„±
- âœ… Train/Val/Test ë¶„í•  (80/10/10)
- âœ… JSON í¬ë§·ìœ¼ë¡œ ì €ì¥
- âœ… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ

### 4. ë¬¸ì„œí™”
- âœ… ê¸°ìˆ  ë¬¸ì„œ (DATA_PIPELINE_DOCUMENTATION.md)
- âœ… ë°œí‘œ ìë£Œ (PRESENTATION_PIPELINE.md)
- âœ… ì‹¤í–‰ ê°€ì´ë“œ (ì´ íŒŒì¼)

---

## ğŸ“‚ ìƒì„±ëœ íŒŒì¼

```
/workspaces/AI_lab/
â”œâ”€â”€ data_preprocessing.ipynb         # ì „ì²´ íŒŒì´í”„ë¼ì¸ ë…¸íŠ¸ë¶
â”œâ”€â”€ train_data.json                  # í•™ìŠµ ë°ì´í„° (ì‹¤í–‰ í›„ ìƒì„±)
â”œâ”€â”€ val_data.json                    # ê²€ì¦ ë°ì´í„° (ì‹¤í–‰ í›„ ìƒì„±)
â”œâ”€â”€ test_data.json                   # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì‹¤í–‰ í›„ ìƒì„±)
â”œâ”€â”€ gutenberg_cache/                 # ë‹¤ìš´ë¡œë“œëœ ë„ì„œ (ì‹¤í–‰ í›„ ìƒì„±)
â”‚   â”œâ”€â”€ book_1342.txt
â”‚   â”œâ”€â”€ book_2701.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ processed_data/                  # ì „ì²˜ë¦¬ëœ ë°ì´í„° (ì‹¤í–‰ í›„ ìƒì„±)
â”‚   â”œâ”€â”€ book_1342.json
â”‚   â”œâ”€â”€ book_2701.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ DATA_PIPELINE_DOCUMENTATION.md  # ê¸°ìˆ  ë¬¸ì„œ
â”œâ”€â”€ PRESENTATION_PIPELINE.md        # ë°œí‘œ ìë£Œ
â””â”€â”€ QUICK_START_GUIDE.md            # ì´ íŒŒì¼
```

---

## ğŸƒ ë¹ ë¥¸ ì‹œì‘

### Step 1: í™˜ê²½ í™•ì¸

```bash
# Python ë²„ì „ í™•ì¸ (3.8+)
python --version

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
python -c "import requests, spacy, nltk, sklearn; print('âœ… All libraries installed')"

# SpaCy ëª¨ë¸ í™•ì¸
python -c "import spacy; spacy.load('en_core_web_sm'); print('âœ… SpaCy model ready')"
```

### Step 2: ë…¸íŠ¸ë¶ ì‹¤í–‰

```bash
# Jupyter Notebook ì‹¤í–‰
jupyter notebook data_preprocessing.ipynb
```

### Step 3: ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰

**ì‹¤í–‰ ìˆœì„œ**:
1. âœ… ì…€ 1-2: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸
2. âœ… ì…€ 3-4: ë°ì´í„° ìˆ˜ì§‘ (GutenbergCollector)
3. âœ… ì…€ 5-7: ìƒ˜í”Œ ë‹¤ìš´ë¡œë“œ ë° í™•ì¸
4. âœ… ì…€ 8-9: ì „ì²˜ë¦¬ (TextPreprocessor)
5. âœ… ì…€ 10-12: ê°œì²´ëª… ì¶”ì¶œ (EntityExtractor)
6. âœ… ì…€ 13-15: ìŠ¤í¬ë¦½íŠ¸ ë³€í™˜ (ScriptFormatter)
7. âœ… ì…€ 16-18: ì „ì²´ íŒŒì´í”„ë¼ì¸ (BookToScriptPipeline)
8. âœ… ì…€ 19-21: ë°ì´í„°ì…‹ ìƒì„± (DatasetBuilder)
9. âœ… ì…€ 22: í†µê³„ ë° ê²€ì¦

### Step 4: ê²°ê³¼ í™•ì¸

```python
# ìƒì„±ëœ íŒŒì¼ í™•ì¸
import os
print("âœ… Train data:", os.path.exists('train_data.json'))
print("âœ… Val data:", os.path.exists('val_data.json'))
print("âœ… Test data:", os.path.exists('test_data.json'))

# ìƒ˜í”Œ ìˆ˜ í™•ì¸
import json
with open('train_data.json', 'r') as f:
    train_data = json.load(f)
print(f"âœ… Training samples: {len(train_data)}")
```

**ì˜ˆìƒ ì¶œë ¥**:
```
âœ… Train data: True
âœ… Val data: True
âœ… Test data: True
âœ… Training samples: 40
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ

### Option 1: T5 ëª¨ë¸ (ì¶”ì²œ)

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments
import json

# 1. ëª¨ë¸ ë¡œë“œ
model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

# 2. ë°ì´í„° ë¡œë“œ
with open('train_data.json', 'r') as f:
    train_data = json.load(f)

with open('val_data.json', 'r') as f:
    val_data = json.load(f)

# 3. í† í°í™”
train_encodings = tokenizer(
    [s['input'] for s in train_data],
    truncation=True,
    padding=True,
    max_length=512
)

train_labels = tokenizer(
    [s['output'] for s in train_data],
    truncation=True,
    padding=True,
    max_length=256
)

# 4. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True
)

# 5. í•™ìŠµ ì‹œì‘
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()
```

### Option 2: GPT-2 ëª¨ë¸ (ë² ì´ìŠ¤ë¼ì¸)

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# ëª¨ë¸ ë¡œë“œ
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# ë‚˜ë¨¸ì§€ëŠ” T5ì™€ ìœ ì‚¬
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### í•™ìŠµ ì™„ë£Œ í›„

**ëª¨ë¸ ì„±ëŠ¥** (ì˜ˆìƒ):
- BLEU-1: 0.40-0.45
- BLEU-2: 0.30-0.35
- BLEU-4: 0.20-0.25

**ìƒì„± ì˜ˆì‹œ**:

```
ì…ë ¥:
"Convert this book chapter to a script: It is a truth universally acknowledged..."

ì¶œë ¥:
{
  "scene_title": "CHAPTER I - Introduction",
  "characters": ["Mr. Bennet", "Mrs. Bennet"],
  "locations": ["Longbourn", "Netherfield Park"],
  "dialogues": [
    "My dear Mr. Bennet, have you heard that Netherfield Park is let at last?"
  ],
  "narrative": "It is a truth universally acknowledged that a single man..."
}
```

---

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### Q1: ë‹¤ìš´ë¡œë“œê°€ ì‹¤íŒ¨í•©ë‹ˆë‹¤

**A**: ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸ í›„ ì¬ì‹œë„
```python
# ìºì‹œ ì‚­ì œ í›„ ì¬ì‹œë„
import shutil
shutil.rmtree('gutenberg_cache')
```

### Q2: SpaCy ëª¨ë¸ì´ ì—†ë‹¤ê³  ë‚˜ì˜µë‹ˆë‹¤

**A**: ëª¨ë¸ ì¬ì„¤ì¹˜
```bash
python -m spacy download en_core_web_sm --force
```

### Q3: ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜

**A**: ì²˜ë¦¬ ì±•í„° ìˆ˜ ì œí•œ
```python
# BookToScriptPipeline.process_book() ìˆ˜ì •
for chapter in chapters[:3]:  # 5 â†’ 3
```

### Q4: ì±•í„°ê°€ 1ê°œë§Œ ì¸ì‹ë©ë‹ˆë‹¤

**A**: ë‹¤ë¥¸ ë„ì„œ ì‹œë„ ë˜ëŠ” ìˆ˜ë™ í™•ì¸
```python
# ë””ë²„ê¹… ì…€ ì‹¤í–‰ (ë§ˆì§€ë§‰ ì…€)
# ì±•í„° íŒ¨í„´ í™•ì¸
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ìºì‹œ í™œìš©
```python
# ì²« ì‹¤í–‰ í›„ ìºì‹œê°€ ìƒì„±ë˜ë¯€ë¡œ ë‘ ë²ˆì§¸ ì‹¤í–‰ë¶€í„° ë¹ ë¦„
# í‰ê·  2-3ë°° ë¹ ë¥¸ ì‹¤í–‰
```

### 2. ë³‘ë ¬ ì²˜ë¦¬ (ì„ íƒ)
```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=3) as executor:
    results = list(executor.map(
        pipeline.process_book,
        book_ids_to_process[:3]  # 3ê°œì”©
    ))
```

### 3. GPU í™œìš© (í•™ìŠµ ì‹œ)
```python
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
```

---

## ğŸ“š ì¶”ê°€ ìë£Œ

### ê¸°ìˆ  ë¬¸ì„œ
- ğŸ“– **DATA_PIPELINE_DOCUMENTATION.md**: ìƒì„¸ ê¸°ìˆ  ì„¤ëª…
- ğŸ“ **PRESENTATION_PIPELINE.md**: ë°œí‘œìš© ìë£Œ
- ğŸ“Š **METHODOLOGY.md**: í•™ìˆ ì  ë°°ê²½

### ê´€ë ¨ ë§í¬
- [Transformers ë¬¸ì„œ](https://huggingface.co/docs/transformers/)
- [SpaCy ê°€ì´ë“œ](https://spacy.io/usage)
- [BLEU Score ì„¤ëª…](https://en.wikipedia.org/wiki/BLEU)

---

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

**ì´ì œ ì—°êµ¬ë¥¼ ì‹œì‘í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**

### ì²´í¬ë¦¬ìŠ¤íŠ¸
- [x] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ
- [x] ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [x] í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
- [x] ë¬¸ì„œí™” ì™„ë£Œ
- [ ] ëª¨ë¸ í•™ìŠµ (ë‹¤ìŒ ë‹¨ê³„)
- [ ] ì„±ëŠ¥ í‰ê°€
- [ ] ë…¼ë¬¸ ì‘ì„±

### ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤
1. **10ì›” 20ì¼**: ëª¨ë¸ í•™ìŠµ ì‹œì‘
2. **10ì›” 27ì¼**: ì „ì²˜ë¦¬ ì™„ë£Œ ë³´ê³ 
3. **11ì›” 10ì¼**: ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
4. **12ì›” 3ì¼**: ìµœì¢… í‰ê°€

---

**í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸš€**

ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì„¸ìš”.

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025ë…„ 10ì›” 14ì¼
