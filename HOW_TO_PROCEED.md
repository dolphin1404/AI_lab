# ë‹¤ìŒ ì£¼ê¹Œì§€ ë°ì´í„° ì „ì²˜ë¦¬ ì§„í–‰ ë°©ì•ˆ

## ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
> "êµìˆ˜ë‹˜ì˜ ë©´ë‹´ì„ í† ëŒ€ë¡œ, ë‹¤ìŒ ì£¼ê¹Œì§€ ë°ì´í„° ì „ì²˜ë¦¬ì— ëŒ€í•´ ì§„í–‰í•´ì•¼ í•´ ì–´ë–»ê²Œ ì§„í–‰í•´ì•¼í• ê¹Œ?"

## ğŸ“‹ ìš”ì•½ ë‹µë³€

êµìˆ˜ë‹˜ì˜ Comment-2ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ë³‘ë ¬ ì‘ì—… ë°©ì‹**ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤:

1. **ë°ì´í„° ìˆ˜ì§‘ ë‹´ë‹¹ì (1ëª…)**: ë„ì„œ ë‹¤ìš´ë¡œë“œì— ì§‘ì¤‘
2. **í”„ë ˆì„ì›Œí¬ ë‹´ë‹¹ì (ë‚˜ë¨¸ì§€)**: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìµœì í™” ë° ê²€ì¦

ì´ë¯¸ ëª¨ë“  í•„ìš”í•œ ì½”ë“œì™€ ê°€ì´ë“œê°€ `data_preprocessing.ipynb`ì— ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íš (7ì¼)

### Day 1-2: ì¤€ë¹„ ë° ì‹œì‘ (10ì›” 12-13ì¼)

#### ì „ì²´ íŒ€ì›
```bash
# 1. í™˜ê²½ ì„¤ì •
pip install torch transformers datasets nltk spacy beautifulsoup4 requests scikit-learn
python -m spacy download en_core_web_sm

# 2. ë…¸íŠ¸ë¶ ì‹¤í–‰ í™•ì¸
jupyter notebook data_preprocessing.ipynb
```

#### ë°ì´í„° ìˆ˜ì§‘ ë‹´ë‹¹
```python
# data_preprocessing.ipynbì—ì„œ ì‹¤í–‰
collector = GutenbergCollector()

# ëª©í‘œ ë„ì„œ ID ë¦¬ìŠ¤íŠ¸ (50ê¶Œ)
book_ids = collector.get_popular_books(limit=50)

# ë§¤ì¼ 10-15ê¶Œì”© ë‹¤ìš´ë¡œë“œ
for book_id in book_ids[:15]:
    print(f"Downloading book {book_id}...")
    text = collector.download_book(book_id)
    if text:
        # ì €ì¥
        with open(f'raw_data/book_{book_id}.txt', 'w') as f:
            f.write(text)
    time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
```

#### í”„ë ˆì„ì›Œí¬ ë‹´ë‹¹
```python
# ìƒ˜í”Œ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ ê²€ì¦
pipeline = BookToScriptPipeline()

# í…ŒìŠ¤íŠ¸ (Pride and Prejudice)
result = pipeline.process_book(1342)

# ê²°ê³¼ í™•ì¸
print(f"Chapters: {result['total_chapters']}")
print(f"First chapter entities: {result['processed_chapters'][0]['entities']}")
```

### Day 3-4: ëŒ€ê·œëª¨ ì²˜ë¦¬ (10ì›” 14-15ì¼)

#### ë°ì´í„° ìˆ˜ì§‘ ë‹´ë‹¹
```python
# ê³„ì† ìˆ˜ì§‘ (ëˆ„ì  30-40ê¶Œ ëª©í‘œ)
# ë‹¤ì–‘í•œ ì¥ë¥´ í™•ë³´
fiction_ids = [1342, 84, 98, 1661, 2701, ...]
drama_ids = [...]
mystery_ids = [...]

all_ids = fiction_ids + drama_ids + mystery_ids
```

#### í”„ë ˆì„ì›Œí¬ ë‹´ë‹¹
```python
# ìˆ˜ì§‘ëœ ë„ì„œ ì „ì²˜ë¦¬ ì‹œì‘
import os
import glob

# raw_data í´ë”ì˜ ëª¨ë“  txt íŒŒì¼ ì²˜ë¦¬
raw_files = glob.glob('raw_data/*.txt')
book_ids = [int(f.split('_')[1].split('.')[0]) for f in raw_files]

# ë°°ì¹˜ ì²˜ë¦¬
results = pipeline.process_multiple_books(
    book_ids,
    output_dir='./processed_data'
)

# ì§„í–‰ ìƒí™© ì €ì¥
import json
with open('processing_log.json', 'w') as f:
    json.dump({
        'date': '2025-10-15',
        'processed_books': len(results),
        'total_chapters': sum(r['total_chapters'] for r in results)
    }, f)
```

### Day 5: í’ˆì§ˆ ê²€ì¦ (10ì›” 16ì¼)

#### ì „ì²´ íŒ€ì› í˜‘ì—…
```python
# 1. ë°ì´í„° í†µê³„ ë¶„ì„
stats = {
    'total_books': 0,
    'total_chapters': 0,
    'total_characters': 0,
    'avg_chapter_length': 0,
    'books_with_errors': []
}

for result in results:
    stats['total_books'] += 1
    stats['total_chapters'] += result['total_chapters']
    stats['total_characters'] += result['total_length']
    
    # ì—ëŸ¬ ì²´í¬
    if result['total_chapters'] == 0:
        stats['books_with_errors'].append(result['book_id'])

stats['avg_chapter_length'] = stats['total_characters'] / stats['total_chapters']

print(json.dumps(stats, indent=2))

# 2. ê²°ì¸¡ì¹˜ í™•ì¸
missing_data = []
for result in results:
    for chapter in result['processed_chapters']:
        if not chapter['entities']['PERSON']:
            missing_data.append({
                'book_id': result['book_id'],
                'chapter': chapter['chapter_number'],
                'issue': 'No characters found'
            })

print(f"Missing data issues: {len(missing_data)}")

# 3. ìƒ˜í”Œ ë°ì´í„° ìˆ˜ë™ ê²€ì¦
import random
sample = random.choice(results)
print(f"\n=== Sample Book {sample['book_id']} ===")
print(f"Chapters: {sample['total_chapters']}")
sample_chapter = sample['processed_chapters'][0]
print(f"Sample chapter: {sample_chapter['chapter_title']}")
print(f"Characters: {[c for c, _ in sample_chapter['entities']['PERSON'][:5]]}")
print(f"Locations: {[l for l, _ in (sample_chapter['entities']['GPE'] + sample_chapter['entities']['LOC'])[:3]]}")
```

### Day 6: í•™ìŠµ ë°ì´í„° êµ¬ì¶• (10ì›” 17ì¼)

```python
# 1. í•™ìŠµ ë°ì´í„° í¬ë§· ì •ì˜
training_dataset = []

for result in results:
    for chapter in result['processed_chapters']:
        # ì…ë ¥: ì›ë³¸ ì±•í„° í…ìŠ¤íŠ¸
        input_text = chapter['original_text']
        
        # ì¶œë ¥: êµ¬ì¡°í™”ëœ ìŠ¤í¬ë¦½íŠ¸ ì •ë³´
        output_script = {
            'scene': chapter['chapter_title'],
            'characters': [c for c, _ in chapter['entities']['PERSON'][:10]],
            'locations': [l for l, _ in (chapter['entities']['GPE'] + chapter['entities']['LOC'])[:5]],
            'key_dialogues': chapter['scene_data']['dialogues'][:20],
            'narrative_summary': ' '.join(chapter['scene_data']['narrative_sentences'][:10])
        }
        
        training_dataset.append({
            'input': input_text,
            'output': output_script,
            'metadata': {
                'book_id': result['book_id'],
                'chapter_number': chapter['chapter_number']
            }
        })

print(f"Training dataset size: {len(training_dataset)} samples")

# 2. ë°ì´í„°ì…‹ ë¶„í• 
from sklearn.model_selection import train_test_split

train_data, temp_data = train_test_split(training_dataset, test_size=0.2, random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

print(f"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}")

# 3. ì €ì¥
import json
with open('train_data.json', 'w') as f:
    json.dump(train_data, f, ensure_ascii=False, indent=2)
with open('val_data.json', 'w') as f:
    json.dump(val_data, f, ensure_ascii=False, indent=2)
with open('test_data.json', 'w') as f:
    json.dump(test_data, f, ensure_ascii=False, indent=2)
```

### Day 7: ë¬¸ì„œí™” ë° ì •ë¦¬ (10ì›” 18ì¼)

```python
# ìµœì¢… ë³´ê³ ì„œ ìƒì„±
final_report = {
    'project': 'Book to Script Conversion - Data Preprocessing',
    'date': '2025-10-18',
    'deadline': '2025-10-27',
    'status': 'COMPLETED',
    
    'data_collection': {
        'total_books': stats['total_books'],
        'sources': ['Project Gutenberg'],
        'genres': {
            'fiction': 30,
            'drama': 10,
            'mystery': 10
        }
    },
    
    'data_preprocessing': {
        'total_chapters': stats['total_chapters'],
        'total_characters': stats['total_characters'],
        'avg_chapter_length': stats['avg_chapter_length'],
        'preprocessing_steps': [
            'Gutenberg header/footer removal',
            'Text cleaning',
            'Chapter segmentation',
            'Entity extraction (PERSON, GPE, LOC, DATE)',
            'Dialogue extraction',
            'Scene structuring'
        ]
    },
    
    'dataset': {
        'train_samples': len(train_data),
        'val_samples': len(val_data),
        'test_samples': len(test_data),
        'total_samples': len(training_dataset)
    },
    
    'next_steps': [
        'Model selection (T5, BART, GPT-2)',
        'Fine-tuning strategy',
        'Training setup',
        'Evaluation metrics implementation'
    ],
    
    'issues_found': stats['books_with_errors'],
    'issues_resolved': len(stats['books_with_errors']) - len(missing_data)
}

# ë³´ê³ ì„œ ì €ì¥
with open('final_report.json', 'w') as f:
    json.dump(final_report, f, ensure_ascii=False, indent=2)

# ìš”ì•½ ì¶œë ¥
print("="*50)
print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ ë³´ê³ ")
print("="*50)
print(f"ğŸ“š ìˆ˜ì§‘ëœ ë„ì„œ: {final_report['data_collection']['total_books']}ê¶Œ")
print(f"ğŸ“– ì²˜ë¦¬ëœ ì±•í„°: {final_report['data_preprocessing']['total_chapters']}ê°œ")
print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {final_report['dataset']['total_samples']}ê°œ")
print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {final_report['status']}")
print("="*50)
```

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

### 1. ì´ë¯¸ ì¤€ë¹„ëœ ê²ƒ
âœ… `data_preprocessing.ipynb` - ì™„ì „í•œ íŒŒì´í”„ë¼ì¸  
âœ… `DATA_PREPROCESSING_GUIDE.md` - ìƒì„¸ ê°€ì´ë“œ  
âœ… `QUICK_START.md` - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ  

### 2. í•´ì•¼ í•  ê²ƒ (ìˆœì„œëŒ€ë¡œ)
1. ë…¸íŠ¸ë¶ ì—´ê¸°: `data_preprocessing.ipynb`
2. ëª¨ë“  ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰
3. ë„ì„œ ID ë¦¬ìŠ¤íŠ¸ í™•ì¥ (50ê¶Œ ëª©í‘œ)
4. `pipeline.process_multiple_books()` ì‹¤í–‰
5. ê²°ê³¼ ê²€ì¦ ë° ì €ì¥

### 3. ì‹œê°„ ë°°ë¶„
- **ë°ì´í„° ìˆ˜ì§‘**: 40% (ë„ì„œ ë‹¤ìš´ë¡œë“œ)
- **ì „ì²˜ë¦¬ ì‹¤í–‰**: 30% (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
- **í’ˆì§ˆ ê²€ì¦**: 20% (ê²°ê³¼ í™•ì¸)
- **ë¬¸ì„œí™”**: 10% (ë³´ê³ ì„œ ì‘ì„±)

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê²ƒ
1. **ë§¤ì¼ ì§„í–‰ìƒí™© ì €ì¥**: ë°ì´í„° ì†ì‹¤ ë°©ì§€
2. **ì •ê¸° ë°±ì—…**: `raw_data/`, `processed_data/` í´ë”
3. **ì—ëŸ¬ ë¡œê¹…**: ì‹¤íŒ¨í•œ ë„ì„œ ID ê¸°ë¡
4. **íŒ€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜**: ë§¤ì¼ ì˜¤í›„ 6ì‹œ ì§„í–‰ ê³µìœ 

### ë¬¸ì œ ë°œìƒ ì‹œ
```python
# ë¬¸ì œ ìƒí™© ê¸°ë¡
error_log = {
    'date': '2025-10-XX',
    'error_type': 'Download failed',
    'book_id': 1234,
    'error_message': '...',
    'solution': 'Retried with longer timeout'
}
```

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼ë¬¼ (10ì›” 27ì¼)

### íŒŒì¼ êµ¬ì¡°
```
AI_lab/
â”œâ”€â”€ data_preprocessing.ipynb          # ì‘ì—… ë…¸íŠ¸ë¶
â”œâ”€â”€ raw_data/                          # ì›ë³¸ ë„ì„œ (50+ files)
â”‚   â”œâ”€â”€ book_1342.txt
â”‚   â”œâ”€â”€ book_84.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ processed_data/                    # ì „ì²˜ë¦¬ëœ ë°ì´í„° (50+ files)
â”‚   â”œâ”€â”€ book_1342.json
â”‚   â”œâ”€â”€ book_84.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train_data.json                    # í•™ìŠµ ë°ì´í„° (80%)
â”œâ”€â”€ val_data.json                      # ê²€ì¦ ë°ì´í„° (10%)
â”œâ”€â”€ test_data.json                     # í…ŒìŠ¤íŠ¸ ë°ì´í„° (10%)
â”œâ”€â”€ final_report.json                  # ìµœì¢… ë³´ê³ ì„œ
â”œâ”€â”€ processing_log.json                # ì²˜ë¦¬ ë¡œê·¸
â””â”€â”€ progress_summary.json              # ì§„í–‰ ìƒí™© ìš”ì•½
```

### ë‹¬ì„± ëª©í‘œ
- âœ… 50ê¶Œ ì´ìƒ ë„ì„œ ìˆ˜ì§‘
- âœ… ì „ì²´ ë„ì„œ ì „ì²˜ë¦¬ ì™„ë£Œ
- âœ… í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•
- âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- âœ… ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„ (ëª¨ë¸ë§)

## ğŸ“ êµìˆ˜ë‹˜ í”¼ë“œë°± ë°˜ì˜

### Comment-1: ì„±ëŠ¥ í‰ê°€ ìš°ì„ ìˆœìœ„
1. **BLEU** (í•„ìˆ˜) âœ… ì´ë¯¸ êµ¬í˜„ë¨
2. **FVD** (ì„ íƒ) - ëª¨ë¸ë§ í›„ ê²°ì •
3. **CLIPScore** (ì„ íƒ) - ì—¬ìœ  ìˆì„ ë•Œ

### Comment-2: ë³‘ë ¬ ì‘ì—…
âœ… ë°ì´í„° ìˆ˜ì§‘ + í”„ë ˆì„ì›Œí¬ ì…‹ì—… ë™ì‹œ ì§„í–‰  
âœ… ì—­í•  ë¶„ë‹´ ëª…í™•íˆ  
âœ… ë§¤ì¼ ì§„í–‰ ìƒí™© ê³µìœ   

---

**ì‹œì‘í•˜ê¸°**: `data_preprocessing.ipynb` ì—´ê³  ì²« ë²ˆì§¸ ì…€ë¶€í„° ì‹¤í–‰!

**ì§ˆë¬¸/ë¬¸ì œ**: íŒ€ ì±„íŒ…ë°©ì— ê³µìœ 

**ë§ˆê°**: 2025ë…„ 10ì›” 27ì¼

**ë‹¤ìŒ ë‹¨ê³„**: ëª¨ë¸ë§ (10ì›” 28ì¼~)
